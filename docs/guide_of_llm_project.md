## ğŸ—ï¸ í”„ë¡œì íŠ¸ êµ¬ì¡°ì™€ í™˜ê²½ ì„¤ì •

LLM ëª¨ë¸ë§ í”„ë¡œì íŠ¸ë¥¼ ì²´ê³„ì ìœ¼ë¡œ ê´€ë¦¬í•˜ê¸° ìœ„í•œ ë””ë ‰í† ë¦¬ êµ¬ì¡°ë¥¼ ë¨¼ì € ì‚´í´ë³´ì.

```
llm-modeling/
â”œâ”€â”€ pyproject.toml              # Poetry ì„¤ì •
â”œâ”€â”€ README.md
â”œâ”€â”€ .gitignore
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ models/                 # Lightning ëª¨ë“ˆë“¤
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ base_llm.py        # ê¸°ë³¸ LLM ëª¨ë“ˆ
â”‚   â”‚   â”œâ”€â”€ classification.py   # ë¶„ë¥˜ íƒœìŠ¤í¬
â”‚   â”‚   â”œâ”€â”€ generation.py      # í…ìŠ¤íŠ¸ ìƒì„±
â”‚   â”‚   â””â”€â”€ qa.py              # ì§ˆë¬¸ ë‹µë³€
â”‚   â”œâ”€â”€ data/                  # ë°ì´í„° ëª¨ë“ˆë“¤
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ base_datamodule.py
â”‚   â”‚   â””â”€â”€ text_datasets.py
â”‚   â”œâ”€â”€ callbacks/             # ì»¤ìŠ¤í…€ ì½œë°±ë“¤
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ model_monitoring.py
â”‚   â”‚   â””â”€â”€ text_generation.py
â”‚   â”œâ”€â”€ utils/                 # ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ config.py
â”‚   â”‚   â””â”€â”€ tokenization.py
â”‚   â””â”€â”€ experiments/           # ì‹¤í—˜ ìŠ¤í¬ë¦½íŠ¸ë“¤
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ train_classifier.py
â”‚       â””â”€â”€ train_generator.py
â”œâ”€â”€ configs/                   # ì„¤ì • íŒŒì¼ë“¤
â”‚   â”œâ”€â”€ model/
â”‚   â”œâ”€â”€ data/
â”‚   â””â”€â”€ training/
â”œâ”€â”€ notebooks/                 # Jupyter ë…¸íŠ¸ë¶ë“¤
â”œâ”€â”€ tests/                     # í…ŒìŠ¤íŠ¸ íŒŒì¼ë“¤
â””â”€â”€ outputs/                   # ì‹¤í—˜ ê²°ê³¼ë¬¼ë“¤
    â”œâ”€â”€ checkpoints/
    â”œâ”€â”€ logs/
    â””â”€â”€ wandb/
```

### Poetryë¥¼ í†µí•œ í™˜ê²½ êµ¬ì¶•

ë¨¼ì € Poetryë¡œ í”„ë¡œì íŠ¸ë¥¼ ì´ˆê¸°í™”í•˜ê³  ì˜ì¡´ì„±ì„ ì„¤ì¹˜í•œë‹¤.

```bash
# í”„ë¡œì íŠ¸ ì´ˆê¸°í™”
poetry new llm-modeling
cd llm-modeling

# Python 3.11 ê°€ìƒí™˜ê²½ ìƒì„±
poetry env use python3.11

# ì˜ì¡´ì„± ì„¤ì¹˜
poetry add torch pytorch-lightning transformers datasets wandb
poetry add peft accelerate deepspeed bitsandbytes
poetry add --group dev jupyter black isort pytest

# ê°€ìƒí™˜ê²½ í™œì„±í™”
poetry shell

# íŒ¨í‚¤ì§€ ì„¤ì¹˜ í™•ì¸
poetry show
```

### ê¸°ë³¸ ì„¤ì • íŒŒì¼ ìƒì„±

```python
# src/utils/config.py
"""í”„ë¡œì íŠ¸ ì „ì—­ ì„¤ì • ê´€ë¦¬"""

import os
from pathlib import Path
from dataclasses import dataclass, field
from typing import Optional, List, Dict, Any


@dataclass
class ModelConfig:
    """ëª¨ë¸ ê´€ë ¨ ì„¤ì •"""
    name: str = "gpt2"  # Hugging Face ëª¨ë¸ ì´ë¦„
    max_length: int = 512
    num_labels: Optional[int] = None
    cache_dir: Optional[str] = None

    # PEFT ì„¤ì •
    use_peft: bool = False
    peft_config: Dict[str, Any] = field(default_factory=dict)


@dataclass
class DataConfig:
    """ë°ì´í„° ê´€ë ¨ ì„¤ì •"""
    dataset_name: str = "imdb"
    train_split: str = "train"
    val_split: str = "test"
    test_split: Optional[str] = None

    batch_size: int = 8
    num_workers: int = 4
    max_samples: Optional[int] = None  # ë””ë²„ê¹…ìš©

    # í† í¬ë‚˜ì´ì € ì„¤ì •
    padding: str = "max_length"
    truncation: bool = True


@dataclass
class TrainingConfig:
    """í›ˆë ¨ ê´€ë ¨ ì„¤ì •"""
    max_epochs: int = 3
    learning_rate: float = 2e-5
    weight_decay: float = 0.01
    warmup_steps: int = 100

    # ë¶„ì‚° í•™ìŠµ ì„¤ì •
    accelerator: str = "gpu"
    devices: int = 1
    strategy: str = "auto"
    precision: str = "bf16-mixed"

    # ì²´í¬í¬ì¸íŒ…
    save_top_k: int = 3
    monitor: str = "val_loss"
    mode: str = "min"

    # ì¡°ê¸° ì¢…ë£Œ
    patience: int = 3
    min_delta: float = 0.001


@dataclass
class WandbConfig:
    """Wandb ì„¤ì •"""
    project: str = "llm-modeling"
    entity: Optional[str] = None
    name: Optional[str] = None
    tags: List[str] = field(default_factory=list)
    notes: Optional[str] = None
    offline: bool = False


@dataclass
class Config:
    """ì „ì²´ ì„¤ì •"""
    model: ModelConfig = field(default_factory=ModelConfig)
    data: DataConfig = field(default_factory=DataConfig)
    training: TrainingConfig = field(default_factory=TrainingConfig)
    wandb: WandbConfig = field(default_factory=WandbConfig)

    # í”„ë¡œì íŠ¸ ê²½ë¡œ
    project_root: Path = field(default_factory=lambda: Path(__file__).parent.parent.parent)
    output_dir: Path = field(default_factory=lambda: Path("outputs"))

    def __post_init__(self):
        """ì„¤ì • ê²€ì¦ ë° í›„ì²˜ë¦¬"""
        # ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜
        self.output_dir = self.project_root / self.output_dir

        # ë””ë ‰í† ë¦¬ ìƒì„±
        self.output_dir.mkdir(exist_ok=True, parents=True)
        (self.output_dir / "checkpoints").mkdir(exist_ok=True)
        (self.output_dir / "logs").mkdir(exist_ok=True)

        # í™˜ê²½ ë³€ìˆ˜ì—ì„œ Wandb ì„¤ì • ì½ê¸°
        if not self.wandb.entity:
            self.wandb.entity = os.getenv("WANDB_ENTITY")


# ì „ì—­ ì„¤ì • ì¸ìŠ¤í„´ìŠ¤
config = Config()

print("âœ… ì„¤ì • íŒŒì¼ ë¡œë“œ ì™„ë£Œ!")
print(f"í”„ë¡œì íŠ¸ ë£¨íŠ¸: {config.project_root}")
print(f"ì¶œë ¥ ë””ë ‰í† ë¦¬: {config.output_dir}")
```

## ğŸ§  LLMì„ ìœ„í•œ ê¸°ë³¸ LightningModule

LLM ëª¨ë¸ë§ì˜ ê¸°ë°˜ì´ ë˜ëŠ” **BaseLLMModule**ì„ ë¨¼ì € êµ¬í˜„í•´ë³´ì. ì´ í´ë˜ìŠ¤ëŠ” ëª¨ë“  LLM íƒœìŠ¤í¬ì— ê³µí†µìœ¼ë¡œ ì‚¬ìš©ë˜ëŠ” ê¸°ëŠ¥ë“¤ì„ ì œê³µí•œë‹¤.

```python
# src/models/base_llm.py
"""LLM ëª¨ë¸ë§ì„ ìœ„í•œ ê¸°ë³¸ Lightning ëª¨ë“ˆ"""

import pytorch_lightning as pl
import torch
import torch.nn as nn
from torch.optim import AdamW
from torch.optim.lr_scheduler import get_linear_schedule_with_warmup
from transformers import (
    AutoModel, AutoTokenizer, AutoConfig,
    get_linear_schedule_with_warmup as transformers_scheduler
)
from peft import get_peft_model, LoraConfig, TaskType
from typing import Dict, Any, Optional, Tuple, Union
import logging

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class BaseLLMModule(pl.LightningModule):
    """
    LLM ëª¨ë¸ë§ì„ ìœ„í•œ ê¸°ë³¸ Lightning ëª¨ë“ˆ

    ì´ í´ë˜ìŠ¤ëŠ” ë‹¤ìŒ ê¸°ëŠ¥ë“¤ì„ ì œê³µí•©ë‹ˆë‹¤:
    - Hugging Face ëª¨ë¸ ë¡œë”© ë° ì„¤ì •
    - PEFT (LoRA/QLoRA) ì§€ì›
    - ì˜µí‹°ë§ˆì´ì € ë° ìŠ¤ì¼€ì¤„ëŸ¬ ì„¤ì •
    - ë©”íŠ¸ë¦­ ë¡œê¹…
    - ì²´í¬í¬ì¸íŠ¸ ê´€ë¦¬
    """

    def __init__(
            self,
            model_name: str = "gpt2",
            learning_rate: float = 2e-5,
            weight_decay: float = 0.01,
            warmup_steps: int = 100,
            max_epochs: int = 3,
            use_peft: bool = False,
            peft_config: Optional[Dict[str, Any]] = None,
            model_max_length: int = 512,
            **kwargs
    ):
        super().__init__()

        # í•˜ì´í¼íŒŒë¼ë¯¸í„° ìë™ ì €ì¥ (Wandbì— ìë™ ë¡œê¹…ë¨)
        self.save_hyperparameters()

        # ëª¨ë¸ ì„¤ì •
        self.model_name = model_name
        self.learning_rate = learning_rate
        self.weight_decay = weight_decay
        self.warmup_steps = warmup_steps
        self.max_epochs = max_epochs
        self.use_peft = use_peft
        self.model_max_length = model_max_length

        # ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ì´ˆê¸°í™”
        self._setup_model_and_tokenizer()

        # PEFT ì ìš© (LoRA ë“±)
        if use_peft:
            self._setup_peft(peft_config or {})

        # ë©”íŠ¸ë¦­ ì €ì¥ìš© ë¦¬ìŠ¤íŠ¸
        self.training_step_outputs = []
        self.validation_step_outputs = []

        logger.info(f"âœ… {self.__class__.__name__} ì´ˆê¸°í™” ì™„ë£Œ")
        logger.info(f"ëª¨ë¸: {model_name}")
        logger.info(f"PEFT ì‚¬ìš©: {use_peft}")
        logger.info(f"ì´ íŒŒë¼ë¯¸í„°: {self.count_parameters():,}")

    def _setup_model_and_tokenizer(self):
        """ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ì„¤ì •"""
        try:
            # ì„¤ì • ë¡œë“œ
            self.config = AutoConfig.from_pretrained(self.model_name)

            # í† í¬ë‚˜ì´ì € ë¡œë“œ
            self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)

            # íŒ¨ë“œ í† í° ì„¤ì • (GPT ê³„ì—´ ëª¨ë¸ì˜ ê²½ìš°)
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token

            # ëª¨ë¸ ìµœëŒ€ ê¸¸ì´ ì„¤ì •
            if hasattr(self.tokenizer, 'model_max_length'):
                self.tokenizer.model_max_length = self.model_max_length

            # ê¸°ë³¸ ëª¨ë¸ ë¡œë“œ (í•˜ìœ„ í´ë˜ìŠ¤ì—ì„œ ì˜¤ë²„ë¼ì´ë“œ)
            self.model = AutoModel.from_pretrained(
                self.model_name,
                config=self.config,
                torch_dtype=torch.bfloat16,  # ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±
                attn_implementation="flash_attention_2",  # Flash Attention ì‚¬ìš©
            )

            logger.info(f"ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë“œ ì™„ë£Œ: {self.model_name}")

        except Exception as e:
            logger.error(f"ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            raise

    def _setup_peft(self, peft_config: Dict[str, Any]):
        """PEFT (Parameter-Efficient Fine-Tuning) ì„¤ì •"""
        default_peft_config = {
            "task_type": TaskType.CAUSAL_LM,  # í•˜ìœ„ í´ë˜ìŠ¤ì—ì„œ ì˜¤ë²„ë¼ì´ë“œ
            "r": 16,  # LoRA rank
            "lora_alpha": 32,  # LoRA alpha
            "lora_dropout": 0.1,  # LoRA dropout
            "target_modules": ["q_proj", "v_proj"],  # ì ìš©í•  ëª¨ë“ˆë“¤
        }

        # ê¸°ë³¸ ì„¤ì •ê³¼ ì‚¬ìš©ì ì„¤ì • ë³‘í•©
        default_peft_config.update(peft_config)

        # LoRA ì„¤ì • ìƒì„±
        lora_config = LoraConfig(**default_peft_config)

        # ëª¨ë¸ì— PEFT ì ìš©
        self.model = get_peft_model(self.model, lora_config)

        # í›ˆë ¨ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„°ë§Œ ì¶œë ¥
        self.model.print_trainable_parameters()

        logger.info("PEFT (LoRA) ì„¤ì • ì™„ë£Œ")

    def count_parameters(self) -> int:
        """ëª¨ë¸ì˜ ì „ì²´ íŒŒë¼ë¯¸í„° ê°œìˆ˜ ê³„ì‚°"""
        return sum(p.numel() for p in self.parameters())

    def count_trainable_parameters(self) -> int:
        """í›ˆë ¨ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„° ê°œìˆ˜ ê³„ì‚°"""
        return sum(p.numel() for p in self.parameters() if p.requires_grad)

    def forward(self, **inputs) -> Dict[str, torch.Tensor]:
        """
        ìˆœì „íŒŒ - í•˜ìœ„ í´ë˜ìŠ¤ì—ì„œ êµ¬ì²´ì ìœ¼ë¡œ êµ¬í˜„

        Args:
            **inputs: ëª¨ë¸ ì…ë ¥ (input_ids, attention_mask ë“±)

        Returns:
            ëª¨ë¸ ì¶œë ¥ì„ ë‹´ì€ ë”•ì…”ë„ˆë¦¬
        """
        raise NotImplementedError("í•˜ìœ„ í´ë˜ìŠ¤ì—ì„œ êµ¬í˜„í•´ì•¼ í•©ë‹ˆë‹¤.")

    def training_step(self, batch: Dict[str, torch.Tensor], batch_idx: int) -> torch.Tensor:
        """
        í›ˆë ¨ ìŠ¤í… - í•˜ìœ„ í´ë˜ìŠ¤ì—ì„œ êµ¬ì²´ì ìœ¼ë¡œ êµ¬í˜„

        Args:
            batch: ë°°ì¹˜ ë°ì´í„°
            batch_idx: ë°°ì¹˜ ì¸ë±ìŠ¤

        Returns:
            ì†ì‹¤ê°’
        """
        raise NotImplementedError("í•˜ìœ„ í´ë˜ìŠ¤ì—ì„œ êµ¬í˜„í•´ì•¼ í•©ë‹ˆë‹¤.")

    def validation_step(self, batch: Dict[str, torch.Tensor], batch_idx: int) -> torch.Tensor:
        """
        ê²€ì¦ ìŠ¤í… - í•˜ìœ„ í´ë˜ìŠ¤ì—ì„œ êµ¬ì²´ì ìœ¼ë¡œ êµ¬í˜„

        Args:
            batch: ë°°ì¹˜ ë°ì´í„°
            batch_idx: ë°°ì¹˜ ì¸ë±ìŠ¤

        Returns:
            ì†ì‹¤ê°’
        """
        raise NotImplementedError("í•˜ìœ„ í´ë˜ìŠ¤ì—ì„œ êµ¬í˜„í•´ì•¼ í•©ë‹ˆë‹¤.")

    def on_train_epoch_end(self):
        """í›ˆë ¨ ì—í¬í¬ ì¢…ë£Œ ì‹œ ì²˜ë¦¬"""
        # í‰ê·  ë©”íŠ¸ë¦­ ê³„ì‚° ë° ë¡œê¹…
        if self.training_step_outputs:
            avg_loss = torch.stack([x['loss'] for x in self.training_step_outputs]).mean()
            self.log('train_loss_epoch', avg_loss, prog_bar=True, sync_dist=True)

            # GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë¡œê¹…
            if torch.cuda.is_available():
                self.log('gpu_memory_gb', torch.cuda.memory_allocated() / 1024 ** 3)

            # ë©”ëª¨ë¦¬ ì •ë¦¬
            self.training_step_outputs.clear()

    def on_validation_epoch_end(self):
        """ê²€ì¦ ì—í¬í¬ ì¢…ë£Œ ì‹œ ì²˜ë¦¬"""
        if self.validation_step_outputs:
            avg_loss = torch.stack([x['loss'] for x in self.validation_step_outputs]).mean()
            self.log('val_loss_epoch', avg_loss, prog_bar=True, sync_dist=True)

            # ë©”ëª¨ë¦¬ ì •ë¦¬
            self.validation_step_outputs.clear()

    def configure_optimizers(self) -> Dict[str, Any]:
        """
        ì˜µí‹°ë§ˆì´ì €ì™€ ìŠ¤ì¼€ì¤„ëŸ¬ ì„¤ì •

        LLM í›ˆë ¨ì— ìµœì í™”ëœ AdamW + Linear Warmup ìŠ¤ì¼€ì¤„ëŸ¬ ì‚¬ìš©
        """
        # ê°€ì¤‘ì¹˜ ê°ì‡ ë¥¼ ì ìš©í•˜ì§€ ì•Šì„ íŒŒë¼ë¯¸í„°ë“¤
        no_decay = ["bias", "LayerNorm.weight", "layer_norm.weight"]

        # íŒŒë¼ë¯¸í„° ê·¸ë£¹ ë¶„ë¦¬
        optimizer_grouped_parameters = [
            {
                "params": [
                    p for n, p in self.named_parameters()
                    if not any(nd in n for nd in no_decay) and p.requires_grad
                ],
                "weight_decay": self.weight_decay,
            },
            {
                "params": [
                    p for n, p in self.named_parameters()
                    if any(nd in n for nd in no_decay) and p.requires_grad
                ],
                "weight_decay": 0.0,
            },
        ]

        # AdamW ì˜µí‹°ë§ˆì´ì €
        optimizer = AdamW(
            optimizer_grouped_parameters,
            lr=self.learning_rate,
            eps=1e-8,
            betas=(0.9, 0.999)  # BERT ë…¼ë¬¸ ì„¤ì •
        )

        # ì´ ìŠ¤í… ìˆ˜ ê³„ì‚°
        total_steps = self.trainer.estimated_stepping_batches

        # Linear Warmup + Linear Decay ìŠ¤ì¼€ì¤„ëŸ¬
        scheduler = get_linear_schedule_with_warmup(
            optimizer,
            num_warmup_steps=self.warmup_steps,
            num_training_steps=total_steps
        )

        logger.info(f"ì˜µí‹°ë§ˆì´ì € ì„¤ì • ì™„ë£Œ")
        logger.info(f"í•™ìŠµë¥ : {self.learning_rate}")
        logger.info(f"ì´ ìŠ¤í…: {total_steps}")
        logger.info(f"ì›Œë°ì—… ìŠ¤í…: {self.warmup_steps}")

        return {
            "optimizer": optimizer,
            "lr_scheduler": {
                "scheduler": scheduler,
                "interval": "step",  # ìŠ¤í…ë§ˆë‹¤ ìŠ¤ì¼€ì¤„ëŸ¬ ì—…ë°ì´íŠ¸
                "frequency": 1,
                "name": "learning_rate"  # Wandbì— ë¡œê¹…ë  ì´ë¦„
            }
        }

    def lr_scheduler_step(self, scheduler, metric):
        """í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬ ìŠ¤í… (ìˆ˜ë™ ì œì–´)"""
        scheduler.step()

    def get_model_info(self) -> Dict[str, Any]:
        """ëª¨ë¸ ì •ë³´ ë°˜í™˜ (ë””ë²„ê¹… ë° ë¡œê¹…ìš©)"""
        info = {
            "model_name": self.model_name,
            "total_parameters": self.count_parameters(),
            "trainable_parameters": self.count_trainable_parameters(),
            "use_peft": self.use_peft,
            "device": next(self.parameters()).device.type,
            "dtype": next(self.parameters()).dtype,
        }

        # PEFT ì •ë³´ ì¶”ê°€
        if self.use_peft and hasattr(self.model, 'peft_config'):
            info["peft_config"] = self.model.peft_config

        return info


# ê¸°ë³¸ ëª¨ë“ˆ í…ŒìŠ¤íŠ¸
if __name__ == "__main__":
    # ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸
    module = BaseLLMModule(
        model_name="gpt2",
        use_peft=True,
        peft_config={"r": 8, "lora_alpha": 16}
    )

    print("ëª¨ë¸ ì •ë³´:")
    for key, value in module.get_model_info().items():
        print(f"  {key}: {value}")
```

## ğŸ“Š LLM í…ìŠ¤íŠ¸ ë¶„ë¥˜ ëª¨ë“ˆ êµ¬í˜„

ì´ì œ BaseLLMModuleì„ ìƒì†ë°›ì•„ **í…ìŠ¤íŠ¸ ë¶„ë¥˜ íƒœìŠ¤í¬**ë¥¼ ìœ„í•œ êµ¬ì²´ì ì¸ ëª¨ë“ˆì„ êµ¬í˜„í•´ë³´ì.

```python
# src/models/classification.py
"""í…ìŠ¤íŠ¸ ë¶„ë¥˜ë¥¼ ìœ„í•œ Lightning ëª¨ë“ˆ"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch import Tensor
from transformers import AutoModelForSequenceClassification
from torchmetrics import Accuracy, F1Score, Precision, Recall, AUROC
from typing import Dict, Any, Optional, List
import logging

from .base_llm import BaseLLMModule

logger = logging.getLogger(__name__)


class TextClassificationModule(BaseLLMModule):
    """
    í…ìŠ¤íŠ¸ ë¶„ë¥˜ë¥¼ ìœ„í•œ Lightning ëª¨ë“ˆ

    Features:
    - ë‹¤ì¤‘ í´ë˜ìŠ¤ ë¶„ë¥˜ ì§€ì›
    - í´ë˜ìŠ¤ ë¶ˆê· í˜• ì²˜ë¦¬ (ê°€ì¤‘ì¹˜ ì ìš©)
    - ë‹¤ì–‘í•œ ë©”íŠ¸ë¦­ ìë™ ê³„ì‚°
    - í˜¼ë™ í–‰ë ¬ ë¡œê¹…
    """

    def __init__(
            self,
            num_labels: int,
            class_weights: Optional[List[float]] = None,
            label_names: Optional[List[str]] = None,
            dropout_rate: float = 0.1,
            **kwargs
    ):
        # ë¶„ë¥˜ íƒœìŠ¤í¬ìš© ì„¤ì • ì¶”ê°€
        if 'peft_config' in kwargs and 'task_type' not in kwargs['peft_config']:
            from peft import TaskType
            kwargs['peft_config']['task_type'] = TaskType.SEQ_CLS

        super().__init__(**kwargs)

        self.num_labels = num_labels
        self.label_names = label_names or [f"Label_{i}" for i in range(num_labels)]
        self.dropout_rate = dropout_rate

        # í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ì„¤ì • (ë¶ˆê· í˜• ë°ì´í„° ì²˜ë¦¬)
        self.register_buffer(
            'class_weights',
            torch.tensor(class_weights) if class_weights else None
        )

        # ëª¨ë¸ ì¬ì„¤ì • (ë¶„ë¥˜ìš©)
        self._setup_classification_model()

        # ì†ì‹¤ í•¨ìˆ˜
        self.criterion = nn.CrossEntropyLoss(weight=self.class_weights)

        # ë©”íŠ¸ë¦­ ì„¤ì •
        self._setup_metrics()

        logger.info(f"í…ìŠ¤íŠ¸ ë¶„ë¥˜ ëª¨ë“ˆ ì´ˆê¸°í™” ì™„ë£Œ")
        logger.info(f"í´ë˜ìŠ¤ ìˆ˜: {num_labels}")
        logger.info(f"í´ë˜ìŠ¤ ì´ë¦„: {self.label_names}")

    def _setup_classification_model(self):
        """ë¶„ë¥˜ìš© ëª¨ë¸ ì„¤ì •"""
        # ê¸°ì¡´ ëª¨ë¸ ì œê±°
        del self.model

        # ë¶„ë¥˜ìš© ëª¨ë¸ ë¡œë“œ
        self.model = AutoModelForSequenceClassification.from_pretrained(
            self.model_name,
            num_labels=self.num_labels,
            torch_dtype=torch.bfloat16,
            attn_implementation="flash_attention_2",
        )

        # ë¶„ë¥˜ê¸° í—¤ë“œì— ë“œë¡­ì•„ì›ƒ ì¶”ê°€
        if hasattr(self.model, 'classifier') and self.dropout_rate > 0:
            classifier_input_dim = self.model.classifier.in_features
            self.model.classifier = nn.Sequential(
                nn.Dropout(self.dropout_rate),
                nn.Linear(classifier_input_dim, self.num_labels)
            )

        # PEFT ì¬ì ìš© (í•„ìš”í•œ ê²½ìš°)
        if self.use_peft:
            from peft import get_peft_model, LoraConfig, TaskType

            peft_config = getattr(self, 'hparams', {}).get('peft_config', {})
            default_config = {
                "task_type": TaskType.SEQ_CLS,
                "r": 16,
                "lora_alpha": 32,
                "lora_dropout": 0.1,
                "target_modules": ["q_proj", "v_proj"],
            }
            default_config.update(peft_config)

            lora_config = LoraConfig(**default_config)
            self.model = get_peft_model(self.model, lora_config)

    def _setup_metrics(self):
        """ë©”íŠ¸ë¦­ ì„¤ì •"""
        task = "multiclass" if self.num_labels > 2 else "binary"
        average = "macro" if self.num_labels > 2 else "binary"

        # í›ˆë ¨ ë©”íŠ¸ë¦­
        self.train_accuracy = Accuracy(task=task, num_classes=self.num_labels)
        self.train_f1 = F1Score(task=task, num_classes=self.num_labels, average=average)

        # ê²€ì¦ ë©”íŠ¸ë¦­
        self.val_accuracy = Accuracy(task=task, num_classes=self.num_labels)
        self.val_f1 = F1Score(task=task, num_classes=self.num_labels, average=average)
        self.val_precision = Precision(task=task, num_classes=self.num_labels, average=average)
        self.val_recall = Recall(task=task, num_classes=self.num_labels, average=average)

        # AUROC (í™•ë¥ ì´ í•„ìš”í•˜ë¯€ë¡œ ë¡œì§“ ì‚¬ìš©)
        if self.num_labels == 2:
            self.val_auroc = AUROC(task="binary")
        else:
            self.val_auroc = AUROC(task="multiclass", num_classes=self.num_labels)

        logger.info(f"ë©”íŠ¸ë¦­ ì„¤ì • ì™„ë£Œ: {task} ë¶„ë¥˜")

    def forward(self, input_ids: Tensor, attention_mask: Tensor, **kwargs) -> Dict[str, Tensor]:
        """
        ìˆœì „íŒŒ

        Args:
            input_ids: í† í° ID í…ì„œ [batch_size, seq_len]
            attention_mask: ì–´í…ì…˜ ë§ˆìŠ¤í¬ [batch_size, seq_len]

        Returns:
            ëª¨ë¸ ì¶œë ¥ ë”•ì…”ë„ˆë¦¬ (logits, hidden_states ë“±)
        """
        outputs = self.model(
            input_ids=input_ids,
            attention_mask=attention_mask,
            output_hidden_states=True,
            return_dict=True,
            **kwargs
        )

        return {
            "logits": outputs.logits,
            "hidden_states": outputs.hidden_states,
            "attentions": getattr(outputs, 'attentions', None)
        }

    def training_step(self, batch: Dict[str, Tensor], batch_idx: int) -> Tensor:
        """í›ˆë ¨ ìŠ¤í…"""
        # ìˆœì „íŒŒ
        outputs = self.forward(
            input_ids=batch["input_ids"],
            attention_mask=batch["attention_mask"]
        )

        logits = outputs["logits"]
        labels = batch["labels"]

        # ì†ì‹¤ ê³„ì‚°
        loss = self.criterion(logits, labels)

        # ì˜ˆì¸¡ ë° ë©”íŠ¸ë¦­ ê³„ì‚°
        preds = torch.argmax(logits, dim=-1)

        # ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸
        self.train_accuracy(preds, labels)
        self.train_f1(preds, labels)

        # ë¡œê¹…
        self.log("train_loss", loss, on_step=True, on_epoch=True, prog_bar=True, sync_dist=True)
        self.log("train_acc", self.train_accuracy, on_epoch=True, prog_bar=True, sync_dist=True)
        self.log("train_f1", self.train_f1, on_epoch=True, sync_dist=True)

        # í•™ìŠµë¥  ë¡œê¹…
        current_lr = self.trainer.optimizers[0].param_groups[0]['lr']
        self.log("learning_rate", current_lr, on_step=True, sync_dist=True)

        # ì¶œë ¥ ì €ì¥ (ì—í¬í¬ ì¢…ë£Œ ì‹œ ì²˜ë¦¬ìš©)
        output = {
            "loss": loss,
            "preds": preds.detach(),
            "labels": labels.detach(),
            "logits": logits.detach()
        }
        self.training_step_outputs.append(output)

        return loss

    def validation_step(self, batch: Dict[str, Tensor], batch_idx: int) -> Tensor:
        """ê²€ì¦ ìŠ¤í…"""
        # ìˆœì „íŒŒ
        outputs = self.forward(
            input_ids=batch["input_ids"],
            attention_mask=batch["attention_mask"]
        )

        logits = outputs["logits"]
        labels = batch["labels"]

        # ì†ì‹¤ ê³„ì‚°
        loss = self.criterion(logits, labels)

        # ì˜ˆì¸¡ ë° í™•ë¥  ê³„ì‚°
        preds = torch.argmax(logits, dim=-1)
        probs = F.softmax(logits, dim=-1)

        # ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸
        self.val_accuracy(preds, labels)
        self.val_f1(preds, labels)
        self.val_precision(preds, labels)
        self.val_recall(preds, labels)

        # AUROC ì—…ë°ì´íŠ¸
        if self.num_labels == 2:
            self.val_auroc(probs[:, 1], labels)  # ì–‘ì„± í´ë˜ìŠ¤ í™•ë¥ 
        else:
            self.val_auroc(probs, labels)

        # ë¡œê¹…
        self.log("val_loss", loss, on_epoch=True, prog_bar=True, sync_dist=True)
        self.log("val_acc", self.val_accuracy, on_epoch=True, prog_bar=True, sync_dist=True)
        self.log("val_f1", self.val_f1, on_epoch=True, sync_dist=True)
        self.log("val_precision", self.val_precision, on_epoch=True, sync_dist=True)
        self.log("val_recall", self.val_recall, on_epoch=True, sync_dist=True)
        self.log("val_auroc", self.val_auroc, on_epoch=True, sync_dist=True)

        # ì¶œë ¥ ì €ì¥
        output = {
            "loss": loss,
            "preds": preds.detach(),
            "labels": labels.detach(),
            "probs": probs.detach()
        }
        self.validation_step_outputs.append(output)

        return loss

    def on_validation_epoch_end(self):
        """ê²€ì¦ ì—í¬í¬ ì¢…ë£Œ ì‹œ í˜¼ë™ í–‰ë ¬ ë¡œê¹…"""
        super().on_validation_epoch_end()

        # í˜¼ë™ í–‰ë ¬ ìƒì„± ë° ë¡œê¹… (Wandbì— ìë™ ì „ì†¡)
        if self.validation_step_outputs and hasattr(self.logger, 'experiment'):
            all_preds = torch.cat([x["preds"] for x in self.validation_step_outputs])
            all_labels = torch.cat([x["labels"] for x in self.validation_step_outputs])

            # Wandb confusion matrix
            try:
                import wandb

                # í˜¼ë™ í–‰ë ¬ ìƒì„±
                cm = wandb.plot.confusion_matrix(
                    probs=None,
                    y_true=all_labels.cpu().numpy(),
                    preds=all_preds.cpu().numpy(),
                    class_names=self.label_names
                )

                # Wandbì— ë¡œê¹…
                self.logger.experiment.log({"confusion_matrix": cm})

            except ImportError:
                logger.warning("Wandbë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ì–´ í˜¼ë™ í–‰ë ¬ ë¡œê¹…ì„ ê±´ë„ˆëœë‹ˆë‹¤.")

    def predict_step(self, batch: Dict[str, Tensor], batch_idx: int) -> Dict[str, Tensor]:
        """ì˜ˆì¸¡ ìŠ¤í… (ì¶”ë¡ ìš©)"""
        outputs = self.forward(
            input_ids=batch["input_ids"],
            attention_mask=batch["attention_mask"]
        )

        logits = outputs["logits"]
        probs = F.softmax(logits, dim=-1)
        preds = torch.argmax(logits, dim=-1)

        return {
            "predictions": preds,
            "probabilities": probs,
            "logits": logits
        }

    def get_sample_predictions(self, texts: List[str], max_samples: int = 5) -> List[Dict[str, Any]]:
        """
        ìƒ˜í”Œ í…ìŠ¤íŠ¸ì— ëŒ€í•œ ì˜ˆì¸¡ ê²°ê³¼ ë°˜í™˜ (ë””ë²„ê¹…ìš©)

        Args:
            texts: ì˜ˆì¸¡í•  í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
            max_samples: ìµœëŒ€ ìƒ˜í”Œ ìˆ˜

        Returns:
            ì˜ˆì¸¡ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        """
        self.eval()
        results = []

        with torch.no_grad():
            for i, text in enumerate(texts[:max_samples]):
                # í† í¬ë‚˜ì´ì§•
                inputs = self.tokenizer(
                    text,
                    truncation=True,
                    padding=True,
                    max_length=self.model_max_length,
                    return_tensors="pt"
                ).to(self.device)

                # ì˜ˆì¸¡
                outputs = self.forward(**inputs)
                logits = outputs["logits"]
                probs = F.softmax(logits, dim=-1)
                pred_idx = torch.argmax(logits, dim=-1).item()
                confidence = probs[0, pred_idx].item()

                results.append({
                    "text": text,
                    "predicted_label": self.label_names[pred_idx],
                    "confidence": confidence,
                    "all_probabilities": {
                        self.label_names[j]: probs[0, j].item()
                        for j in range(self.num_labels)
                    }
                })

        return results


# ì‚¬ìš© ì˜ˆì‹œ
if __name__ == "__main__":
    # ì´ì§„ ë¶„ë¥˜ ëª¨ë¸ í…ŒìŠ¤íŠ¸
    model = TextClassificationModule(
        model_name="distilbert-base-uncased",
        num_labels=2,
        label_names=["Negative", "Positive"],
        learning_rate=2e-5,
        use_peft=True,
        peft_config={"r": 8, "lora_alpha": 16}
    )

    print("ë¶„ë¥˜ ëª¨ë¸ ì •ë³´:")
    for key, value in model.get_model_info().items():
        print(f"  {key}: {value}")

    # ìƒ˜í”Œ ì˜ˆì¸¡ í…ŒìŠ¤íŠ¸
    sample_texts = [
        "This movie is amazing!",
        "I hate this film.",
        "It's okay, not great but not terrible either."
    ]

    predictions = model.get_sample_predictions(sample_texts)
    print("\nìƒ˜í”Œ ì˜ˆì¸¡:")
    for pred in predictions:
        print(f"Text: {pred['text']}")
        print(f"Prediction: {pred['predicted_label']} (confidence: {pred['confidence']:.3f})")
        print()
```

## ğŸ“ í…ìŠ¤íŠ¸ ìƒì„± ëª¨ë“ˆ êµ¬í˜„

ë‹¤ìŒìœ¼ë¡œ **í…ìŠ¤íŠ¸ ìƒì„± íƒœìŠ¤í¬**ë¥¼ ìœ„í•œ ëª¨ë“ˆì„ êµ¬í˜„í•´ë³´ì. ì´ ëª¨ë“ˆì€ ì–¸ì–´ ëª¨ë¸ë§ê³¼ ì¡°ê±´ë¶€ ìƒì„±ì„ ì§€ì›í•œë‹¤.

```python
# src/models/generation.py
"""í…ìŠ¤íŠ¸ ìƒì„±ì„ ìœ„í•œ Lightning ëª¨ë“ˆ"""

import torch
import torch.nn.functional as F
from torch import Tensor
from transformers import AutoModelForCausalLM, GenerationConfig
from torchmetrics.text import Perplexity, BLEUScore, ROUGEScore
from typing import Dict, Any, Optional, List, Union
import logging

from .base_llm import BaseLLMModule

logger = logging.getLogger(__name__)


class TextGenerationModule(BaseLLMModule):
    """
    í…ìŠ¤íŠ¸ ìƒì„±ì„ ìœ„í•œ Lightning ëª¨ë“ˆ

    Features:
    - Causal Language Modeling
    - ì¡°ê±´ë¶€ í…ìŠ¤íŠ¸ ìƒì„±
    - ë‹¤ì–‘í•œ ë””ì½”ë”© ì „ëµ ì§€ì›
    - ìƒì„± í’ˆì§ˆ ë©”íŠ¸ë¦­ ìë™ ê³„ì‚°
    """

    def __init__(
            self,
            max_new_tokens: int = 50,
            temperature: float = 1.0,
            top_k: int = 50,
            top_p: float = 0.9,
            do_sample: bool = True,
            pad_token_id: Optional[int] = None,
            **kwargs
    ):
        # ìƒì„± íƒœìŠ¤í¬ìš© ì„¤ì •
        if 'peft_config' in kwargs and 'task_type' not in kwargs['peft_config']:
            from peft import TaskType
            kwargs['peft_config']['task_type'] = TaskType.CAUSAL_LM

        super().__init__(**kwargs)

        # ìƒì„± ì„¤ì •
        self.max_new_tokens = max_new_tokens
        self.temperature = temperature
        self.top_k = top_k
        self.top_p = top_p
        self.do_sample = do_sample

        # ëª¨ë¸ ì¬ì„¤ì • (ìƒì„±ìš©)
        self._setup_generation_model()

        # ìƒì„± ì„¤ì •
        self._setup_generation_config(pad_token_id)

        # ë©”íŠ¸ë¦­ ì„¤ì •
        self._setup_metrics()

        logger.info(f"í…ìŠ¤íŠ¸ ìƒì„± ëª¨ë“ˆ ì´ˆê¸°í™” ì™„ë£Œ")
        logger.info(f"ìµœëŒ€ ìƒì„± í† í°: {max_new_tokens}")
        logger.info(f"ìƒ˜í”Œë§: {do_sample}, ì˜¨ë„: {temperature}")

    def _setup_generation_model(self):
        """ìƒì„±ìš© ëª¨ë¸ ì„¤ì •"""
        # ê¸°ì¡´ ëª¨ë¸ ì œê±°
        del self.model

        # ìƒì„±ìš© ëª¨ë¸ ë¡œë“œ
        self.model = AutoModelForCausalLM.from_pretrained(
            self.model_name,
            torch_dtype=torch.bfloat16,
            attn_implementation="flash_attention_2",
        )

        # PEFT ì¬ì ìš© (í•„ìš”í•œ ê²½ìš°)
        if self.use_peft:
            from peft import get_peft_model, LoraConfig, TaskType

            peft_config = getattr(self, 'hparams', {}).get('peft_config', {})
            default_config = {
                "task_type": TaskType.CAUSAL_LM,
                "r": 16,
                "lora_alpha": 32,
                "lora_dropout": 0.1,
                "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"],
            }
            default_config.update(peft_config)

            lora_config = LoraConfig(**default_config)
            self.model = get_peft_model(self.model, lora_config)

    def _setup_generation_config(self, pad_token_id: Optional[int]):
        """ìƒì„± ì„¤ì •"""
        # íŒ¨ë“œ í† í° ì„¤ì •
        if pad_token_id is None:
            pad_token_id = self.tokenizer.eos_token_id

        self.generation_config = GenerationConfig(
            max_new_tokens=self.max_new_tokens,
            temperature=self.temperature,
            top_k=self.top_k,
            top_p=self.top_p,
            do_sample=self.do_sample,
            pad_token_id=pad_token_id,
            eos_token_id=self.tokenizer.eos_token_id,
            repetition_penalty=1.1,  # ë°˜ë³µ ë°©ì§€
            length_penalty=1.0,  # ê¸¸ì´ í˜ë„í‹°
        )

        # ëª¨ë¸ì— ìƒì„± ì„¤ì • ì ìš©
        self.model.generation_config = self.generation_config

    def _setup_metrics(self):
        """ë©”íŠ¸ë¦­ ì„¤ì •"""
        # Perplexity (í˜¼ë€ë„) - ì–¸ì–´ ëª¨ë¸ í’ˆì§ˆ ì§€í‘œ
        self.train_perplexity = Perplexity()
        self.val_perplexity = Perplexity()

        # BLEU Score - ìƒì„± í’ˆì§ˆ ì§€í‘œ (ì°¸ì¡° í…ìŠ¤íŠ¸ í•„ìš”)
        self.val_bleu = BLEUScore()

        # ROUGE Score - ìš”ì•½ í’ˆì§ˆ ì§€í‘œ
        self.val_rouge = ROUGEScore()

        logger.info("ìƒì„± ë©”íŠ¸ë¦­ ì„¤ì • ì™„ë£Œ")

    def forward(self, input_ids: Tensor, attention_mask: Tensor, **kwargs) -> Dict[str, Tensor]:
        """
        ìˆœì „íŒŒ

        Args:
            input_ids: í† í° ID í…ì„œ [batch_size, seq_len]
            attention_mask: ì–´í…ì…˜ ë§ˆìŠ¤í¬ [batch_size, seq_len]

        Returns:
            ëª¨ë¸ ì¶œë ¥ ë”•ì…”ë„ˆë¦¬
        """
        outputs = self.model(
            input_ids=input_ids,
            attention_mask=attention_mask,
            labels=input_ids,  # Causal LMì€ inputê³¼ labelì´ ë™ì¼
            output_hidden_states=True,
            return_dict=True,
            **kwargs
        )

        return {
            "logits": outputs.logits,
            "loss": outputs.loss,
            "hidden_states": outputs.hidden_states,
            "attentions": getattr(outputs, 'attentions', None)
        }

    def training_step(self, batch: Dict[str, Tensor], batch_idx: int) -> Tensor:
        """í›ˆë ¨ ìŠ¤í…"""
        # ìˆœì „íŒŒ
        outputs = self.forward(
            input_ids=batch["input_ids"],
            attention_mask=batch["attention_mask"]
        )

        loss = outputs["loss"]
        logits = outputs["logits"]

        # Perplexity ê³„ì‚°
        # ë§ˆì§€ë§‰ í† í°ì„ ì œì™¸í•œ logitsì™€ ì²« ë²ˆì§¸ í† í°ì„ ì œì™¸í•œ labels ì‚¬ìš©
        shift_logits = logits[..., :-1, :].contiguous()
        shift_labels = batch["input_ids"][..., 1:].contiguous()

        # íŒ¨ë”© í† í° ë§ˆìŠ¤í‚¹
        attention_mask = batch["attention_mask"][..., 1:].contiguous()

        # Perplexity ì—…ë°ì´íŠ¸ (ìœ íš¨í•œ í† í°ë§Œ)
        valid_mask = attention_mask.bool()
        if valid_mask.sum() > 0:
            valid_logits = shift_logits[valid_mask]
            valid_labels = shift_labels[valid_mask]
            self.train_perplexity(valid_logits, valid_labels)

        # ë¡œê¹…
        self.log("train_loss", loss, on_step=True, on_epoch=True, prog_bar=True, sync_dist=True)
        self.log("train_perplexity", self.train_perplexity, on_epoch=True, prog_bar=True, sync_dist=True)

        # í•™ìŠµë¥  ë¡œê¹…
        current_lr = self.trainer.optimizers[0].param_groups[0]['lr']
        self.log("learning_rate", current_lr, on_step=True, sync_dist=True)

        # ì¶œë ¥ ì €ì¥
        output = {
            "loss": loss,
            "logits": logits.detach(),
            "labels": batch["input_ids"].detach()
        }
        self.training_step_outputs.append(output)

        return loss

    def validation_step(self, batch: Dict[str, Tensor], batch_idx: int) -> Tensor:
        """ê²€ì¦ ìŠ¤í…"""
        # ìˆœì „íŒŒ
        outputs = self.forward(
            input_ids=batch["input_ids"],
            attention_mask=batch["attention_mask"]
        )

        loss = outputs["loss"]
        logits = outputs["logits"]

        # Perplexity ê³„ì‚°
        shift_logits = logits[..., :-1, :].contiguous()
        shift_labels = batch["input_ids"][..., 1:].contiguous()
        attention_mask = batch["attention_mask"][..., 1:].contiguous()

        valid_mask = attention_mask.bool()
        if valid_mask.sum() > 0:
            valid_logits = shift_logits[valid_mask]
            valid_labels = shift_labels[valid_mask]
            self.val_perplexity(valid_logits, valid_labels)

        # ë¡œê¹…
        self.log("val_loss", loss, on_epoch=True, prog_bar=True, sync_dist=True)
        self.log("val_perplexity", self.val_perplexity, on_epoch=True, prog_bar=True, sync_dist=True)

        # ì¶œë ¥ ì €ì¥
        output = {
            "loss": loss,
            "input_ids": batch["input_ids"].detach(),
            "attention_mask": batch["attention_mask"].detach()
        }
        self.validation_step_outputs.append(output)

        return loss

    def generate_text(
            self,
            prompt: str,
            max_new_tokens: Optional[int] = None,
            temperature: Optional[float] = None,
            top_k: Optional[int] = None,
            top_p: Optional[float] = None,
            do_sample: Optional[bool] = None,
            num_return_sequences: int = 1
    ) -> List[str]:
        """
        í…ìŠ¤íŠ¸ ìƒì„±

        Args:
            prompt: ìƒì„± ì‹œì‘ í”„ë¡¬í”„íŠ¸
            max_new_tokens: ìµœëŒ€ ìƒì„± í† í° ìˆ˜
            temperature: ìƒì„± ì˜¨ë„
            top_k: Top-k ìƒ˜í”Œë§
            top_p: Top-p (nucleus) ìƒ˜í”Œë§
            do_sample: ìƒ˜í”Œë§ ì—¬ë¶€
            num_return_sequences: ë°˜í™˜í•  ì‹œí€€ìŠ¤ ìˆ˜

        Returns:
            ìƒì„±ëœ í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
        """
        self.eval()

        # ì…ë ¥ í† í¬ë‚˜ì´ì§•
        inputs = self.tokenizer(
            prompt,
            return_tensors="pt",
            truncation=True,
            max_length=self.model_max_length - (max_new_tokens or self.max_new_tokens)
        ).to(self.device)

        # ìƒì„± ì„¤ì • ì˜¤ë²„ë¼ì´ë“œ
        generation_kwargs = {
            "max_new_tokens": max_new_tokens or self.max_new_tokens,
            "temperature": temperature or self.temperature,
            "top_k": top_k or self.top_k,
            "top_p": top_p or self.top_p,
            "do_sample": do_sample if do_sample is not None else self.do_sample,
            "num_return_sequences": num_return_sequences,
            "pad_token_id": self.tokenizer.pad_token_id or self.tokenizer.eos_token_id,
            "eos_token_id": self.tokenizer.eos_token_id,
        }

        with torch.no_grad():
            # ìƒì„±
            outputs = self.model.generate(
                **inputs,
                **generation_kwargs
            )

            # ë””ì½”ë”©
            generated_texts = []
            for output in outputs:
                # ì›ë˜ ì…ë ¥ ì œê±°
                new_tokens = output[inputs["input_ids"].shape[1]:]
                generated_text = self.tokenizer.decode(new_tokens, skip_special_tokens=True)
                generated_texts.append(generated_text)

        return generated_texts

    def on_validation_epoch_end(self):
        """ê²€ì¦ ì—í¬í¬ ì¢…ë£Œ ì‹œ ìƒ˜í”Œ ìƒì„±"""
        super().on_validation_epoch_end()

        # ìƒ˜í”Œ ìƒì„± ë° ë¡œê¹… (Wandbì— ìë™ ì „ì†¡)
        if hasattr(self.logger, 'experiment'):
            sample_prompts = [
                "The future of artificial intelligence is",
                "In a world where technology",
                "Once upon a time in a distant galaxy"
            ]

            try:
                import wandb

                # ê° í”„ë¡¬í”„íŠ¸ì— ëŒ€í•´ ìƒì„±
                generated_samples = []
                for prompt in sample_prompts:
                    generated_texts = self.generate_text(
                        prompt=prompt,
                        max_new_tokens=50,
                        temperature=0.8,
                        num_return_sequences=2
                    )

                    for i, text in enumerate(generated_texts):
                        generated_samples.append([prompt, f"Sample {i + 1}", text])

                # Wandb í…Œì´ë¸”ë¡œ ë¡œê¹…
                table = wandb.Table(
                    columns=["Prompt", "Sample", "Generated Text"],
                    data=generated_samples
                )

                self.logger.experiment.log({"generated_samples": table})

            except ImportError:
                logger.warning("Wandbë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ì–´ ìƒì„± ìƒ˜í”Œ ë¡œê¹…ì„ ê±´ë„ˆëœë‹ˆë‹¤.")

    def predict_step(self, batch: Dict[str, Tensor], batch_idx: int) -> Dict[str, Any]:
        """ì˜ˆì¸¡ ìŠ¤í… (ì¶”ë¡ ìš©)"""
        # ì…ë ¥ í…ìŠ¤íŠ¸ ë””ì½”ë”©
        prompts = [
            self.tokenizer.decode(ids, skip_special_tokens=True)
            for ids in batch["input_ids"]
        ]

        # ê° í”„ë¡¬í”„íŠ¸ì— ëŒ€í•´ ìƒì„±
        all_generated = []
        for prompt in prompts:
            generated = self.generate_text(prompt, num_return_sequences=1)
            all_generated.extend(generated)

        return {
            "prompts": prompts,
            "generated_texts": all_generated
        }


# ì‚¬ìš© ì˜ˆì‹œ
if __name__ == "__main__":
    # ìƒì„± ëª¨ë¸ í…ŒìŠ¤íŠ¸
    model = TextGenerationModule(
        model_name="gpt2",
        max_new_tokens=50,
        temperature=0.8,
        learning_rate=1e-4,
        use_peft=True,
        peft_config={"r": 8, "lora_alpha": 16}
    )

    print("ìƒì„± ëª¨ë¸ ì •ë³´:")
    for key, value in model.get_model_info().items():
        print(f"  {key}: {value}")

    # ìƒ˜í”Œ ìƒì„± í…ŒìŠ¤íŠ¸
    sample_prompts = [
        "The future of AI is",
        "In the year 2050,",
        "Machine learning will"
    ]

    print("\nìƒ˜í”Œ ìƒì„±:")
    for prompt in sample_prompts:
        generated = model.generate_text(prompt, max_new_tokens=30, temperature=0.8)
        print(f"Prompt: {prompt}")
        print(f"Generated: {generated[0]}")
        print()
```

## ğŸ’¾ LLM ë°ì´í„° ëª¨ë“ˆ êµ¬í˜„

LLM í•™ìŠµì„ ìœ„í•œ **íš¨ìœ¨ì ì¸ ë°ì´í„° ì²˜ë¦¬**ë¥¼ ë‹´ë‹¹í•˜ëŠ” DataModuleì„ êµ¬í˜„í•´ë³´ì.

```python
# src/data/base_datamodule.py
"""LLM í•™ìŠµì„ ìœ„í•œ ê¸°ë³¸ ë°ì´í„° ëª¨ë“ˆ"""

import pytorch_lightning as pl
from torch.utils.data import DataLoader, Dataset, random_split
from transformers import AutoTokenizer
from datasets import load_dataset, Dataset as HFDataset
from typing import Optional, Dict, Any, List, Union, Callable
import torch
import logging

logger = logging.getLogger(__name__)


class LLMDataModule(pl.LightningDataModule):
    """
    LLM í•™ìŠµì„ ìœ„í•œ ê¸°ë³¸ ë°ì´í„° ëª¨ë“ˆ

    Features:
    - Hugging Face Datasets í†µí•©
    - ìë™ í† í¬ë‚˜ì´ì§•
    - ë™ì  íŒ¨ë”©
    - ë©”ëª¨ë¦¬ íš¨ìœ¨ì  ì²˜ë¦¬
    """

    def __init__(
            self,
            tokenizer_name: str,
            dataset_name: Optional[str] = None,
            dataset_config: Optional[str] = None,
            train_split: str = "train",
            val_split: str = "validation",
            test_split: Optional[str] = "test",
            batch_size: int = 8,
            max_length: int = 512,
            num_workers: int = 4,
            pin_memory: bool = True,
            train_val_split: float = 0.9,
            streaming: bool = False,
            cache_dir: Optional[str] = None,
            max_samples: Optional[int] = None,  # ë””ë²„ê¹…ìš©
            **tokenizer_kwargs
    ):
        super().__init__()

        # ì„¤ì • ì €ì¥
        self.tokenizer_name = tokenizer_name
        self.dataset_name = dataset_name
        self.dataset_config = dataset_config
        self.train_split = train_split
        self.val_split = val_split
        self.test_split = test_split
        self.batch_size = batch_size
        self.max_length = max_length
        self.num_workers = num_workers
        self.pin_memory = pin_memory
        self.train_val_split = train_val_split
        self.streaming = streaming
        self.cache_dir = cache_dir
        self.max_samples = max_samples
        self.tokenizer_kwargs = tokenizer_kwargs

        # í† í¬ë‚˜ì´ì € ë¡œë“œ
        self._setup_tokenizer()

        # ë°ì´í„°ì…‹ ë³€ìˆ˜ ì´ˆê¸°í™”
        self.train_dataset = None
        self.val_dataset = None
        self.test_dataset = None

        logger.info(f"LLM ë°ì´í„° ëª¨ë“ˆ ì´ˆê¸°í™” ì™„ë£Œ")
        logger.info(f"í† í¬ë‚˜ì´ì €: {tokenizer_name}")
        logger.info(f"ë°ì´í„°ì…‹: {dataset_name}")
        logger.info(f"ë°°ì¹˜ í¬ê¸°: {batch_size}")
        logger.info(f"ìµœëŒ€ ê¸¸ì´: {max_length}")

    def _setup_tokenizer(self):
        """í† í¬ë‚˜ì´ì € ì„¤ì •"""
        self.tokenizer = AutoTokenizer.from_pretrained(
            self.tokenizer_name,
            cache_dir=self.cache_dir,
            **self.tokenizer_kwargs
        )

        # íŒ¨ë“œ í† í° ì„¤ì •
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
            logger.info("íŒ¨ë“œ í† í°ì„ EOS í† í°ìœ¼ë¡œ ì„¤ì •í–ˆìŠµë‹ˆë‹¤.")

        # ëª¨ë¸ ìµœëŒ€ ê¸¸ì´ ì„¤ì •
        if hasattr(self.tokenizer, 'model_max_length'):
            self.tokenizer.model_max_length = self.max_length

        logger.info(f"í† í¬ë‚˜ì´ì € ì„¤ì • ì™„ë£Œ: {self.tokenizer_name}")
        logger.info(f"ì–´íœ˜ í¬ê¸°: {len(self.tokenizer)}")
        logger.info(f"íŒ¨ë“œ í† í°: {self.tokenizer.pad_token}")

    def prepare_data(self):
        """ë°ì´í„° ë‹¤ìš´ë¡œë“œ (ë©€í‹° í”„ë¡œì„¸ìŠ¤ í™˜ê²½ì—ì„œ í•œ ë²ˆë§Œ ì‹¤í–‰)"""
        if self.dataset_name:
            try:
                load_dataset(
                    self.dataset_name,
                    self.dataset_config,
                    cache_dir=self.cache_dir,
                    streaming=self.streaming
                )
                logger.info(f"ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {self.dataset_name}")
            except Exception as e:
                logger.error(f"ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}")
                raise

    def setup(self, stage: Optional[str] = None):
        """ë°ì´í„°ì…‹ ì„¤ì • (ê° í”„ë¡œì„¸ìŠ¤ì—ì„œ ì‹¤í–‰)"""
        if self.dataset_name is None:
            logger.warning("ë°ì´í„°ì…‹ì´ ì§€ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì»¤ìŠ¤í…€ ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.")
            return

        try:
            # ë°ì´í„°ì…‹ ë¡œë“œ
            dataset = load_dataset(
                self.dataset_name,
                self.dataset_config,
                cache_dir=self.cache_dir,
                streaming=self.streaming
            )

            if stage == "fit" or stage is None:
                # í›ˆë ¨/ê²€ì¦ ë°ì´í„° ì„¤ì •
                if self.val_split in dataset:
                    train_data = dataset[self.train_split]
                    val_data = dataset[self.val_split]
                else:
                    # ê²€ì¦ ë°ì´í„°ê°€ ì—†ìœ¼ë©´ í›ˆë ¨ ë°ì´í„°ì—ì„œ ë¶„í• 
                    full_train = dataset[self.train_split]

                    if self.streaming:
                        # ìŠ¤íŠ¸ë¦¬ë° ë°ì´í„°ëŠ” ë¶„í• í•˜ì§€ ì•Šê³  ê·¸ëŒ€ë¡œ ì‚¬ìš©
                        train_data = full_train
                        val_data = full_train  # ë™ì¼í•œ ë°ì´í„° ì‚¬ìš© (ì£¼ì˜: ì‹¤ì œë¡œëŠ” ë³„ë„ ë°ì´í„° í•„ìš”)
                        logger.warning("ìŠ¤íŠ¸ë¦¬ë° ëª¨ë“œì—ì„œëŠ” ì ì ˆí•œ ê²€ì¦ ë°ì´í„° ë¶„í• ì´ ì–´ë µìŠµë‹ˆë‹¤.")
                    else:
                        # ë©”ëª¨ë¦¬ì— ë¡œë“œí•˜ì—¬ ë¶„í• 
                        total_size = len(full_train)
                        train_size = int(total_size * self.train_val_split)
                        val_size = total_size - train_size

                        train_data, val_data = random_split(
                            full_train, [train_size, val_size]
                        )

                # ìƒ˜í”Œ ìˆ˜ ì œí•œ (ë””ë²„ê¹…ìš©)
                if self.max_samples and not self.streaming:
                    train_data = train_data.select(range(min(self.max_samples, len(train_data))))
                    val_data = val_data.select(range(min(self.max_samples // 5, len(val_data))))

                # í† í¬ë‚˜ì´ì§• ì ìš©
                self.train_dataset = self._tokenize_dataset(train_data, "train")
                self.val_dataset = self._tokenize_dataset(val_data, "validation")

                logger.info(
                    f"í›ˆë ¨ ë°ì´í„° í¬ê¸°: {len(self.train_dataset) if hasattr(self.train_dataset, '__len__') else 'Unknown (streaming)'}")
                logger.info(
                    f"ê²€ì¦ ë°ì´í„° í¬ê¸°: {len(self.val_dataset) if hasattr(self.val_dataset, '__len__') else 'Unknown (streaming)'}")

            if stage == "test" or stage is None:
                # í…ŒìŠ¤íŠ¸ ë°ì´í„° ì„¤ì •
                if self.test_split and self.test_split in dataset:
                    test_data = dataset[self.test_split]

                    if self.max_samples and not self.streaming:
                        test_data = test_data.select(range(min(self.max_samples // 5, len(test_data))))

                    self.test_dataset = self._tokenize_dataset(test_data, "test")
                    logger.info(
                        f"í…ŒìŠ¤íŠ¸ ë°ì´í„° í¬ê¸°: {len(self.test_dataset) if hasattr(self.test_dataset, '__len__') else 'Unknown (streaming)'}")

        except Exception as e:
            logger.error(f"ë°ì´í„°ì…‹ ì„¤ì • ì‹¤íŒ¨: {e}")
            raise

    def _tokenize_dataset(self, dataset: Union[HFDataset, Dataset], split: str) -> HFDataset:
        """
        ë°ì´í„°ì…‹ í† í¬ë‚˜ì´ì§• - í•˜ìœ„ í´ë˜ìŠ¤ì—ì„œ ì˜¤ë²„ë¼ì´ë“œ

        Args:
            dataset: ì›ë³¸ ë°ì´í„°ì…‹
            split: ë°ì´í„° ë¶„í•  ì´ë¦„ ("train", "validation", "test")

        Returns:
            í† í¬ë‚˜ì´ì§•ëœ ë°ì´í„°ì…‹
        """
        raise NotImplementedError("í•˜ìœ„ í´ë˜ìŠ¤ì—ì„œ êµ¬í˜„í•´ì•¼ í•©ë‹ˆë‹¤.")

    def _collate_fn(self, batch: List[Dict[str, Any]]) -> Dict[str, torch.Tensor]:
        """
        ë°°ì¹˜ ì½œë ˆì´ì…˜ í•¨ìˆ˜ - ë™ì  íŒ¨ë”© ì ìš©

        Args:
            batch: ë°°ì¹˜ ë°ì´í„° ë¦¬ìŠ¤íŠ¸

        Returns:
            íŒ¨ë”©ëœ ë°°ì¹˜ í…ì„œ
        """
        # ë°°ì¹˜ì—ì„œ í‚¤ ì¶”ì¶œ
        keys = batch[0].keys()

        # í…ì„œë¡œ ë³€í™˜
        batch_dict = {}
        for key in keys:
            if key in ["input_ids", "attention_mask", "labels"]:
                # ì‹œí€€ìŠ¤ ë°ì´í„°ëŠ” íŒ¨ë”© ì ìš©
                sequences = [item[key] for item in batch]

                # íŒ¨ë”©
                if isinstance(sequences[0], torch.Tensor):
                    batch_dict[key] = torch.nn.utils.rnn.pad_sequence(
                        sequences, batch_first=True, padding_value=self.tokenizer.pad_token_id
                    )
                else:
                    # í† í¬ë‚˜ì´ì €ë¥¼ ì‚¬ìš©í•œ íŒ¨ë”©
                    batch_dict[key] = self.tokenizer.pad(
                        {"input_ids": sequences},
                        padding=True,
                        return_tensors="pt"
                    )["input_ids"]
            else:
                # ë‹¤ë¥¸ ë°ì´í„°ëŠ” ìŠ¤íƒ
                batch_dict[key] = torch.stack([item[key] for item in batch])

        return batch_dict

    def train_dataloader(self) -> DataLoader:
        """í›ˆë ¨ ë°ì´í„°ë¡œë”"""
        if self.train_dataset is None:
            raise ValueError("í›ˆë ¨ ë°ì´í„°ì…‹ì´ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. setup()ì„ ë¨¼ì € í˜¸ì¶œí•˜ì„¸ìš”.")

        return DataLoader(
            self.train_dataset,
            batch_size=self.batch_size,
            shuffle=True,  # ìŠ¤íŠ¸ë¦¬ë° ëª¨ë“œì—ì„œëŠ” ìë™ìœ¼ë¡œ False
            num_workers=self.num_workers,
            pin_memory=self.pin_memory,
            collate_fn=self._collate_fn,
            persistent_workers=True if self.num_workers > 0 else False
        )

    def val_dataloader(self) -> DataLoader:
        """ê²€ì¦ ë°ì´í„°ë¡œë”"""
        if self.val_dataset is None:
            raise ValueError("ê²€ì¦ ë°ì´í„°ì…‹ì´ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. setup()ì„ ë¨¼ì € í˜¸ì¶œí•˜ì„¸ìš”.")

        return DataLoader(
            self.val_dataset,
            batch_size=self.batch_size,
            shuffle=False,
            num_workers=self.num_workers,
            pin_memory=self.pin_memory,
            collate_fn=self._collate_fn,
            persistent_workers=True if self.num_workers > 0 else False
        )

    def test_dataloader(self) -> DataLoader:
        """í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¡œë”"""
        if self.test_dataset is None:
            raise ValueError("í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ì´ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

        return DataLoader(
            self.test_dataset,
            batch_size=self.batch_size,
            shuffle=False,
            num_workers=self.num_workers,
            pin_memory=self.pin_memory,
            collate_fn=self._collate_fn
        )

    def get_sample_data(self, split: str = "train", num_samples: int = 3) -> List[Dict[str, Any]]:
        """
        ìƒ˜í”Œ ë°ì´í„° ë°˜í™˜ (ë””ë²„ê¹…ìš©)

        Args:
            split: ë°ì´í„° ë¶„í•  ("train", "validation", "test")
            num_samples: ìƒ˜í”Œ ìˆ˜

        Returns:
            ìƒ˜í”Œ ë°ì´í„° ë¦¬ìŠ¤íŠ¸
        """
        if split == "train" and self.train_dataset:
            dataset = self.train_dataset
        elif split == "validation" and self.val_dataset:
            dataset = self.val_dataset
        elif split == "test" and self.test_dataset:
            dataset = self.test_dataset
        else:
            raise ValueError(f"'{split}' ë°ì´í„°ì…‹ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

        samples = []
        for i in range(min(num_samples, len(dataset))):
            sample = dataset[i]

            # í† í°ì„ í…ìŠ¤íŠ¸ë¡œ ë””ì½”ë”©
            if "input_ids" in sample:
                sample["decoded_text"] = self.tokenizer.decode(
                    sample["input_ids"],
                    skip_special_tokens=True
                )

            samples.append(sample)

        return samples


# í…ìŠ¤íŠ¸ ë¶„ë¥˜ë¥¼ ìœ„í•œ êµ¬ì²´ì ì¸ ë°ì´í„° ëª¨ë“ˆ
class TextClassificationDataModule(LLMDataModule):
    """í…ìŠ¤íŠ¸ ë¶„ë¥˜ë¥¼ ìœ„í•œ ë°ì´í„° ëª¨ë“ˆ"""

    def __init__(
            self,
            text_column: str = "text",
            label_column: str = "label",
            **kwargs
    ):
        super().__init__(**kwargs)
        self.text_column = text_column
        self.label_column = label_column

    def _tokenize_dataset(self, dataset: HFDataset, split: str) -> HFDataset:
        """í…ìŠ¤íŠ¸ ë¶„ë¥˜ìš© í† í¬ë‚˜ì´ì§•"""

        def tokenize_function(examples):
            # í…ìŠ¤íŠ¸ í† í¬ë‚˜ì´ì§•
            tokenized = self.tokenizer(
                examples[self.text_column],
                truncation=True,
                padding=False,  # ë°°ì¹˜ì—ì„œ ë™ì  íŒ¨ë”©
                max_length=self.max_length,
                return_tensors=None  # ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜
            )

            # ë ˆì´ë¸” ì¶”ê°€
            if self.label_column in examples:
                tokenized["labels"] = examples[self.label_column]

            return tokenized

        # í† í¬ë‚˜ì´ì§• ì ìš©
        tokenized_dataset = dataset.map(
            tokenize_function,
            batched=True,
            remove_columns=dataset.column_names if hasattr(dataset, 'column_names') else []
        )

        return tokenized_dataset


# í…ìŠ¤íŠ¸ ìƒì„±ì„ ìœ„í•œ êµ¬ì²´ì ì¸ ë°ì´í„° ëª¨ë“ˆ
class TextGenerationDataModule(LLMDataModule):
    """í…ìŠ¤íŠ¸ ìƒì„±ì„ ìœ„í•œ ë°ì´í„° ëª¨ë“ˆ"""

    def __init__(
            self,
            text_column: str = "text",
            **kwargs
    ):
        super().__init__(**kwargs)
        self.text_column = text_column

    def _tokenize_dataset(self, dataset: HFDataset, split: str) -> HFDataset:
        """í…ìŠ¤íŠ¸ ìƒì„±ìš© í† í¬ë‚˜ì´ì§•"""

        def tokenize_function(examples):
            # í…ìŠ¤íŠ¸ í† í¬ë‚˜ì´ì§•
            tokenized = self.tokenizer(
                examples[self.text_column],
                truncation=True,
                padding=False,
                max_length=self.max_length,
                return_tensors=None
            )

            # Causal LMì—ì„œëŠ” input_idsì™€ labelsê°€ ë™ì¼
            tokenized["labels"] = tokenized["input_ids"].copy()

            return tokenized

        # í† í¬ë‚˜ì´ì§• ì ìš©
        tokenized_dataset = dataset.map(
            tokenize_function,
            batched=True,
            remove_columns=dataset.column_names if hasattr(dataset, 'column_names') else []
        )

        return tokenized_dataset


# ì‚¬ìš© ì˜ˆì‹œ
if __name__ == "__main__":
    # ë¶„ë¥˜ ë°ì´í„° ëª¨ë“ˆ í…ŒìŠ¤íŠ¸
    data_module = TextClassificationDataModule(
        tokenizer_name="distilbert-base-uncased",
        dataset_name="imdb",
        batch_size=8,
        max_length=256,
        max_samples=100  # í…ŒìŠ¤íŠ¸ìš©
    )

    # ë°ì´í„° ì¤€ë¹„
    data_module.prepare_data()
    data_module.setup("fit")

    # ìƒ˜í”Œ ë°ì´í„° í™•ì¸
    samples = data_module.get_sample_data("train", num_samples=2)
    for i, sample in enumerate(samples):
        print(f"ìƒ˜í”Œ {i + 1}:")
        print(f"  í…ìŠ¤íŠ¸: {sample['decoded_text'][:100]}...")
        print(f"  ë ˆì´ë¸”: {sample['labels']}")
        print()

    # ë°ì´í„°ë¡œë” í…ŒìŠ¤íŠ¸
    train_loader = data_module.train_dataloader()
    batch = next(iter(train_loader))

    print("ë°°ì¹˜ ì •ë³´:")
    for key, value in batch.items():
        if isinstance(value, torch.Tensor):
            print(f"  {key}: {value.shape}")
        else:
            print(f"  {key}: {type(value)}")
```

## ğŸ”§ ì»¤ìŠ¤í…€ ì½œë°± êµ¬í˜„

LLM í•™ìŠµì„ íš¨ê³¼ì ìœ¼ë¡œ ëª¨ë‹ˆí„°ë§í•˜ê³  ì œì–´í•˜ê¸° ìœ„í•œ **ì»¤ìŠ¤í…€ ì½œë°±ë“¤**ì„ êµ¬í˜„í•´ë³´ì.

```python
# src/callbacks/model_monitoring.py
"""LLM ëª¨ë¸ ëª¨ë‹ˆí„°ë§ì„ ìœ„í•œ ì»¤ìŠ¤í…€ ì½œë°±ë“¤"""

import pytorch_lightning as pl
from pytorch_lightning.callbacks import Callback
import torch
import torch.nn.functional as F
import numpy as np
from typing import Any, Dict, List, Optional, Union
import logging
import time
import psutil
import GPUtil

logger = logging.getLogger(__name__)


class LLMModelMonitoringCallback(Callback):
    """
    LLM ëª¨ë¸ ì„±ëŠ¥ê³¼ ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ëŸ‰ì„ ëª¨ë‹ˆí„°ë§í•˜ëŠ” ì½œë°±

    Features:
    - GPU/CPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¶”ì 
    - ê·¸ë˜ë””ì–¸íŠ¸ ë…¸ë¦„ ëª¨ë‹ˆí„°ë§
    - ê°€ì¤‘ì¹˜ ë¶„í¬ ì¶”ì 
    - í•™ìŠµ ì†ë„ ì¸¡ì •
    """

    def __init__(
            self,
            log_every_n_steps: int = 100,
            monitor_gradients: bool = True,
            monitor_weights: bool = True,
            monitor_resources: bool = True,
            gradient_clip_threshold: float = 1.0
    ):
        super().__init__()
        self.log_every_n_steps = log_every_n_steps
        self.monitor_gradients = monitor_gradients
        self.monitor_weights = monitor_weights
        self.monitor_resources = monitor_resources
        self.gradient_clip_threshold = gradient_clip_threshold

        # ì‹œê°„ ì¶”ì 
        self.batch_start_time = None
        self.epoch_start_time = None

        # ë©”íŠ¸ë¦­ ëˆ„ì 
        self.step_times = []
        self.gradient_norms = []

    def on_train_epoch_start(self, trainer: pl.Trainer, pl_module: pl.LightningModule) -> None:
        """ì—í¬í¬ ì‹œì‘ ì‹œ ì‹œê°„ ê¸°ë¡"""
        self.epoch_start_time = time.time()
        self.step_times = []
        self.gradient_norms = []

        logger.info(f"ì—í¬í¬ {trainer.current_epoch + 1} ì‹œì‘")

    def on_train_batch_start(
            self,
            trainer: pl.Trainer,
            pl_module: pl.LightningModule,
            batch: Any,
            batch_idx: int
    ) -> None:
        """ë°°ì¹˜ ì‹œì‘ ì‹œ ì‹œê°„ ê¸°ë¡"""
        self.batch_start_time = time.time()

    def on_train_batch_end(
            self,
            trainer: pl.Trainer,
            pl_module: pl.LightningModule,
            outputs: Any,
            batch: Any,
            batch_idx: int
    ) -> None:
        """ë°°ì¹˜ ì¢…ë£Œ ì‹œ ëª¨ë‹ˆí„°ë§"""
        # ë°°ì¹˜ ì²˜ë¦¬ ì‹œê°„ ê³„ì‚°
        if self.batch_start_time:
            batch_time = time.time() - self.batch_start_time
            self.step_times.append(batch_time)

        # ì£¼ê¸°ì  ë¡œê¹…
        if batch_idx % self.log_every_n_steps == 0:
            self._log_monitoring_metrics(trainer, pl_module, batch_idx)

    def on_after_backward(self, trainer: pl.Trainer, pl_module: pl.LightningModule) -> None:
        """ë°±í”„ë¡­ í›„ ê·¸ë˜ë””ì–¸íŠ¸ ëª¨ë‹ˆí„°ë§"""
        if self.monitor_gradients:
            self._monitor_gradients(trainer, pl_module)

    def on_train_epoch_end(self, trainer: pl.Trainer, pl_module: pl.LightningModule) -> None:
        """ì—í¬í¬ ì¢…ë£Œ ì‹œ ìš”ì•½ ë¡œê¹…"""
        if self.epoch_start_time:
            epoch_time = time.time() - self.epoch_start_time

            # í‰ê·  ë°°ì¹˜ ì‹œê°„
            avg_batch_time = np.mean(self.step_times) if self.step_times else 0

            # ë¡œê¹…
            pl_module.log("epoch_time_minutes", epoch_time / 60, sync_dist=True)
            pl_module.log("avg_batch_time_seconds", avg_batch_time, sync_dist=True)

            # ê·¸ë˜ë””ì–¸íŠ¸ ë…¸ë¦„ í†µê³„
            if self.gradient_norms:
                pl_module.log("avg_gradient_norm", np.mean(self.gradient_norms), sync_dist=True)
                pl_module.log("max_gradient_norm", np.max(self.gradient_norms), sync_dist=True)

            logger.info(f"ì—í¬í¬ {trainer.current_epoch + 1} ì™„ë£Œ - ì†Œìš” ì‹œê°„: {epoch_time / 60:.2f}ë¶„")

    def _log_monitoring_metrics(
            self,
            trainer: pl.Trainer,
            pl_module: pl.LightningModule,
            batch_idx: int
    ) -> None:
        """ëª¨ë‹ˆí„°ë§ ë©”íŠ¸ë¦­ ë¡œê¹…"""
        metrics = {}

        # ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§
        if self.monitor_resources:
            # GPU ë©”ëª¨ë¦¬
            if torch.cuda.is_available():
                gpu_memory_allocated = torch.cuda.memory_allocated() / 1024 ** 3  # GB
                gpu_memory_reserved = torch.cuda.memory_reserved() / 1024 ** 3  # GB
                gpu_memory_free = torch.cuda.memory_reserved() - torch.cuda.memory_allocated()
                gpu_memory_free = gpu_memory_free / 1024 ** 3  # GB

                metrics.update({
                    "gpu_memory_allocated_gb": gpu_memory_allocated,
                    "gpu_memory_reserved_gb": gpu_memory_reserved,
                    "gpu_memory_free_gb": gpu_memory_free,
                })

                # GPU ì‚¬ìš©ë¥  (GPUtil ì‚¬ìš©)
                try:
                    gpus = GPUtil.getGPUs()
                    if gpus:
                        gpu = gpus[0]  # ì²« ë²ˆì§¸ GPU
                        metrics.update({
                            "gpu_utilization_percent": gpu.load * 100,
                            "gpu_temperature_celsius": gpu.temperature,
                        })
                except:
                    pass

            # CPU ë©”ëª¨ë¦¬
            cpu_memory = psutil.virtual_memory()
            metrics.update({
                "cpu_memory_used_gb": cpu_memory.used / 1024 ** 3,
                "cpu_memory_percent": cpu_memory.percent,
                "cpu_utilization_percent": psutil.cpu_percent(),
            })

        # ê°€ì¤‘ì¹˜ ëª¨ë‹ˆí„°ë§
        if self.monitor_weights:
            weight_stats = self._get_weight_statistics(pl_module)
            metrics.update(weight_stats)

        # ë°°ì¹˜ ì²˜ë¦¬ ì†ë„
        if self.step_times:
            recent_times = self.step_times[-self.log_every_n_steps:]
            metrics["recent_avg_batch_time"] = np.mean(recent_times)
            metrics["throughput_samples_per_second"] = trainer.datamodule.batch_size / np.mean(recent_times)

        # ë¡œê¹…
        for key, value in metrics.items():
            pl_module.log(key, value, on_step=True, sync_dist=True)

    def _monitor_gradients(self, trainer: pl.Trainer, pl_module: pl.LightningModule) -> None:
        """ê·¸ë˜ë””ì–¸íŠ¸ ëª¨ë‹ˆí„°ë§"""
        total_norm = 0.0
        param_count = 0

        for name, param in pl_module.named_parameters():
            if param.grad is not None:
                param_norm = param.grad.data.norm(2)
                total_norm += param_norm.item() ** 2
                param_count += 1

        if param_count > 0:
            total_norm = total_norm ** (1. / 2)
            self.gradient_norms.append(total_norm)

            # ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘ ê²½ê³ 
            if total_norm > self.gradient_clip_threshold:
                logger.warning(f"ë†’ì€ ê·¸ë˜ë””ì–¸íŠ¸ ë…¸ë¦„ ê°ì§€: {total_norm:.4f}")

            # ì£¼ê¸°ì  ë¡œê¹…
            if trainer.global_step % self.log_every_n_steps == 0:
                pl_module.log("gradient_norm", total_norm, on_step=True, sync_dist=True)

    def _get_weight_statistics(self, pl_module: pl.LightningModule) -> Dict[str, float]:
        """ê°€ì¤‘ì¹˜ í†µê³„ ê³„ì‚°"""
        stats = {}

        # ì „ì²´ ê°€ì¤‘ì¹˜ í†µê³„
        all_weights = []
        for name, param in pl_module.named_parameters():
            if param.requires_grad and 'weight' in name:
                all_weights.append(param.data.flatten())

        if all_weights:
            all_weights = torch.cat(all_weights)
            stats.update({
                "weight_mean": all_weights.mean().item(),
                "weight_std": all_weights.std().item(),
                "weight_min": all_weights.min().item(),
                "weight_max": all_weights.max().item(),
            })

        # ë ˆì´ì–´ë³„ í†µê³„ (ì£¼ìš” ë ˆì´ì–´ë§Œ)
        for name, param in pl_module.named_parameters():
            if param.requires_grad and any(layer in name for layer in ['embed', 'lm_head', 'classifier']):
                layer_name = name.split('.')[0]  # ì²« ë²ˆì§¸ ë¶€ë¶„ë§Œ ì‚¬ìš©
                stats[f"{layer_name}_weight_norm"] = param.data.norm().item()

        return stats


class TextGenerationCallback(Callback):
    """
    í…ìŠ¤íŠ¸ ìƒì„± ëª¨ë¸ì„ ìœ„í•œ ì½œë°±

    Features:
    - ì£¼ê¸°ì  ìƒ˜í”Œ ìƒì„±
    - ìƒì„± í’ˆì§ˆ ëª¨ë‹ˆí„°ë§
    - í† í° ë¶„í¬ ë¶„ì„
    """

    def __init__(
            self,
            sample_prompts: List[str],
            generate_every_n_epochs: int = 1,
            max_new_tokens: int = 50,
            num_samples_per_prompt: int = 2,
            temperature: float = 0.8,
            top_k: int = 50,
            top_p: float = 0.9
    ):
        super().__init__()
        self.sample_prompts = sample_prompts
        self.generate_every_n_epochs = generate_every_n_epochs
        self.max_new_tokens = max_new_tokens
        self.num_samples_per_prompt = num_samples_per_prompt
        self.temperature = temperature
        self.top_k = top_k
        self.top_p = top_p

    def on_validation_epoch_end(self, trainer: pl.Trainer, pl_module: pl.LightningModule) -> None:
        """ê²€ì¦ ì—í¬í¬ ì¢…ë£Œ ì‹œ ìƒ˜í”Œ ìƒì„±"""
        if (trainer.current_epoch + 1) % self.generate_every_n_epochs == 0:
            self._generate_and_log_samples(trainer, pl_module)

    def _generate_and_log_samples(self, trainer: pl.Trainer, pl_module: pl.LightningModule) -> None:
        """ìƒ˜í”Œ ìƒì„± ë° ë¡œê¹…"""
        if not hasattr(pl_module, 'generate_text'):
            logger.warning("ëª¨ë¸ì— generate_text ë©”ì„œë“œê°€ ì—†ìŠµë‹ˆë‹¤.")
            return

        pl_module.eval()

        try:
            import wandb

            # ê° í”„ë¡¬í”„íŠ¸ì— ëŒ€í•´ ìƒì„±
            all_samples = []
            for prompt in self.sample_prompts:
                generated_texts = pl_module.generate_text(
                    prompt=prompt,
                    max_new_tokens=self.max_new_tokens,
                    temperature=self.temperature,
                    top_k=self.top_k,
                    top_p=self.top_p,
                    num_return_sequences=self.num_samples_per_prompt
                )

                for i, text in enumerate(generated_texts):
                    all_samples.append([
                        f"Epoch {trainer.current_epoch + 1}",
                        prompt,
                        f"Sample {i + 1}",
                        text
                    ])

            # Wandb í…Œì´ë¸”ë¡œ ë¡œê¹…
            if hasattr(trainer.logger, 'experiment'):
                table = wandb.Table(
                    columns=["Epoch", "Prompt", "Sample", "Generated Text"],
                    data=all_samples
                )
                trainer.logger.experiment.log({
                    f"generated_samples_epoch_{trainer.current_epoch + 1}": table
                })

            logger.info(f"ì—í¬í¬ {trainer.current_epoch + 1}: {len(all_samples)}ê°œ ìƒ˜í”Œ ìƒì„± ì™„ë£Œ")

        except ImportError:
            logger.warning("Wandbë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ì–´ ìƒì„± ìƒ˜í”Œ ë¡œê¹…ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
        except Exception as e:
            logger.error(f"ìƒ˜í”Œ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

        pl_module.train()


class ModelSizeCallback(Callback):
    """
    ëª¨ë¸ í¬ê¸°ì™€ íŒŒë¼ë¯¸í„° ì •ë³´ë¥¼ ë¡œê¹…í•˜ëŠ” ì½œë°±
    """

    def on_fit_start(self, trainer: pl.Trainer, pl_module: pl.LightningModule) -> None:
        """í•™ìŠµ ì‹œì‘ ì‹œ ëª¨ë¸ ì •ë³´ ë¡œê¹…"""
        # ëª¨ë¸ í¬ê¸° ì •ë³´
        total_params = sum(p.numel() for p in pl_module.parameters())
        trainable_params = sum(p.numel() for p in pl_module.parameters() if p.requires_grad)
        frozen_params = total_params - trainable_params

        # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ (ëŒ€ëµì  ê³„ì‚°)
        model_size_mb = total_params * 4 / 1024 / 1024  # float32 ê¸°ì¤€

        # ë¡œê¹…
        metrics = {
            "model_total_parameters": total_params,
            "model_trainable_parameters": trainable_params,
            "model_frozen_parameters": frozen_params,
            "model_size_mb": model_size_mb,
            "trainable_parameter_ratio": trainable_params / total_params if total_params > 0 else 0
        }

        for key, value in metrics.items():
            pl_module.log(key, value)

        # ì½˜ì†” ì¶œë ¥
        logger.info("=" * 50)
        logger.info("ëª¨ë¸ ì •ë³´")
        logger.info("=" * 50)
        logger.info(f"ì´ íŒŒë¼ë¯¸í„°: {total_params:,}")
        logger.info(f"í›ˆë ¨ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„°: {trainable_params:,}")
        logger.info(f"ê³ ì •ëœ íŒŒë¼ë¯¸í„°: {frozen_params:,}")
        logger.info(f"ëª¨ë¸ í¬ê¸°: {model_size_mb:.2f} MB")
        logger.info(f"í›ˆë ¨ ê°€ëŠ¥ ë¹„ìœ¨: {trainable_params / total_params * 100:.2f}%")
        logger.info("=" * 50)

        # PEFT ì •ë³´ (ì‚¬ìš© ì¤‘ì¸ ê²½ìš°)
        if hasattr(pl_module, 'use_peft') and pl_module.use_peft:
            if hasattr(pl_module.model, 'print_trainable_parameters'):
                logger.info("PEFT íŒŒë¼ë¯¸í„° ì •ë³´:")
                pl_module.model.print_trainable_parameters()


# ì‚¬ìš© ì˜ˆì‹œ
if __name__ == "__main__":
    # ì½œë°± í…ŒìŠ¤íŠ¸
    monitoring_callback = LLMModelMonitoringCallback(
        log_every_n_steps=50,
        monitor_gradients=True,
        monitor_weights=True,
        monitor_resources=True
    )

    generation_callback = TextGenerationCallback(
        sample_prompts=[
            "The future of AI is",
            "In a world where technology",
            "Once upon a time"
        ],
        generate_every_n_epochs=2,
        max_new_tokens=30
    )

    model_size_callback = ModelSizeCallback()

    print("ì»¤ìŠ¤í…€ ì½œë°±ë“¤ì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤:")
    print(f"- ëª¨ë‹ˆí„°ë§ ì½œë°±: {monitoring_callback.__class__.__name__}")
    print(f"- ìƒì„± ì½œë°±: {generation_callback.__class__.__name__}")
    print(f"- ëª¨ë¸ í¬ê¸° ì½œë°±: {model_size_callback.__class__.__name__}")
```

## ğŸ“Š Wandb Logger í†µí•©ê³¼ ì‹¤í—˜ ì¶”ì 

LLM ì‹¤í—˜ì„ ì²´ê³„ì ìœ¼ë¡œ ê´€ë¦¬í•˜ê¸° ìœ„í•œ **Wandb í†µí•©**ì„ êµ¬í˜„í•´ë³´ì.

```python
# src/utils/wandb_integration.py
"""Wandbë¥¼ í™œìš©í•œ ì‹¤í—˜ ì¶”ì  ë° ê´€ë¦¬"""

import os
import wandb
import pytorch_lightning as pl
from pytorch_lightning.loggers import WandbLogger
from pytorch_lightning.callbacks import Callback
from typing import Dict, Any, Optional, List, Union
import torch
import json
import logging
from pathlib import Path

logger = logging.getLogger(__name__)


class EnhancedWandbLogger(WandbLogger):
    """
    Wandb Logger í™•ì¥ í´ë˜ìŠ¤

    Features:
    - í•˜ì´í¼íŒŒë¼ë¯¸í„° ìë™ ë¡œê¹…
    - ëª¨ë¸ ì•„í‹°íŒ©íŠ¸ ê´€ë¦¬
    - ì½”ë“œ ìŠ¤ëƒ…ìƒ· ì €ì¥
    - í™˜ê²½ ì •ë³´ ê¸°ë¡
    """

    def __init__(
            self,
            project: str,
            entity: Optional[str] = None,
            name: Optional[str] = None,
            tags: Optional[List[str]] = None,
            notes: Optional[str] = None,
            config: Optional[Dict[str, Any]] = None,
            save_code: bool = True,
            save_artifacts: bool = True,
            **kwargs
    ):
        super().__init__(
            project=project,
            entity=entity,
            name=name,
            tags=tags,
            notes=notes,
            **kwargs
        )

        self.save_code = save_code
        self.save_artifacts = save_artifacts
        self._config = config or {}

        # í™˜ê²½ ì •ë³´ ìˆ˜ì§‘
        self._log_environment_info()

        logger.info(f"Enhanced Wandb Logger ì´ˆê¸°í™” ì™„ë£Œ")
        logger.info(f"í”„ë¡œì íŠ¸: {project}")
        logger.info(f"ì‹¤í–‰ ì´ë¦„: {name or 'auto-generated'}")

    def _log_environment_info(self):
        """í™˜ê²½ ì •ë³´ ë¡œê¹…"""
        env_info = {
            "python_version": ".".join(map(str, __import__("sys").version_info[:3])),
            "pytorch_version": torch.__version__,
            "cuda_available": torch.cuda.is_available(),
            "cuda_version": torch.version.cuda if torch.cuda.is_available() else None,
            "gpu_count": torch.cuda.device_count() if torch.cuda.is_available() else 0,
            "gpu_names": [torch.cuda.get_device_name(i) for i in
                          range(torch.cuda.device_count())] if torch.cuda.is_available() else [],
        }

        # GPU ë©”ëª¨ë¦¬ ì •ë³´
        if torch.cuda.is_available():
            gpu_memory_info = []
            for i in range(torch.cuda.device_count()):
                memory_total = torch.cuda.get_device_properties(i).total_memory / 1024 ** 3
                gpu_memory_info.append(f"GPU {i}: {memory_total:.1f} GB")
            env_info["gpu_memory"] = gpu_memory_info

        # ì„¤ì •ì— ì¶”ê°€
        self._config.update({"environment": env_info})

    @property
    def experiment(self) -> wandb.sdk.wandb_run.Run:
        """Wandb run ê°ì²´ ë°˜í™˜"""
        if self._experiment is None:
            self._experiment = self._get_experiment()

            # ì´ˆê¸° ì„¤ì • ë¡œê¹…
            if self._config:
                self._experiment.config.update(self._config)

            # ì½”ë“œ ì €ì¥
            if self.save_code:
                self._save_code_snapshot()

        return self._experiment

    def _save_code_snapshot(self):
        """ì½”ë“œ ìŠ¤ëƒ…ìƒ· ì €ì¥"""
        try:
            # í˜„ì¬ ë””ë ‰í† ë¦¬ì˜ Python íŒŒì¼ë“¤ ì €ì¥
            code_dir = Path(".")
            python_files = list(code_dir.rglob("*.py"))

            # ì£¼ìš” íŒŒì¼ë“¤ë§Œ ì €ì¥ (ìš©ëŸ‰ ì œí•œ)
            important_files = []
            for file_path in python_files:
                # ì œì™¸í•  ë””ë ‰í† ë¦¬ë“¤
                exclude_dirs = {'.git', '__pycache__', '.pytest_cache', 'outputs', 'wandb'}
                if not any(exclude_dir in file_path.parts for exclude_dir in exclude_dirs):
                    important_files.append(str(file_path))

            # Wandbì— ì½”ë“œ ì €ì¥
            if important_files:
                wandb.save(important_files[:20])  # ìµœëŒ€ 20ê°œ íŒŒì¼
                logger.info(f"{len(important_files)} ê°œì˜ Python íŒŒì¼ì„ Wandbì— ì €ì¥í–ˆìŠµë‹ˆë‹¤.")

        except Exception as e:
            logger.warning(f"ì½”ë“œ ìŠ¤ëƒ…ìƒ· ì €ì¥ ì‹¤íŒ¨: {e}")

    def log_model_artifact(
            self,
            model_path: str,
            name: str,
            version: Optional[str] = None,
            metadata: Optional[Dict[str, Any]] = None
    ):
        """ëª¨ë¸ ì•„í‹°íŒ©íŠ¸ ë¡œê¹…"""
        if not self.save_artifacts:
            return

        try:
            # ì•„í‹°íŒ©íŠ¸ ìƒì„±
            artifact = wandb.Artifact(
                name=name,
                type="model",
                description=f"Model checkpoint at {model_path}",
                metadata=metadata or {}
            )

            # ëª¨ë¸ íŒŒì¼ ì¶”ê°€
            artifact.add_file(model_path)

            # ì•„í‹°íŒ©íŠ¸ ë¡œê¹…
            self.experiment.log_artifact(artifact)

            logger.info(f"ëª¨ë¸ ì•„í‹°íŒ©íŠ¸ ì €ì¥ ì™„ë£Œ: {name}")

        except Exception as e:
            logger.error(f"ëª¨ë¸ ì•„í‹°íŒ©íŠ¸ ì €ì¥ ì‹¤íŒ¨: {e}")

    def log_dataset_artifact(
            self,
            dataset_path: str,
            name: str,
            description: Optional[str] = None,
            metadata: Optional[Dict[str, Any]] = None
    ):
        """ë°ì´í„°ì…‹ ì•„í‹°íŒ©íŠ¸ ë¡œê¹…"""
        if not self.save_artifacts:
            return

        try:
            # ì•„í‹°íŒ©íŠ¸ ìƒì„±
            artifact = wandb.Artifact(
                name=name,
                type="dataset",
                description=description or f"Dataset at {dataset_path}",
                metadata=metadata or {}
            )

            # ë°ì´í„°ì…‹ ì¶”ê°€ (ë””ë ‰í† ë¦¬ ë˜ëŠ” íŒŒì¼)
            if Path(dataset_path).is_dir():
                artifact.add_dir(dataset_path)
            else:
                artifact.add_file(dataset_path)

            # ì•„í‹°íŒ©íŠ¸ ë¡œê¹…
            self.experiment.log_artifact(artifact)

            logger.info(f"ë°ì´í„°ì…‹ ì•„í‹°íŒ©íŠ¸ ì €ì¥ ì™„ë£Œ: {name}")

        except Exception as e:
            logger.error(f"ë°ì´í„°ì…‹ ì•„í‹°íŒ©íŠ¸ ì €ì¥ ì‹¤íŒ¨: {e}")


class WandbModelCheckpointCallback(Callback):
    """
    ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ë¥¼ Wandb ì•„í‹°íŒ©íŠ¸ë¡œ ì €ì¥í•˜ëŠ” ì½œë°±
    """

    def __init__(
            self,
            monitor: str = "val_loss",
            mode: str = "min",
            save_top_k: int = 3,
            save_last: bool = True
    ):
        super().__init__()
        self.monitor = monitor
        self.mode = mode
        self.save_top_k = save_top_k
        self.save_last = save_last
        self.best_k_models = {}
        self.kth_best_model_path = ""
        self.best_model_score = None
        self.best_model_path = ""

    def on_validation_end(self, trainer: pl.Trainer, pl_module: pl.LightningModule):
        """ê²€ì¦ ì¢…ë£Œ ì‹œ ì²´í¬í¬ì¸íŠ¸ ì €ì¥"""
        if not isinstance(trainer.logger, (WandbLogger, EnhancedWandbLogger)):
            return

        # í˜„ì¬ ë©”íŠ¸ë¦­ ê°’ ê°€ì ¸ì˜¤ê¸°
        current_score = trainer.callback_metrics.get(self.monitor)
        if current_score is None:
            return

        current_score = current_score.item()

        # ìµœê³  ì„±ëŠ¥ ì²´í¬
        if self._is_better_score(current_score):
            self.best_model_score = current_score

            # ì²´í¬í¬ì¸íŠ¸ ì €ì¥
            checkpoint_path = f"checkpoints/best_model_epoch_{trainer.current_epoch}.ckpt"
            trainer.save_checkpoint(checkpoint_path)
            self.best_model_path = checkpoint_path

            # Wandb ì•„í‹°íŒ©íŠ¸ë¡œ ì €ì¥
            if hasattr(trainer.logger, 'log_model_artifact'):
                trainer.logger.log_model_artifact(
                    model_path=checkpoint_path,
                    name=f"best_model",
                    metadata={
                        "epoch": trainer.current_epoch,
                        "score": current_score,
                        "monitor": self.monitor,
                        "mode": self.mode
                    }
                )

            logger.info(f"ìƒˆë¡œìš´ ìµœê³  ëª¨ë¸ ì €ì¥: {self.monitor}={current_score:.4f}")

    def _is_better_score(self, score: float) -> bool:
        """ì ìˆ˜ê°€ ë” ì¢‹ì€ì§€ í™•ì¸"""
        if self.best_model_score is None:
            return True

        if self.mode == "min":
            return score < self.best_model_score
        else:
            return score > self.best_model_score


class WandbExperimentManager:
    """
    Wandb ì‹¤í—˜ ê´€ë¦¬ í´ë˜ìŠ¤
    """

    def __init__(self, project: str, entity: Optional[str] = None):
        self.project = project
        self.entity = entity

        # Wandb ë¡œê·¸ì¸ í™•ì¸
        self._ensure_wandb_login()

    def _ensure_wandb_login(self):
        """Wandb ë¡œê·¸ì¸ í™•ì¸"""
        try:
            wandb.login()
            logger.info("Wandb ë¡œê·¸ì¸ í™•ì¸ ì™„ë£Œ")
        except Exception as e:
            logger.error(f"Wandb ë¡œê·¸ì¸ ì‹¤íŒ¨: {e}")
            raise

    def create_experiment(
            self,
            name: str,
            config: Dict[str, Any],
            tags: Optional[List[str]] = None,
            notes: Optional[str] = None
    ) -> EnhancedWandbLogger:
        """ìƒˆë¡œìš´ ì‹¤í—˜ ìƒì„±"""
        # ì‹¤í—˜ ì´ë¦„ì— íƒ€ì„ìŠ¤íƒ¬í”„ ì¶”ê°€
        import datetime
        timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
        full_name = f"{name}_{timestamp}"

        # ë¡œê±° ìƒì„±
        logger_instance = EnhancedWandbLogger(
            project=self.project,
            entity=self.entity,
            name=full_name,
            tags=tags,
            notes=notes,
            config=config
        )

        logger.info(f"ìƒˆë¡œìš´ ì‹¤í—˜ ìƒì„±: {full_name}")
        return logger_instance

    def get_sweep_config(
            self,
            method: str = "bayes",
            metric_name: str = "val_loss",
            metric_goal: str = "minimize",
            parameters: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """Sweep ì„¤ì • ë°˜í™˜"""
        if parameters is None:
            # ê¸°ë³¸ í•˜ì´í¼íŒŒë¼ë¯¸í„° ë²”ìœ„
            parameters = {
                "learning_rate": {
                    "distribution": "log_uniform_values",
                    "min": 1e-6,
                    "max": 1e-3
                },
                "batch_size": {
                    "values": [8, 16, 32, 64]
                },
                "warmup_steps": {
                    "values": [100, 500, 1000]
                },
                "weight_decay": {
                    "distribution": "uniform",
                    "min": 0.0,
                    "max": 0.1
                }
            }

        sweep_config = {
            "method": method,
            "metric": {
                "name": metric_name,
                "goal": metric_goal
            },
            "parameters": parameters
        }

        return sweep_config

    def create_sweep(
            self,
            sweep_config: Dict[str, Any],
            project: Optional[str] = None
    ) -> str:
        """Sweep ìƒì„±"""
        sweep_id = wandb.sweep(
            sweep_config,
            project=project or self.project,
            entity=self.entity
        )

        logger.info(f"Sweep ìƒì„± ì™„ë£Œ: {sweep_id}")
        return sweep_id

    def run_sweep(
            self,
            sweep_id: str,
            train_function: callable,
            count: Optional[int] = None
    ):
        """Sweep ì‹¤í–‰"""
        wandb.agent(
            sweep_id,
            function=train_function,
            count=count,
            project=self.project,
            entity=self.entity
        )


# ì‚¬ìš© ì˜ˆì‹œ
if __name__ == "__main__":
    # ì‹¤í—˜ ê´€ë¦¬ì ìƒì„±
    experiment_manager = WandbExperimentManager(
        project="llm-modeling",
        entity="your-wandb-entity"  # ì‹¤ì œ entityë¡œ ë³€ê²½
    )

    # ì‹¤í—˜ ì„¤ì •
    config = {
        "model_name": "gpt2",
        "learning_rate": 2e-5,
        "batch_size": 16,
        "max_epochs": 3,
        "use_peft": True,
        "peft_config": {
            "r": 8,
            "lora_alpha": 16
        }
    }

    # ì‹¤í—˜ ìƒì„±
    logger_instance = experiment_manager.create_experiment(
        name="gpt2_classification",
        config=config,
        tags=["gpt2", "classification", "peft"],
        notes="GPT-2 ëª¨ë¸ì„ ì‚¬ìš©í•œ í…ìŠ¤íŠ¸ ë¶„ë¥˜ ì‹¤í—˜"
    )

    print(f"ì‹¤í—˜ ìƒì„± ì™„ë£Œ: {logger_instance.experiment.name}")
    print(f"ì‹¤í—˜ URL: {logger_instance.experiment.url}")

    # Sweep ì„¤ì • ì˜ˆì‹œ
    sweep_config = experiment_manager.get_sweep_config(
        method="bayes",
        metric_name="val_accuracy",
        metric_goal="maximize",
        parameters={
            "learning_rate": {
                "distribution": "log_uniform_values",
                "min": 1e-6,
                "max": 1e-3
            },
            "peft_r": {
                "values": [4, 8, 16, 32]
            },
            "peft_alpha": {
                "values": [8, 16, 32, 64]
            }
        }
    )

    print("Sweep ì„¤ì •:")
    print(json.dumps(sweep_config, indent=2))
```

## ğŸ›ï¸ ê³ ê¸‰ ì „ì´í•™ìŠµ ì „ëµ

### ì ì§„ì  í•´ë™(Progressive Unfreezing)

**ì ì§„ì  í•´ë™**ì€ ì²˜ìŒì—ëŠ” ë°±ë³¸ì„ ê³ ì •í•˜ê³  ë¶„ë¥˜ê¸°ë§Œ í•™ìŠµí•œ í›„, ë‹¨ê³„ì ìœ¼ë¡œ ë°±ë³¸ì˜ ë ˆì´ì–´ë¥¼ í•´ë™í•˜ëŠ” ê¸°ë²•ì´ë‹¤.

```python
class ProgressiveUnfreezingModule(ImageTransferLearningModule):
    def __init__(self, *args, **kwargs):
        # ì ì§„ì  í•´ë™ ê´€ë ¨ íŒŒë¼ë¯¸í„° ì¶”ê°€
        self.unfreeze_epochs = kwargs.pop('unfreeze_epochs', [5, 10, 15])
        self.unfreeze_lr_factors = kwargs.pop('unfreeze_lr_factors', [0.1, 0.01, 0.001])

        super().__init__(*args, **kwargs)

        # ë°±ë³¸ì˜ ë ˆì´ì–´ ê·¸ë£¹ ì •ì˜
        self.setup_layer_groups()

    def setup_layer_groups(self):
        """ë°±ë³¸ì„ ì—¬ëŸ¬ ê·¸ë£¹ìœ¼ë¡œ ë‚˜ëˆ„ê¸°"""
        backbone_children = list(self.backbone.children())
        num_groups = 3

        # ë ˆì´ì–´ë¥¼ ê·¸ë£¹ìœ¼ë¡œ ë‚˜ëˆ„ê¸°
        group_size = len(backbone_children) // num_groups
        self.layer_groups = []

        for i in range(num_groups):
            start_idx = i * group_size
            end_idx = (i + 1) * group_size if i < num_groups - 1 else len(backbone_children)
            group = backbone_children[start_idx:end_idx]
            self.layer_groups.append(group)

        print(f"ë°±ë³¸ì„ {num_groups}ê°œ ê·¸ë£¹ìœ¼ë¡œ ë¶„í• í–ˆìŠµë‹ˆë‹¤.")
        for i, group in enumerate(self.layer_groups):
            print(f"ê·¸ë£¹ {i + 1}: {len(group)}ê°œ ë ˆì´ì–´")

    def unfreeze_layer_group(self, group_idx):
        """íŠ¹ì • ë ˆì´ì–´ ê·¸ë£¹ í•´ë™"""
        if group_idx < len(self.layer_groups):
            for layer in self.layer_groups[group_idx]:
                for param in layer.parameters():
                    param.requires_grad = True
            print(f"ğŸ”“ ë ˆì´ì–´ ê·¸ë£¹ {group_idx + 1} í•´ë™ ì™„ë£Œ!")

    def on_train_epoch_start(self):
        """ì—í¬í¬ ì‹œì‘ ì‹œ ì ì§„ì  í•´ë™ ì²´í¬"""
        current_epoch = self.current_epoch

        # íŠ¹ì • ì—í¬í¬ì—ì„œ ë ˆì´ì–´ ê·¸ë£¹ í•´ë™
        for i, unfreeze_epoch in enumerate(self.unfreeze_epochs):
            if current_epoch == unfreeze_epoch and i < len(self.layer_groups):
                self.unfreeze_layer_group(i)

                # í•™ìŠµë¥  ì¡°ì •
                if hasattr(self.trainer, 'optimizers'):
                    for optimizer in self.trainer.optimizers:
                        for param_group in optimizer.param_groups:
                            param_group['lr'] *= self.unfreeze_lr_factors[i]

                print(f"í•™ìŠµë¥ ì„ {self.unfreeze_lr_factors[i]}ë°°ë¡œ ì¡°ì •í–ˆìŠµë‹ˆë‹¤.")


# ì ì§„ì  í•´ë™ ëª¨ë¸ ìƒì„±
progressive_model = ProgressiveUnfreezingModule(
    model_name='resnet50',
    num_classes=100,
    learning_rate=1e-3,
    freeze_backbone=True,
    unfreeze_epochs=[3, 6, 9],  # 3, 6, 9 ì—í¬í¬ì—ì„œ í•´ë™
    unfreeze_lr_factors=[0.1, 0.1, 0.1]  # ê°ê° 10%ë¡œ í•™ìŠµë¥  ê°ì†Œ
)

print("ì ì§„ì  í•´ë™ ëª¨ë¸ ìƒì„± ì™„ë£Œ!")
# ì¶œë ¥: ì ì§„ì  í•´ë™ ëª¨ë¸ ìƒì„± ì™„ë£Œ!
```

### ì°¨ë³„ì  í•™ìŠµë¥ (Differential Learning Rates)

**ì°¨ë³„ì  í•™ìŠµë¥ **ì€ ë„¤íŠ¸ì›Œí¬ì˜ ë‹¤ë¥¸ ë¶€ë¶„ì— ì„œë¡œ ë‹¤ë¥¸ í•™ìŠµë¥ ì„ ì ìš©í•˜ëŠ” ê¸°ë²•ì´ë‹¤. ì¼ë°˜ì ìœ¼ë¡œ ì‚¬ì „ í›ˆë ¨ëœ ë¶€ë¶„ì€ ë‚®ì€ í•™ìŠµë¥ ì„, ìƒˆë¡œ ì¶”ê°€ëœ ë¶€ë¶„ì€ ë†’ì€ í•™ìŠµë¥ ì„ ì‚¬ìš©í•œë‹¤.

```python
class DifferentialLRModule(ImageTransferLearningModule):
    def __init__(self, *args, **kwargs):
        # ì°¨ë³„ì  í•™ìŠµë¥  íŒŒë¼ë¯¸í„°
        self.backbone_lr = kwargs.pop('backbone_lr', 1e-4)
        self.classifier_lr = kwargs.pop('classifier_lr', 1e-3)

        super().__init__(*args, freeze_backbone=False, **kwargs)

    def configure_optimizers(self):
        """ì°¨ë³„ì  í•™ìŠµë¥  ì„¤ì •"""
        # íŒŒë¼ë¯¸í„° ê·¸ë£¹ ë¶„ë¦¬
        backbone_params = []
        classifier_params = []

        # ë°±ë³¸ íŒŒë¼ë¯¸í„°
        for name, param in self.backbone.named_parameters():
            if param.requires_grad:
                backbone_params.append(param)

        # ë¶„ë¥˜ê¸° íŒŒë¼ë¯¸í„°
        for name, param in self.classifier.named_parameters():
            if param.requires_grad:
                classifier_params.append(param)

        # íŒŒë¼ë¯¸í„° ê·¸ë£¹ë³„ ì„¤ì •
        param_groups = [
            {
                'params': backbone_params,
                'lr': self.backbone_lr,
                'weight_decay': 1e-4
            },
            {
                'params': classifier_params,
                'lr': self.classifier_lr,
                'weight_decay': 1e-3
            }
        ]

        optimizer = AdamW(param_groups)

        # ìŠ¤ì¼€ì¤„ëŸ¬ (ì „ì²´ í•™ìŠµë¥ ì— ì˜í–¥)
        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
            optimizer,
            T_max=30,
            eta_min=1e-6
        )

        print(f"ì°¨ë³„ì  í•™ìŠµë¥  ì„¤ì •:")
        print(f"  ë°±ë³¸: {self.backbone_lr}")
        print(f"  ë¶„ë¥˜ê¸°: {self.classifier_lr}")

        return {
            "optimizer": optimizer,
            "lr_scheduler": {
                "scheduler": scheduler,
                "interval": "epoch"
            }
        }


# ì°¨ë³„ì  í•™ìŠµë¥  ëª¨ë¸ ìƒì„±
differential_lr_model = DifferentialLRModule(
    model_name='resnet50',
    num_classes=100,
    backbone_lr=1e-5,  # ë°±ë³¸ì€ ë§¤ìš° ë‚®ì€ í•™ìŠµë¥ 
    classifier_lr=1e-3  # ë¶„ë¥˜ê¸°ëŠ” ë†’ì€ í•™ìŠµë¥ 
)

print("ì°¨ë³„ì  í•™ìŠµë¥  ëª¨ë¸ ìƒì„± ì™„ë£Œ!")
# ì¶œë ¥: ì°¨ë³„ì  í•™ìŠµë¥  ëª¨ë¸ ìƒì„± ì™„ë£Œ!
```

## ğŸ¤– ìì—°ì–´ ì²˜ë¦¬ë¥¼ ìœ„í•œ ì „ì´í•™ìŠµ

### BERT ê¸°ë°˜ í…ìŠ¤íŠ¸ ë¶„ë¥˜

```python
import pytorch_lightning as pl
import torch
import torch.nn as nn
from transformers import AutoModel, AutoTokenizer, AutoConfig
from torchmetrics import Accuracy, F1Score


class BERTTransferLearningModule(pl.LightningModule):
    def __init__(
            self,
            model_name='bert-base-uncased',
            num_classes=2,
            learning_rate=2e-5,
            max_length=512,
            freeze_bert=False,
            dropout_rate=0.3
    ):
        super().__init__()
        self.save_hyperparameters()

        # BERT ëª¨ë¸ê³¼ ì„¤ì • ë¡œë“œ
        self.config = AutoConfig.from_pretrained(model_name)
        self.bert = AutoModel.from_pretrained(model_name)
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)

        # BERT ê³ ì • ì—¬ë¶€
        if freeze_bert:
            self.freeze_bert_layers()

        # ë¶„ë¥˜ í—¤ë“œ
        self.classifier = nn.Sequential(
            nn.Dropout(dropout_rate),
            nn.Linear(self.config.hidden_size, 512),
            nn.GELU(),
            nn.Dropout(dropout_rate),
            nn.Linear(512, num_classes)
        )

        # ì†ì‹¤ í•¨ìˆ˜ì™€ ë©”íŠ¸ë¦­
        self.criterion = nn.CrossEntropyLoss()
        self.train_acc = Accuracy(task='multiclass', num_classes=num_classes)
        self.val_acc = Accuracy(task='multiclass', num_classes=num_classes)
        self.f1 = F1Score(task='multiclass', num_classes=num_classes)

    def freeze_bert_layers(self):
        """BERT ë ˆì´ì–´ ê³ ì •"""
        for param in self.bert.parameters():
            param.requires_grad = False
        print("ğŸ”’ BERT ë ˆì´ì–´ê°€ ê³ ì •ë˜ì—ˆìŠµë‹ˆë‹¤.")

    def unfreeze_bert_layers(self, num_layers=None):
        """BERT ë ˆì´ì–´ í•´ë™ (ìƒìœ„ Nê°œ ë ˆì´ì–´ë§Œ)"""
        if num_layers is None:
            # ì „ì²´ í•´ë™
            for param in self.bert.parameters():
                param.requires_grad = True
            print("ğŸ”“ ëª¨ë“  BERT ë ˆì´ì–´ê°€ í•´ë™ë˜ì—ˆìŠµë‹ˆë‹¤.")
        else:
            # ìƒìœ„ Nê°œ ë ˆì´ì–´ë§Œ í•´ë™
            total_layers = len(self.bert.encoder.layer)
            layers_to_unfreeze = total_layers - num_layers

            for i in range(layers_to_unfreeze, total_layers):
                for param in self.bert.encoder.layer[i].parameters():
                    param.requires_grad = True

            print(f"ğŸ”“ ìƒìœ„ {num_layers}ê°œ BERT ë ˆì´ì–´ê°€ í•´ë™ë˜ì—ˆìŠµë‹ˆë‹¤.")

    def forward(self, input_ids, attention_mask=None, token_type_ids=None):
        """ìˆœì „íŒŒ"""
        # BERT ì¸ì½”ë”©
        outputs = self.bert(
            input_ids=input_ids,
            attention_mask=attention_mask,
            token_type_ids=token_type_ids
        )

        # [CLS] í† í°ì˜ hidden state ì‚¬ìš©
        pooled_output = outputs.pooler_output

        # ë¶„ë¥˜
        logits = self.classifier(pooled_output)
        return logits

    def training_step(self, batch, batch_idx):
        """í•™ìŠµ ìŠ¤í…"""
        input_ids = batch['input_ids']
        attention_mask = batch['attention_mask']
        labels = batch['labels']

        logits = self(input_ids, attention_mask)
        loss = self.criterion(logits, labels)

        # ë©”íŠ¸ë¦­ ê³„ì‚°
        preds = torch.argmax(logits, dim=1)
        self.train_acc(preds, labels)

        # ë¡œê¹…
        self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True)
        self.log('train_acc', self.train_acc, on_epoch=True, prog_bar=True)

        return loss

    def validation_step(self, batch, batch_idx):
        """ê²€ì¦ ìŠ¤í…"""
        input_ids = batch['input_ids']
        attention_mask = batch['attention_mask']
        labels = batch['labels']

        logits = self(input_ids, attention_mask)
        loss = self.criterion(logits, labels)

        preds = torch.argmax(logits, dim=1)
        self.val_acc(preds, labels)
        self.f1(preds, labels)

        self.log('val_loss', loss, on_epoch=True, prog_bar=True)
        self.log('val_acc', self.val_acc, on_epoch=True, prog_bar=True)
        self.log('val_f1', self.f1, on_epoch=True)

        return loss

    def configure_optimizers(self):
        """ì˜µí‹°ë§ˆì´ì € ì„¤ì •"""
        # AdamW ì˜µí‹°ë§ˆì´ì € (BERTì— ìµœì í™”)
        optimizer = torch.optim.AdamW(
            self.parameters(),
            lr=self.hparams.learning_rate,
            weight_decay=0.01,
            eps=1e-8
        )

        # ì„ í˜• í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬ (ì›Œë°ì—… í¬í•¨)
        num_training_steps = self.trainer.estimated_stepping_batches
        num_warmup_steps = int(0.1 * num_training_steps)

        scheduler = torch.optim.lr_scheduler.LinearLR(
            optimizer,
            start_factor=0.1,
            total_iters=num_warmup_steps
        )

        return {
            "optimizer": optimizer,
            "lr_scheduler": {
                "scheduler": scheduler,
                "interval": "step"
            }
        }


# NLP ë°ì´í„° ëª¨ë“ˆ
class TextClassificationDataModule(pl.LightningDataModule):
    def __init__(
            self,
            tokenizer,
            train_texts,
            train_labels,
            val_texts,
            val_labels,
            max_length=512,
            batch_size=16
    ):
        super().__init__()
        self.tokenizer = tokenizer
        self.train_texts = train_texts
        self.train_labels = train_labels
        self.val_texts = val_texts
        self.val_labels = val_labels
        self.max_length = max_length
        self.batch_size = batch_size

    def setup(self, stage=None):
        """ë°ì´í„°ì…‹ ì„¤ì •"""
        from torch.utils.data import Dataset

        class TextDataset(Dataset):
            def __init__(self, texts, labels, tokenizer, max_length):
                self.texts = texts
                self.labels = labels
                self.tokenizer = tokenizer
                self.max_length = max_length

            def __len__(self):
                return len(self.texts)

            def __getitem__(self, idx):
                text = str(self.texts[idx])
                label = self.labels[idx]

                # í† í°í™”
                encoding = self.tokenizer(
                    text,
                    truncation=True,
                    padding='max_length',
                    max_length=self.max_length,
                    return_tensors='pt'
                )

                return {
                    'input_ids': encoding['input_ids'].flatten(),
                    'attention_mask': encoding['attention_mask'].flatten(),
                    'labels': torch.tensor(label, dtype=torch.long)
                }

        self.train_dataset = TextDataset(
            self.train_texts, self.train_labels,
            self.tokenizer, self.max_length
        )

        self.val_dataset = TextDataset(
            self.val_texts, self.val_labels,
            self.tokenizer, self.max_length
        )

    def train_dataloader(self):
        return DataLoader(
            self.train_dataset,
            batch_size=self.batch_size,
            shuffle=True,
            num_workers=4
        )

    def val_dataloader(self):
        return DataLoader(
            self.val_dataset,
            batch_size=self.batch_size,
            shuffle=False,
            num_workers=4
        )


# BERT ì „ì´í•™ìŠµ ëª¨ë¸ ìƒì„±
bert_model = BERTTransferLearningModule(
    model_name='bert-base-uncased',
    num_classes=2,
    learning_rate=2e-5,
    freeze_bert=False
)

print("BERT ì „ì´í•™ìŠµ ëª¨ë¸ ìƒì„± ì™„ë£Œ!")
print(f"ì´ íŒŒë¼ë¯¸í„°: {sum(p.numel() for p in bert_model.parameters()):,}")
print(f"í•™ìŠµ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„°: {sum(p.numel() for p in bert_model.parameters() if p.requires_grad):,}")
# ì¶œë ¥: BERT ì „ì´í•™ìŠµ ëª¨ë¸ ìƒì„± ì™„ë£Œ!
# ì¶œë ¥: ì´ íŒŒë¼ë¯¸í„°: 109,483,778
# ì¶œë ¥: í•™ìŠµ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„°: 109,483,778
```

## ğŸ“Š ì „ì´í•™ìŠµ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ê³¼ ì½œë°±

### ì „ì´í•™ìŠµ ì „ìš© ì½œë°±

```python
import pytorch_lightning as pl
from pytorch_lightning.callbacks import Callback
import matplotlib.pyplot as plt
import numpy as np


class TransferLearningCallback(Callback):
    def __init__(self, unfreeze_epoch=5):
        super().__init__()
        self.unfreeze_epoch = unfreeze_epoch
        self.metrics_history = {
            'train_loss': [],
            'val_loss': [],
            'train_acc': [],
            'val_acc': [],
            'learning_rates': []
        }
        self.frozen_performance = None
        self.unfrozen_performance = None

    def on_train_epoch_end(self, trainer, pl_module):
        """ì—í¬í¬ ì¢…ë£Œ ì‹œ ë©”íŠ¸ë¦­ ì €ì¥"""
        # í˜„ì¬ ë©”íŠ¸ë¦­ ìˆ˜ì§‘
        train_loss = trainer.callback_metrics.get('train_loss_epoch', 0)
        val_loss = trainer.callback_metrics.get('val_loss', 0)
        train_acc = trainer.callback_metrics.get('train_acc', 0)
        val_acc = trainer.callback_metrics.get('val_acc', 0)

        # í˜„ì¬ í•™ìŠµë¥ 
        current_lr = trainer.optimizers[0].param_groups[0]['lr']

        # ë©”íŠ¸ë¦­ ì €ì¥
        self.metrics_history['train_loss'].append(float(train_loss))
        self.metrics_history['val_loss'].append(float(val_loss))
        self.metrics_history['train_acc'].append(float(train_acc))
        self.metrics_history['val_acc'].append(float(val_acc))
        self.metrics_history['learning_rates'].append(current_lr)

        # ìë™ í•´ë™ (ëª¨ë¸ì— unfreeze_backbone ë©”ì„œë“œê°€ ìˆëŠ” ê²½ìš°)
        if (trainer.current_epoch == self.unfreeze_epoch and
                hasattr(pl_module, 'unfreeze_backbone')):

            # ê³ ì • ìƒíƒœì—ì„œì˜ ì„±ëŠ¥ ì €ì¥
            self.frozen_performance = {
                'val_loss': float(val_loss),
                'val_acc': float(val_acc)
            }

            # ë°±ë³¸ í•´ë™
            pl_module.unfreeze_backbone()

            # í•™ìŠµë¥  ì¡°ì • (ë³´í†µ ë” ë‚®ê²Œ)
            for optimizer in trainer.optimizers:
                for param_group in optimizer.param_groups:
                    param_group['lr'] *= 0.1  # 10ë¶„ì˜ 1ë¡œ ê°ì†Œ

            print(f"ğŸ”„ ì—í¬í¬ {self.unfreeze_epoch}ì—ì„œ ë°±ë³¸ í•´ë™ ë° í•™ìŠµë¥  ì¡°ì •!")

    def on_train_end(self, trainer, pl_module):
        """í•™ìŠµ ì¢…ë£Œ ì‹œ ì„±ëŠ¥ ë¶„ì„"""
        if len(self.metrics_history['val_acc']) > 0:
            self.unfrozen_performance = {
                'val_loss': self.metrics_history['val_loss'][-1],
                'val_acc': self.metrics_history['val_acc'][-1]
            }

            # ì„±ëŠ¥ ë¹„êµ ë¦¬í¬íŠ¸
            self.generate_performance_report()

            # í•™ìŠµ ê³¡ì„  ì‹œê°í™”
            self.plot_training_curves()

    def generate_performance_report(self):
        """ì„±ëŠ¥ ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„±"""
        print("\n" + "=" * 50)
        print("ğŸ“Š ì „ì´í•™ìŠµ ì„±ëŠ¥ ë¶„ì„ ë¦¬í¬íŠ¸")
        print("=" * 50)

        if self.frozen_performance:
            print(f"ğŸ”’ ë°±ë³¸ ê³ ì • ìƒíƒœ (ì—í¬í¬ {self.unfreeze_epoch}):")
            print(f"   ê²€ì¦ ì†ì‹¤: {self.frozen_performance['val_loss']:.4f}")
            print(f"   ê²€ì¦ ì •í™•ë„: {self.frozen_performance['val_acc']:.4f}")

        if self.unfrozen_performance:
            print(f"ğŸ”“ ë°±ë³¸ í•´ë™ í›„ ìµœì¢…:")
            print(f"   ê²€ì¦ ì†ì‹¤: {self.unfrozen_performance['val_loss']:.4f}")
            print(f"   ê²€ì¦ ì •í™•ë„: {self.unfrozen_performance['val_acc']:.4f}")

            if self.frozen_performance:
                acc_improvement = (self.unfrozen_performance['val_acc'] -
                                   self.frozen_performance['val_acc'])
                print(f"ğŸ“ˆ ì •í™•ë„ ê°œì„ : {acc_improvement:+.4f}")

        # ìµœê³  ì„±ëŠ¥
        best_val_acc = max(self.metrics_history['val_acc'])
        best_epoch = self.metrics_history['val_acc'].index(best_val_acc)
        print(f"ğŸ† ìµœê³  ê²€ì¦ ì •í™•ë„: {best_val_acc:.4f} (ì—í¬í¬ {best_epoch + 1})")

        print("=" * 50)

    def plot_training_curves(self):
        """í•™ìŠµ ê³¡ì„  ì‹œê°í™”"""
        epochs = range(1, len(self.metrics_history['train_loss']) + 1)

        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))

        # ì†ì‹¤ ê³¡ì„ 
        ax1.plot(epochs, self.metrics_history['train_loss'], 'b-', label='í•™ìŠµ ì†ì‹¤')
        ax1.plot(epochs, self.metrics_history['val_loss'], 'r-', label='ê²€ì¦ ì†ì‹¤')
        if self.unfreeze_epoch < len(epochs):
            ax1.axvline(x=self.unfreeze_epoch, color='green', linestyle='--',
                        label='ë°±ë³¸ í•´ë™')
        ax1.set_title('ì†ì‹¤ ë³€í™”')
        ax1.set_xlabel('ì—í¬í¬')
        ax1.set_ylabel('ì†ì‹¤')
        ax1.legend()
        ax1.grid(True)

        # ì •í™•ë„ ê³¡ì„ 
        ax2.plot(epochs, self.metrics_history['train_acc'], 'b-', label='í•™ìŠµ ì •í™•ë„')
        ax2.plot(epochs, self.metrics_history['val_acc'], 'r-', label='ê²€ì¦ ì •í™•ë„')
        if self.unfreeze_epoch < len(epochs):
            ax2.axvline(x=self.unfreeze_epoch, color='green', linestyle='--',
                        label='ë°±ë³¸ í•´ë™')
        ax2.set_title('ì •í™•ë„ ë³€í™”')
        ax2.set_xlabel('ì—í¬í¬')
        ax2.set_ylabel('ì •í™•ë„')
        ax2.legend()
        ax2.grid(True)

        # í•™ìŠµë¥  ë³€í™”
        ax3.plot(epochs, self.metrics_history['learning_rates'], 'g-')
        ax3.set_title('í•™ìŠµë¥  ë³€í™”')
        ax3.set_xlabel('ì—í¬í¬')
        ax3.set_ylabel('í•™ìŠµë¥ ')
        ax3.set_yscale('log')
        ax3.grid(True)

        # ê³¼ì í•© ë¶„ì„
        train_val_gap = np.array(self.metrics_history['train_acc']) - np.array(self.metrics_history['val_acc'])
        ax4.plot(epochs, train_val_gap, 'purple', label='ê³¼ì í•© ì •ë„')
        ax4.axhline(y=0, color='black', linestyle='-', alpha=0.3)
        ax4.set_title('ê³¼ì í•© ë¶„ì„ (í•™ìŠµ-ê²€ì¦ ì •í™•ë„ ì°¨ì´)')
        ax4.set_xlabel('ì—í¬í¬')
        ax4.set_ylabel('ì •í™•ë„ ì°¨ì´')
        ax4.legend()
        ax4.grid(True)

        plt.tight_layout()
        plt.savefig('transfer_learning_analysis.png', dpi=300, bbox_inches='tight')
        plt.show()

        print("ğŸ“ˆ í•™ìŠµ ê³¡ì„ ì´ 'transfer_learning_analysis.png'ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤!")


# ì „ì´í•™ìŠµ ì½œë°± ì‚¬ìš©
transfer_callback = TransferLearningCallback(unfreeze_epoch=5)

print("ì „ì´í•™ìŠµ ì „ìš© ì½œë°± ìƒì„± ì™„ë£Œ!")
# ì¶œë ¥: ì „ì´í•™ìŠµ ì „ìš© ì½œë°± ìƒì„± ì™„ë£Œ!
```

## ğŸš€ ì™„ì „í•œ ì‹¤í—˜ ìŠ¤í¬ë¦½íŠ¸

ì´ì œ ëª¨ë“  êµ¬ì„±ìš”ì†Œë¥¼ ì¡°í•©í•˜ì—¬ **ì™„ì „í•œ LLM í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸**ë¥¼ ì‘ì„±í•´ë³´ì.

```python
# src/experiments/train_generator.py
"""í…ìŠ¤íŠ¸ ìƒì„± ëª¨ë¸ í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸"""

import os
import sys
from pathlib import Path
import argparse
import json
import torch
import pytorch_lightning as pl
from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping, LearningRateMonitor
import logging

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì¶”ê°€
sys.path.append(str(Path(__file__).parent.parent))

# í”„ë¡œì íŠ¸ ëª¨ë“ˆë“¤
from models.generation import TextGenerationModule
from data.base_datamodule import TextGenerationDataModule
from callbacks.model_monitoring import (
    LLMModelMonitoringCallback,
    ModelSizeCallback,
    TextGenerationCallback,
    WandbModelCheckpointCallback
)
from utils.wandb_integration import EnhancedWandbLogger, WandbExperimentManager

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


def parse_args():
    """ëª…ë ¹í–‰ ì¸ì íŒŒì‹±"""
    parser = argparse.ArgumentParser(description="LLM í…ìŠ¤íŠ¸ ìƒì„± ëª¨ë¸ í•™ìŠµ")

    # ëª¨ë¸ ì„¤ì •
    parser.add_argument("--model_name", type=str, default="gpt2",
                        help="Hugging Face ëª¨ë¸ ì´ë¦„")
    parser.add_argument("--max_length", type=int, default=512,
                        help="ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´")

    # ìƒì„± ì„¤ì •
    parser.add_argument("--max_new_tokens", type=int, default=50,
                        help="ìµœëŒ€ ìƒì„± í† í° ìˆ˜")
    parser.add_argument("--temperature", type=float, default=1.0,
                        help="ìƒì„± ì˜¨ë„")
    parser.add_argument("--top_k", type=int, default=50,
                        help="Top-k ìƒ˜í”Œë§")
    parser.add_argument("--top_p", type=float, default=0.9,
                        help="Top-p (nucleus) ìƒ˜í”Œë§")
    parser.add_argument("--do_sample", action="store_true", default=True,
                        help="ìƒ˜í”Œë§ ìƒì„± ì—¬ë¶€")

    # ë°ì´í„° ì„¤ì •
    parser.add_argument("--dataset_name", type=str, default="wikitext",
                        help="Hugging Face ë°ì´í„°ì…‹ ì´ë¦„")
    parser.add_argument("--dataset_config", type=str, default="wikitext-2-raw-v1",
                        help="ë°ì´í„°ì…‹ ì„¤ì •")
    parser.add_argument("--text_column", type=str, default="text",
                        help="í…ìŠ¤íŠ¸ ì»¬ëŸ¼ ì´ë¦„")
    parser.add_argument("--batch_size", type=int, default=4,
                        help="ë°°ì¹˜ í¬ê¸°")
    parser.add_argument("--max_samples", type=int, default=None,
                        help="ìµœëŒ€ ìƒ˜í”Œ ìˆ˜ (ë””ë²„ê¹…ìš©)")

    # í›ˆë ¨ ì„¤ì •
    parser.add_argument("--max_epochs", type=int, default=3,
                        help="ìµœëŒ€ ì—í¬í¬ ìˆ˜")
    parser.add_argument("--learning_rate", type=float, default=5e-5,
                        help="í•™ìŠµë¥ ")
    parser.add_argument("--weight_decay", type=float, default=0.01,
                        help="ê°€ì¤‘ì¹˜ ê°ì‡ ")
    parser.add_argument("--warmup_steps", type=int, default=500,
                        help="ì›Œë°ì—… ìŠ¤í… ìˆ˜")

    # PEFT ì„¤ì •
    parser.add_argument("--use_peft", action="store_true",
                        help="PEFT (LoRA) ì‚¬ìš© ì—¬ë¶€")
    parser.add_argument("--peft_r", type=int, default=16,
                        help="LoRA rank")
    parser.add_argument("--peft_alpha", type=int, default=32,
                        help="LoRA alpha")
    parser.add_argument("--peft_dropout", type=float, default=0.1,
                        help="LoRA dropout")

    # ìƒ˜í”Œ í”„ë¡¬í”„íŠ¸ ì„¤ì •
    parser.add_argument("--sample_prompts", type=str, nargs="+",
                        default=[
                            "The future of artificial intelligence is",
                            "In a world where technology",
                            "Once upon a time in a distant galaxy"
                        ],
                        help="ìƒ˜í”Œ ìƒì„±ìš© í”„ë¡¬í”„íŠ¸")
    parser.add_argument("--generate_every_n_epochs", type=int, default=1,
                        help="ëª‡ ì—í¬í¬ë§ˆë‹¤ ìƒ˜í”Œ ìƒì„±í• ì§€")

    # í•˜ë“œì›¨ì–´ ì„¤ì •
    parser.add_argument("--accelerator", type=str, default="auto",
                        help="ê°€ì†ê¸° ì¢…ë¥˜")
    parser.add_argument("--devices", type=int, default=1,
                        help="ì‚¬ìš©í•  ë””ë°”ì´ìŠ¤ ìˆ˜")
    parser.add_argument("--precision", type=str, default="bf16-mixed",
                        help="ì •ë°€ë„")
    parser.add_argument("--strategy", type=str, default="auto",
                        help="ë¶„ì‚° ì „ëµ")

    # Wandb ì„¤ì •
    parser.add_argument("--wandb_project", type=str, default="llm-generation",
                        help="Wandb í”„ë¡œì íŠ¸ ì´ë¦„")
    parser.add_argument("--wandb_entity", type=str, default=None,
                        help="Wandb ì—”í‹°í‹°")
    parser.add_argument("--experiment_name", type=str, default=None,
                        help="ì‹¤í—˜ ì´ë¦„")
    parser.add_argument("--wandb_tags", type=str, nargs="+", default=[],
                        help="Wandb íƒœê·¸ ë¦¬ìŠ¤íŠ¸")
    parser.add_argument("--offline", action="store_true",
                        help="ì˜¤í”„ë¼ì¸ ëª¨ë“œ")

    # ê¸°íƒ€ ì„¤ì •
    parser.add_argument("--output_dir", type=str, default="outputs",
                        help="ì¶œë ¥ ë””ë ‰í† ë¦¬")
    parser.add_argument("--seed", type=int, default=42,
                        help="ëœë¤ ì‹œë“œ")
    parser.add_argument("--num_workers", type=int, default=4,
                        help="ë°ì´í„° ë¡œë” ì›Œì»¤ ìˆ˜")
    parser.add_argument("--fast_dev_run", action="store_true",
                        help="ë¹ ë¥¸ ê°œë°œ í…ŒìŠ¤íŠ¸ ì‹¤í–‰")

    return parser.parse_args()


def create_model(args):
    """ëª¨ë¸ ìƒì„±"""
    # PEFT ì„¤ì •
    peft_config = None
    if args.use_peft:
        peft_config = {
            "r": args.peft_r,
            "lora_alpha": args.peft_alpha,
            "lora_dropout": args.peft_dropout,
            "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"],
        }

    # ëª¨ë¸ ìƒì„±
    model = TextGenerationModule(
        model_name=args.model_name,
        learning_rate=args.learning_rate,
        weight_decay=args.weight_decay,
        warmup_steps=args.warmup_steps,
        max_epochs=args.max_epochs,
        use_peft=args.use_peft,
        peft_config=peft_config,
        model_max_length=args.max_length,
        max_new_tokens=args.max_new_tokens,
        temperature=args.temperature,
        top_k=args.top_k,
        top_p=args.top_p,
        do_sample=args.do_sample
    )

    return model


def create_data_module(args):
    """ë°ì´í„° ëª¨ë“ˆ ìƒì„±"""
    data_module = TextGenerationDataModule(
        tokenizer_name=args.model_name,
        dataset_name=args.dataset_name,
        dataset_config=args.dataset_config,
        text_column=args.text_column,
        batch_size=args.batch_size,
        max_length=args.max_length,
        num_workers=args.num_workers,
        max_samples=args.max_samples
    )

    return data_module


def setup_callbacks(args, wandb_logger):
    """ì½œë°± ì„¤ì •"""
    callbacks = []

    # ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸
    checkpoint_callback = ModelCheckpoint(
        dirpath=f"{args.output_dir}/checkpoints",
        filename="best-{epoch:02d}-{val_perplexity:.2f}",
        monitor="val_perplexity",
        mode="min",
        save_top_k=3,
        save_last=True,
        verbose=True
    )
    callbacks.append(checkpoint_callback)

    # ì¡°ê¸° ì¢…ë£Œ
    early_stopping = EarlyStopping(
        monitor="val_perplexity",
        patience=3,
        mode="min",
        verbose=True,
        min_delta=0.1
    )
    callbacks.append(early_stopping)

    # í•™ìŠµë¥  ëª¨ë‹ˆí„°ë§
    lr_monitor = LearningRateMonitor(
        logging_interval="step",
        log_momentum=True
    )
    callbacks.append(lr_monitor)

    # ì»¤ìŠ¤í…€ ì½œë°±ë“¤
    model_monitoring = LLMModelMonitoringCallback(
        log_every_n_steps=100,
        monitor_gradients=True,
        monitor_weights=True,
        monitor_resources=True
    )
    callbacks.append(model_monitoring)

    model_size_callback = ModelSizeCallback()
    callbacks.append(model_size_callback)

    # í…ìŠ¤íŠ¸ ìƒì„± ì½œë°±
    generation_callback = TextGenerationCallback(
        sample_prompts=args.sample_prompts,
        generate_every_n_epochs=args.generate_every_n_epochs,
        max_new_tokens=args.max_new_tokens,
        temperature=args.temperature,
        top_k=args.top_k,
        top_p=args.top_p
    )
    callbacks.append(generation_callback)

    # Wandb ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸
    wandb_checkpoint = WandbModelCheckpointCallback(
        monitor="val_perplexity",
        mode="min",
        save_top_k=3
    )
    callbacks.append(wandb_checkpoint)

    return callbacks


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    # ì¸ì íŒŒì‹±
    args = parse_args()

    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
    Path(args.output_dir).mkdir(exist_ok=True, parents=True)
    Path(f"{args.output_dir}/checkpoints").mkdir(exist_ok=True, parents=True)

    # ì‹œë“œ ì„¤ì •
    pl.seed_everything(args.seed, workers=True)

    logger.info("=" * 60)
    logger.info("LLM í…ìŠ¤íŠ¸ ìƒì„± ëª¨ë¸ í•™ìŠµ ì‹œì‘")
    logger.info("=" * 60)

    try:
        # Wandb ë¡œê±° ì„¤ì • (ë¶„ë¥˜ ìŠ¤í¬ë¦½íŠ¸ì™€ ìœ ì‚¬í•œ ë¡œì§)
        if not args.offline:
            experiment_manager = WandbExperimentManager(
                project=args.wandb_project,
                entity=args.wandb_entity
            )

            config = vars(args)
            experiment_name = args.experiment_name or f"{args.model_name.replace('/', '-')}_generation"
            tags = args.wandb_tags + [args.model_name.split("/")[-1], "generation"]
            if args.use_peft:
                tags.append("peft")

            wandb_logger = experiment_manager.create_experiment(
                name=experiment_name,
                config=config,
                tags=tags
            )
            logger.info(f"Wandb ì‹¤í—˜ URL: {wandb_logger.experiment.url}")
        else:
            from pytorch_lightning.loggers import CSVLogger
            wandb_logger = CSVLogger(args.output_dir, name="offline_logs")

        # ì½œë°± ì„¤ì •
        callbacks = setup_callbacks(args, wandb_logger)

        # ë°ì´í„° ëª¨ë“ˆ ìƒì„±
        logger.info("ë°ì´í„° ëª¨ë“ˆ ìƒì„± ì¤‘...")
        data_module = create_data_module(args)
        data_module.prepare_data()
        data_module.setup("fit")

        # ëª¨ë¸ ìƒì„±
        logger.info("ëª¨ë¸ ìƒì„± ì¤‘...")
        model = create_model(args)

        # íŠ¸ë ˆì´ë„ˆ ì„¤ì •
        trainer = pl.Trainer(
            max_epochs=args.max_epochs,
            accelerator=args.accelerator,
            devices=args.devices,
            precision=args.precision,
            strategy=args.strategy,
            logger=wandb_logger,
            callbacks=callbacks,
            gradient_clip_val=1.0,
            enable_checkpointing=True,
            enable_progress_bar=True,
            enable_model_summary=True,
            val_check_interval=1.0,
            log_every_n_steps=100,
            fast_dev_run=args.fast_dev_run,
            deterministic=True,
            benchmark=True
        )

        # í•™ìŠµ ì‹œì‘
        logger.info("ğŸš€ í•™ìŠµ ì‹œì‘!")
        trainer.fit(model, data_module)

        # ìƒ˜í”Œ ìƒì„± í…ŒìŠ¤íŠ¸
        logger.info("ğŸ“ ìµœì¢… ìƒ˜í”Œ ìƒì„±:")
        for i, prompt in enumerate(args.sample_prompts[:3]):
            generated = model.generate_text(
                prompt=prompt,
                max_new_tokens=args.max_new_tokens,
                temperature=args.temperature
            )
            logger.info(f"í”„ë¡¬í”„íŠ¸ {i + 1}: {prompt}")
            logger.info(f"ìƒì„± ê²°ê³¼: {generated[0]}")
            logger.info("-" * 50)

        # ê²°ê³¼ ì €ì¥
        best_model_path = trainer.checkpoint_callback.best_model_path
        results = {
            "experiment_name": wandb_logger.experiment.name if hasattr(wandb_logger, 'experiment') else "offline",
            "best_model_path": str(best_model_path),
            "config": vars(args)
        }

        results_path = f"{args.output_dir}/generation_results.json"
        with open(results_path, 'w') as f:
            json.dump(results, f, indent=2)

        logger.info("=" * 60)
        logger.info("í•™ìŠµ ì™„ë£Œ!")
        logger.info(f"ê²°ê³¼ê°€ {results_path}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        logger.info("=" * 60)

    except Exception as e:
        logger.error(f"ì‹¤í—˜ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        raise

    finally:
        if not args.offline:
            try:
                import wandb
                wandb.finish()
            except:
                pass


if __name__ == "__main__":
    main()
```

## ğŸ’¡ ì‹¤ë¬´ í™œìš© íŒê³¼ ë² ìŠ¤íŠ¸ í”„ë™í‹°ìŠ¤

### 1. ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ í•™ìŠµ ì „ëµ

```python
# src/utils/memory_optimization.py
"""ë©”ëª¨ë¦¬ ìµœì í™”ë¥¼ ìœ„í•œ ìœ í‹¸ë¦¬í‹°"""

import torch
import pytorch_lightning as pl
from pytorch_lightning.strategies import DeepSpeedStrategy, FSDPStrategy
from typing import Dict, Any


class MemoryOptimizedTrainingStrategy:
    """ë©”ëª¨ë¦¬ ìµœì í™” ì „ëµ ì„ íƒê¸°"""

    @staticmethod
    def get_strategy_config(
            model_size: str,
            available_memory_gb: float,
            num_gpus: int
    ) -> Dict[str, Any]:
        """
        ëª¨ë¸ í¬ê¸°ì™€ ì‚¬ìš© ê°€ëŠ¥í•œ ë©”ëª¨ë¦¬ì— ë”°ë¥¸ ìµœì  ì „ëµ ì¶”ì²œ

        Args:
            model_size: "small", "medium", "large", "xl"
            available_memory_gb: GPUë‹¹ ì‚¬ìš© ê°€ëŠ¥ ë©”ëª¨ë¦¬ (GB)
            num_gpus: GPU ê°œìˆ˜

        Returns:
            trainer ì„¤ì • ë”•ì…”ë„ˆë¦¬
        """
        config = {
            "precision": "bf16-mixed",
            "accumulate_grad_batches": 1,
            "gradient_clip_val": 1.0
        }

        if model_size == "small":  # ~100M íŒŒë¼ë¯¸í„°
            if available_memory_gb >= 8:
                config.update({
                    "strategy": "auto",
                    "precision": "32"
                })
            else:
                config.update({
                    "strategy": "auto",
                    "precision": "bf16-mixed",
                    "accumulate_grad_batches": 2
                })

        elif model_size == "medium":  # ~300M íŒŒë¼ë¯¸í„°
            if available_memory_gb >= 16:
                config.update({
                    "strategy": "ddp" if num_gpus > 1 else "auto",
                    "precision": "bf16-mixed"
                })
            else:
                config.update({
                    "strategy": "ddp" if num_gpus > 1 else "auto",
                    "precision": "bf16-mixed",
                    "accumulate_grad_batches": 4
                })

        elif model_size == "large":  # ~1B íŒŒë¼ë¯¸í„°
            if available_memory_gb >= 24:
                config.update({
                    "strategy": FSDPStrategy() if num_gpus > 1 else "auto",
                    "precision": "bf16-mixed"
                })
            else:
                # DeepSpeed ZeRO Stage 2
                deepspeed_config = {
                    "zero_optimization": {
                        "stage": 2,
                        "offload_optimizer": {"device": "cpu"},
                        "allgather_partitions": True,
                        "allgather_bucket_size": 5e8,
                        "overlap_comm": True,
                        "reduce_scatter": True,
                        "reduce_bucket_size": 5e8,
                        "contiguous_gradients": True
                    },
                    "train_micro_batch_size_per_gpu": 1,
                    "gradient_accumulation_steps": 8
                }

                config.update({
                    "strategy": DeepSpeedStrategy(config=deepspeed_config),
                    "precision": "bf16-mixed",
                    "accumulate_grad_batches": 8
                })

        elif model_size == "xl":  # ~7B+ íŒŒë¼ë¯¸í„°
            # DeepSpeed ZeRO Stage 3 + CPU Offload
            deepspeed_config = {
                "zero_optimization": {
                    "stage": 3,
                    "offload_optimizer": {"device": "cpu"},
                    "offload_param": {"device": "cpu"},
                    "overlap_comm": True,
                    "contiguous_gradients": True,
                    "sub_group_size": 1e9,
                    "reduce_bucket_size": "auto",
                    "stage3_prefetch_bucket_size": "auto",
                    "stage3_param_persistence_threshold": "auto",
                    "stage3_max_live_parameters": 1e9,
                    "stage3_max_reuse_distance": 1e9
                },
                "train_micro_batch_size_per_gpu": 1,
                "gradient_accumulation_steps": 16
            }

            config.update({
                "strategy": DeepSpeedStrategy(config=deepspeed_config),
                "precision": "bf16-mixed",
                "accumulate_grad_batches": 16
            })

        return config


# ì‚¬ìš© ì˜ˆì‹œ
def get_optimized_trainer_config(model_name: str, num_gpus: int) -> Dict[str, Any]:
    """ëª¨ë¸ì— ë”°ë¥¸ ìµœì í™”ëœ trainer ì„¤ì • ë°˜í™˜"""

    # ëª¨ë¸ í¬ê¸° ì¶”ì • (ì‹¤ì œë¡œëŠ” ë” ì •í™•í•œ ë°©ë²• ì‚¬ìš©)
    model_size_map = {
        "gpt2": "small",
        "gpt2-medium": "medium",
        "gpt2-large": "large",
        "gpt2-xl": "large",
        "microsoft/DialoGPT-large": "large",
        "facebook/opt-1.3b": "large",
        "facebook/opt-6.7b": "xl",
        "meta-llama/Llama-2-7b": "xl"
    }

    model_size = model_size_map.get(model_name, "medium")

    # GPU ë©”ëª¨ë¦¬ í™•ì¸ (ì‹¤ì œ í™˜ê²½ì—ì„œëŠ” ë™ì ìœ¼ë¡œ í™•ì¸)
    if torch.cuda.is_available():
        available_memory_gb = torch.cuda.get_device_properties(0).total_memory / 1024 ** 3
    else:
        available_memory_gb = 16  # ê¸°ë³¸ê°’

    strategy = MemoryOptimizedTrainingStrategy()
    return strategy.get_strategy_config(model_size, available_memory_gb, num_gpus)


print("ë©”ëª¨ë¦¬ ìµœì í™” ì „ëµ ì˜ˆì‹œ:")
config = get_optimized_trainer_config("gpt2-large", num_gpus=2)
for key, value in config.items():
    print(f"  {key}: {value}")
```

### 2. ì‹¤í—˜ ë¹„êµ ë° ë¶„ì„

```python
# src/utils/experiment_analysis.py
"""ì‹¤í—˜ ê²°ê³¼ ë¶„ì„ ë° ë¹„êµ ë„êµ¬"""

import wandb
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from typing import List, Dict, Any, Optional
import numpy as np


class ExperimentAnalyzer:
    """Wandb ì‹¤í—˜ ê²°ê³¼ ë¶„ì„ í´ë˜ìŠ¤"""

    def __init__(self, project: str, entity: Optional[str] = None):
        self.project = project
        self.entity = entity
        self.api = wandb.Api()

    def get_experiment_data(self, filters: Optional[Dict[str, Any]] = None) -> pd.DataFrame:
        """ì‹¤í—˜ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°"""
        # í”„ë¡œì íŠ¸ì—ì„œ ì‹¤í–‰ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
        runs = self.api.runs(
            path=f"{self.entity}/{self.project}" if self.entity else self.project,
            filters=filters
        )

        # ë°ì´í„° ìˆ˜ì§‘
        data = []
        for run in runs:
            row = {
                "run_id": run.id,
                "name": run.name,
                "state": run.state,
                "created_at": run.created_at,
                "tags": run.tags,
                "notes": run.notes
            }

            # ì„¤ì • ì¶”ê°€
            row.update({f"config_{k}": v for k, v in run.config.items()})

            # ìš”ì•½ ë©”íŠ¸ë¦­ ì¶”ê°€
            row.update({f"summary_{k}": v for k, v in run.summary.items()})

            data.append(row)

        return pd.DataFrame(data)

    def compare_experiments(
            self,
            experiment_names: List[str],
            metrics: List[str] = ["val_loss", "val_accuracy"]
    ) -> None:
        """ì‹¤í—˜ ë¹„êµ ì‹œê°í™”"""
        # ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
        filters = {"display_name": {"$in": experiment_names}}
        df = self.get_experiment_data(filters)

        if df.empty:
            print("ë¹„êµí•  ì‹¤í—˜ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return

        # ë©”íŠ¸ë¦­ ë¹„êµ í”Œë¡¯
        fig, axes = plt.subplots(len(metrics), 1, figsize=(12, 6 * len(metrics)))
        if len(metrics) == 1:
            axes = [axes]

        for i, metric in enumerate(metrics):
            metric_col = f"summary_{metric}"
            if metric_col in df.columns:
                # ë°•ìŠ¤ í”Œë¡¯
                df_metric = df[df[metric_col].notna()]
                if not df_metric.empty:
                    sns.boxplot(data=df_metric, x="name", y=metric_col, ax=axes[i])
                    axes[i].set_title(f"{metric} ë¹„êµ")
                    axes[i].tick_params(axis='x', rotation=45)

        plt.tight_layout()
        plt.show()

    def analyze_hyperparameter_impact(
            self,
            hyperparameter: str,
            target_metric: str = "val_accuracy"
    ) -> None:
        """í•˜ì´í¼íŒŒë¼ë¯¸í„°ì˜ ì„±ëŠ¥ ì˜í–¥ ë¶„ì„"""
        df = self.get_experiment_data()

        hp_col = f"config_{hyperparameter}"
        metric_col = f"summary_{target_metric}"

        if hp_col not in df.columns or metric_col not in df.columns:
            print(f"í•˜ì´í¼íŒŒë¼ë¯¸í„° '{hyperparameter}' ë˜ëŠ” ë©”íŠ¸ë¦­ '{target_metric}'ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return

        # ê²°ì¸¡ê°’ ì œê±°
        df_clean = df[[hp_col, metric_col]].dropna()

        if df_clean.empty:
            print("ë¶„ì„í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return

        # ì‹œê°í™”
        plt.figure(figsize=(10, 6))

        # ìˆ˜ì¹˜í˜• í•˜ì´í¼íŒŒë¼ë¯¸í„°ì¸ ê²½ìš° ì‚°ì ë„
        if pd.api.types.is_numeric_dtype(df_clean[hp_col]):
            plt.scatter(df_clean[hp_col], df_clean[metric_col], alpha=0.7)
            plt.xlabel(hyperparameter)
            plt.ylabel(target_metric)
            plt.title(f"{hyperparameter}ì˜ {target_metric}ì— ëŒ€í•œ ì˜í–¥")

            # ì¶”ì„¸ì„  ì¶”ê°€
            z = np.polyfit(df_clean[hp_col], df_clean[metric_col], 1)
            p = np.poly1d(z)
            plt.plot(df_clean[hp_col], p(df_clean[hp_col]), "r--", alpha=0.8)

        else:
            # ë²”ì£¼í˜• í•˜ì´í¼íŒŒë¼ë¯¸í„°ì¸ ê²½ìš° ë°•ìŠ¤ í”Œë¡¯
            sns.boxplot(data=df_clean, x=hp_col, y=metric_col)
            plt.xticks(rotation=45)
            plt.title(f"{hyperparameter}ë³„ {target_metric} ë¶„í¬")

        plt.tight_layout()
        plt.show()

        # í†µê³„ ìš”ì•½
        if pd.api.types.is_numeric_dtype(df_clean[hp_col]):
            correlation = df_clean[hp_col].corr(df_clean[metric_col])
            print(f"ìƒê´€ê³„ìˆ˜: {correlation:.4f}")
        else:
            summary = df_clean.groupby(hp_col)[metric_col].agg(['mean', 'std', 'count'])
            print("ê·¸ë£¹ë³„ ìš”ì•½ í†µê³„:")
            print(summary)

    def find_best_experiments(
            self,
            metric: str = "val_accuracy",
            mode: str = "max",
            top_k: int = 5
    ) -> pd.DataFrame:
        """ìµœê³  ì„±ëŠ¥ ì‹¤í—˜ ì°¾ê¸°"""
        df = self.get_experiment_data()
        metric_col = f"summary_{metric}"

        if metric_col not in df.columns:
            print(f"ë©”íŠ¸ë¦­ '{metric}'ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return pd.DataFrame()

        # ê²°ì¸¡ê°’ ì œê±°
        df_clean = df[df[metric_col].notna()]

        # ì •ë ¬
        ascending = (mode == "min")
        df_sorted = df_clean.sort_values(metric_col, ascending=ascending)

        # ìƒìœ„ Kê°œ ì„ íƒ
        top_experiments = df_sorted.head(top_k)

        # ì£¼ìš” ì»¬ëŸ¼ë§Œ ì„ íƒ
        display_cols = ["name", "state", metric_col]
        config_cols = [col for col in df.columns if col.startswith("config_")]
        display_cols.extend(config_cols[:5])  # ì£¼ìš” ì„¤ì • 5ê°œë§Œ

        return top_experiments[display_cols]


# ì‚¬ìš© ì˜ˆì‹œ
if __name__ == "__main__":
    # ë¶„ì„ê¸° ìƒì„±
    analyzer = ExperimentAnalyzer(
        project="llm-classification",
        entity="your-entity"
    )

    # ìµœê³  ì„±ëŠ¥ ì‹¤í—˜ ì°¾ê¸°
    print("ìµœê³  ì„±ëŠ¥ ì‹¤í—˜:")
    best_experiments = analyzer.find_best_experiments(
        metric="val_accuracy",
        mode="max",
        top_k=3
    )
    print(best_experiments)

    # í•˜ì´í¼íŒŒë¼ë¯¸í„° ì˜í–¥ ë¶„ì„
    analyzer.analyze_hyperparameter_impact(
        hyperparameter="learning_rate",
        target_metric="val_accuracy"
    )
```

### 3. ëª¨ë¸ ë°°í¬ë¥¼ ìœ„í•œ ìµœì í™”

```python
# src/utils/model_optimization.py
"""ëª¨ë¸ ë°°í¬ ìµœì í™” ë„êµ¬"""

import torch
import pytorch_lightning as pl
from pathlib import Path
from typing import Optional, Dict, Any, Union
import onnx
import logging

logger = logging.getLogger(__name__)


class ModelOptimizer:
    """ëª¨ë¸ ìµœì í™” ë° ë°°í¬ ì¤€ë¹„ í´ë˜ìŠ¤"""

    @staticmethod
    def convert_to_torchscript(
            model: pl.LightningModule,
            save_path: str,
            example_input: Optional[torch.Tensor] = None
    ) -> str:
        """TorchScriptë¡œ ë³€í™˜"""
        model.eval()

        if example_input is None:
            # ê¸°ë³¸ ì…ë ¥ ìƒì„± (BERT ìŠ¤íƒ€ì¼)
            batch_size, seq_len = 1, 128
            example_input = {
                "input_ids": torch.randint(0, 1000, (batch_size, seq_len)),
                "attention_mask": torch.ones(batch_size, seq_len)
            }

        # TorchScript ë³€í™˜
        try:
            if isinstance(example_input, dict):
                # ë”•ì…”ë„ˆë¦¬ ì…ë ¥ì˜ ê²½ìš° trace ëŒ€ì‹  script ì‚¬ìš©
                scripted_model = torch.jit.script(model)
            else:
                scripted_model = torch.jit.trace(model, example_input)

            # ì €ì¥
            Path(save_path).parent.mkdir(parents=True, exist_ok=True)
            scripted_model.save(save_path)

            logger.info(f"TorchScript ëª¨ë¸ì´ {save_path}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
            return save_path

        except Exception as e:
            logger.error(f"TorchScript ë³€í™˜ ì‹¤íŒ¨: {e}")
            raise

    @staticmethod
    def convert_to_onnx(
            model: pl.LightningModule,
            save_path: str,
            example_input: Optional[Dict[str, torch.Tensor]] = None,
            dynamic_axes: Optional[Dict[str, Dict[int, str]]] = None
    ) -> str:
        """ONNXë¡œ ë³€í™˜"""
        model.eval()

        if example_input is None:
            batch_size, seq_len = 1, 128
            example_input = {
                "input_ids": torch.randint(0, 1000, (batch_size, seq_len)),
                "attention_mask": torch.ones(batch_size, seq_len)
            }

        if dynamic_axes is None:
            dynamic_axes = {
                "input_ids": {0: "batch_size", 1: "sequence"},
                "attention_mask": {0: "batch_size", 1: "sequence"},
                "output": {0: "batch_size"}
            }

        try:
            # ONNX ë‚´ë³´ë‚´ê¸°
            Path(save_path).parent.mkdir(parents=True, exist_ok=True)

            torch.onnx.export(
                model,
                tuple(example_input.values()),
                save_path,
                input_names=list(example_input.keys()),
                output_names=["output"],
                dynamic_axes=dynamic_axes,
                opset_version=14,
                do_constant_folding=True,
                export_params=True
            )

            # ONNX ëª¨ë¸ ê²€ì¦
            onnx_model = onnx.load(save_path)
            onnx.checker.check_model(onnx_model)

            logger.info(f"ONNX ëª¨ë¸ì´ {save_path}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
            return save_path

        except Exception as e:
            logger.error(f"ONNX ë³€í™˜ ì‹¤íŒ¨: {e}")
            raise

    @staticmethod
    def quantize_model(
            model: pl.LightningModule,
            save_path: str,
            quantization_type: str = "dynamic"
    ) -> str:
        """ëª¨ë¸ ì–‘ìí™”"""
        model.eval()

        try:
            if quantization_type == "dynamic":
                # ë™ì  ì–‘ìí™”
                quantized_model = torch.quantization.quantize_dynamic(
                    model,
                    {torch.nn.Linear},  # ì–‘ìí™”í•  ë ˆì´ì–´ íƒ€ì…
                    dtype=torch.qint8
                )
            else:
                raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì–‘ìí™” íƒ€ì…: {quantization_type}")

            # ì €ì¥
            Path(save_path).parent.mkdir(parents=True, exist_ok=True)
            torch.save(quantized_model.state_dict(), save_path)

            logger.info(f"ì–‘ìí™”ëœ ëª¨ë¸ì´ {save_path}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
            return save_path

        except Exception as e:
            logger.error(f"ëª¨ë¸ ì–‘ìí™” ì‹¤íŒ¨: {e}")
            raise

    @staticmethod
    def optimize_for_inference(
            model: pl.LightningModule,
            output_dir: str
    ) -> Dict[str, str]:
        """ì¶”ë¡  ìµœì í™”ëœ ì—¬ëŸ¬ í¬ë§·ìœ¼ë¡œ ë³€í™˜"""
        model.eval()
        model.freeze()  # PEFT ëª¨ë¸ì˜ ê²½ìš° íŒŒë¼ë¯¸í„° ê³ ì •

        output_paths = {}

        try:
            # TorchScript ë³€í™˜
            torchscript_path = f"{output_dir}/model.pt"
            output_paths["torchscript"] = ModelOptimizer.convert_to_torchscript(
                model, torchscript_path
            )

            # ONNX ë³€í™˜
            onnx_path = f"{output_dir}/model.onnx"
            output_paths["onnx"] = ModelOptimizer.convert_to_onnx(
                model, onnx_path
            )

            # ì–‘ìí™” ëª¨ë¸
            quantized_path = f"{output_dir}/model_quantized.pth"
            output_paths["quantized"] = ModelOptimizer.quantize_model(
                model, quantized_path
            )

            logger.info("ëª¨ë“  ìµœì í™” ì™„ë£Œ:")
            for format_name, path in output_paths.items():
                logger.info(f"  {format_name}: {path}")

            return output_paths

        except Exception as e:
            logger.error(f"ì¶”ë¡  ìµœì í™” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            raise


# ë°°í¬ìš© ì¶”ë¡  í´ë˜ìŠ¤
class OptimizedInference:
    """ìµœì í™”ëœ ëª¨ë¸ì„ ì‚¬ìš©í•œ ì¶”ë¡  í´ë˜ìŠ¤"""

    def __init__(self, model_path: str, tokenizer_name: str):
        self.model_path = model_path
        self.tokenizer_name = tokenizer_name

        # í† í¬ë‚˜ì´ì € ë¡œë“œ
        from transformers import AutoTokenizer
        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)

        # ëª¨ë¸ ë¡œë“œ
        self._load_model()

    def _load_model(self):
        """ëª¨ë¸ ë¡œë“œ"""
        if self.model_path.endswith('.pt'):
            # TorchScript ëª¨ë¸
            self.model = torch.jit.load(self.model_path)
        elif self.model_path.endswith('.onnx'):
            # ONNX ëª¨ë¸
            import onnxruntime as ort
            self.model = ort.InferenceSession(self.model_path)
        else:
            # ì¼ë°˜ PyTorch ëª¨ë¸
            self.model = torch.load(self.model_path)

        logger.info(f"ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {self.model_path}")

    def predict(self, texts: Union[str, list], max_length: int = 512):
        """í…ìŠ¤íŠ¸ ì˜ˆì¸¡"""
        if isinstance(texts, str):
            texts = [texts]

        # í† í¬ë‚˜ì´ì§•
        inputs = self.tokenizer(
            texts,
            truncation=True,
            padding=True,
            max_length=max_length,
            return_tensors="pt"
        )

        # ì¶”ë¡ 
        with torch.no_grad():
            if hasattr(self.model, 'run'):  # ONNX
                # ONNX ì¶”ë¡ 
                input_dict = {
                    name: inputs[name].numpy()
                    for name in inputs.keys()
                }
                outputs = self.model.run(None, input_dict)
                logits = torch.from_numpy(outputs[0])
            else:
                # PyTorch ì¶”ë¡ 
                outputs = self.model(**inputs)
                if isinstance(outputs, dict):
                    logits = outputs["logits"]
                else:
                    logits = outputs

        # ê²°ê³¼ ì²˜ë¦¬
        predictions = torch.argmax(logits, dim=-1)
        probabilities = torch.softmax(logits, dim=-1)

        return {
            "predictions": predictions.tolist(),
            "probabilities": probabilities.tolist()
        }


# ì‚¬ìš© ì˜ˆì‹œ
if __name__ == "__main__":
    # ìµœì í™” ì˜ˆì‹œ (ì‹¤ì œ ëª¨ë¸ê³¼ ê²½ë¡œ ì‚¬ìš©)
    print("ëª¨ë¸ ìµœì í™” ë„êµ¬ ì‚¬ìš© ì˜ˆì‹œ:")
    print("1. TorchScript ë³€í™˜")
    print("2. ONNX ë³€í™˜")
    print("3. ëª¨ë¸ ì–‘ìí™”")
    print("4. í†µí•© ìµœì í™”")

    # ì¶”ë¡  ì˜ˆì‹œ
    print("\nì¶”ë¡  í´ë˜ìŠ¤ ì‚¬ìš© ì˜ˆì‹œ:")
    print("inference = OptimizedInference('model.pt', 'bert-base-uncased')")
    print("results = inference.predict(['This is a test sentence.'])")
```
