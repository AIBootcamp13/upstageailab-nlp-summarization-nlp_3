## 1. Gradient Accumulation (κ·Έλλ””μ–ΈνΈ λ„μ )

### π¤” λ¬Έμ μƒν™©

ν° λ°°μΉ ν¬κΈ°(μ: 64)λ΅ ν›λ ¨ν•κ³  μ‹¶μ§€λ§ GPU λ©”λ¨λ¦¬κ°€ λ¶€μ΅±ν•΄μ„ μ‘μ€ λ°°μΉ(μ: 16)λ§ κ°€λ¥ν• μƒν™©

### π’΅ ν•µμ‹¬ μ•„μ΄λ””μ–΄

μ‘μ€ λ°°μΉλ¥Ό μ—¬λ¬ λ² μ²λ¦¬ν•΄μ„ ν° λ°°μΉ ν¨κ³Όλ¥Ό λ‚΄λ” λ°©λ²•. "λ¶„ν•  κ²°μ "μ™€ κ°™μ€ κ°λ…μ…λ‹λ‹¤.

```mermaid
graph TD
    A[μ›ν•λ” λ°°μΉ: 64] --> B{λ©”λ¨λ¦¬ λ¶€μ΅±}
    B -->|ν•΄κ²°μ±…| C[μ‘μ€ λ°°μΉ 16μ„ 4λ² μ²λ¦¬]

    C --> D[λ°°μΉ1: 16κ°]
    C --> E[λ°°μΉ2: 16κ°]
    C --> F[λ°°μΉ3: 16κ°]
    C --> G[λ°°μΉ4: 16κ°]

    D --> H[κ·Έλλ””μ–ΈνΈ λ„μ ]
    E --> H
    F --> H
    G --> H

    H --> I[ν• λ²μ— μ—…λ°μ΄νΈ]
    I --> J[κ²°κ³Ό: λ°°μΉ 64μ™€ λ™μΌ]
```

### π”§ ν•µμ‹¬ μ½”λ“λ§

```python
# ν•µμ‹¬ κµ¬ν„
accumulation_steps = 4
optimizer.zero_grad()

for i in range(accumulation_steps):
    mini_batch = get_mini_batch(size=16)
    loss = model(mini_batch)
    loss = loss / accumulation_steps  # ν‰κ· μ„ μ„ν•΄ λ‚λ„κΈ°
    loss.backward()  # κ·Έλλ””μ–ΈνΈ λ„μ 

optimizer.step()  # ν• λ²μ— μ—…λ°μ΄νΈ
```

### β–οΈ ν¨κ³Ό

- λ©”λ¨λ¦¬: μ‘μ€ λ°°μΉ ν¬κΈ° μ μ§€
- ν•™μµ ν¨κ³Ό: ν° λ°°μΉμ™€ λ™μΌ
- μ‹κ°„: μ•½κ°„ λ” κ±Έλ¦Ό (μ—¬λ¬ λ² forward pass)

---

## 2. Mixed Precision Training (νΌν•© μ •λ°€λ„ ν›λ ¨)

### π¤” λ¬Έμ μƒν™©

λ¨λ“  μ—°μ‚°μ„ 32λΉ„νΈ(float32)λ΅ ν•λ©΄ λ©”λ¨λ¦¬ μ‚¬μ©λ‰μ΄ λ„λ¬΄ νΌ

### π’΅ ν•µμ‹¬ μ•„μ΄λ””μ–΄

16λΉ„νΈμ™€ 32λΉ„νΈλ¥Ό μ μ ν μ„μ–΄μ„ μ‚¬μ©. "μ••μ¶• νμΌ"κ³Ό κ°™μ€ κ°λ…μΌλ΅ λ©”λ¨λ¦¬ μ λ° μ μ•½.

```mermaid
graph LR
    A[μ…λ ¥ λ°μ΄ν„°] --> B[16λΉ„νΈλ΅ Forward]
    B --> C[16λΉ„νΈλ΅ μ—°μ‚°]
    C --> D[μ†μ‹¤ κ³„μ‚°]
    D --> E[32λΉ„νΈλ΅ Backward]
    E --> F[32λΉ„νΈλ΅ κ°€μ¤‘μΉ μ—…λ°μ΄νΈ]

    style B fill:#e1f5fe
    style C fill:#e1f5fe
    style E fill:#fff3e0
    style F fill:#fff3e0
```

### π”§ ν•µμ‹¬ μ½”λ“λ§

```python
# μλ™ mixed precision
scaler = torch.cuda.amp.GradScaler()

# Forward pass: 16λΉ„νΈ μλ™ μ μ©
with torch.autocast(device_type='cuda', dtype=torch.float16):
    outputs = model(inputs)
    loss = criterion(outputs, targets)

# Backward pass: μ¤μΌ€μΌλ§λ κ·Έλλ””μ–ΈνΈ
scaler.scale(loss).backward()
scaler.step(optimizer)
scaler.update()
```

### π“ λ©”λ¨λ¦¬ μ μ•½ ν¨κ³Ό

```mermaid
pie title λ©”λ¨λ¦¬ μ‚¬μ©λ‰ λΉ„κµ
    "FP32 (κΈ°λ³Έ)" : 100
    "FP16 (Mixed Precision)" : 50
```

---

## 3. Activation Checkpointing (ν™μ„±ν™” μ²΄ν¬ν¬μΈνΈ)

### π¤” λ¬Έμ μƒν™©

λ”¥λ¬λ‹ λ¨λΈμ κ° λ μ΄μ–΄ μ¶λ ¥(activation)μ„ λ¨λ‘ μ €μ¥ν•λ©΄ λ©”λ¨λ¦¬κ°€ λ¶€μ΅±ν•¨

### π’΅ ν•µμ‹¬ μ•„μ΄λ””μ–΄

μΌλ¶€ activationλ§ μ €μ¥ν•κ³ , λ‚λ¨Έμ§€λ” ν•„μ”ν•  λ• μ¬κ³„μ‚°. "λ©”λ¨λ¦¬ vs κ³„μ‚°μ‹κ°„" νΈλ μ΄λ“μ¤ν”„

```mermaid
graph TD
    A[μ…λ ¥] --> B[λ μ΄μ–΄ 1-10]
    B --> C[μ²΄ν¬ν¬μΈνΈ 1 μ €μ¥]
    C --> D[λ μ΄μ–΄ 11-20]
    D --> E[μ²΄ν¬ν¬μΈνΈ 2 μ €μ¥]
    E --> F[λ μ΄μ–΄ 21-30]
    F --> G[μ¶λ ¥]

    H[Backward μ‹ ν•„μ”ν•λ©΄] --> I[μ²΄ν¬ν¬μΈνΈλ¶€ν„° μ¬κ³„μ‚°]

    style C fill:#4caf50
    style E fill:#4caf50
    style I fill:#ff9800
```

### π”§ ν•µμ‹¬ μ½”λ“λ§

```python
from torch.utils.checkpoint import checkpoint

# μ²΄ν¬ν¬μΈνΈ μ μ©
def forward(self, x):
    # μ΄ λ¶€λ¶„μ€ activationμ„ μ €μ¥ν•μ§€ μ•μ
    x = checkpoint(self.layer_block_1, x)
    x = checkpoint(self.layer_block_2, x)
    return self.final_layer(x)
```

### β–οΈ νΈλ μ΄λ“μ¤ν”„

- λ©”λ¨λ¦¬: 30-70% μ μ•½
- κ³„μ‚°μ‹κ°„: 20-50% μ¦κ°€ (μ¬κ³„μ‚° λ•λ¬Έ)

---

## 4. Model Sharding (λ¨λΈ μƒ¤λ”©)

### π¤” λ¬Έμ μƒν™©

λ¨λΈμ΄ λ„λ¬΄ μ»¤μ„ ν•λ‚μ GPU λ©”λ¨λ¦¬μ— λ‹¤ λ“¤μ–΄κ°€μ§€ μ•μ

### π’΅ ν•µμ‹¬ μ•„μ΄λ””μ–΄

λ¨λΈμ„ μ—¬λ¬ μ΅°κ°μΌλ΅ λ‚λ„μ–΄ λ‹¤λ¥Έ μ¥μΉμ— λ¶„μ‚° μ €μ¥. "λ€ν• κ°€κµ¬ λ¶„ν•΄ μ΄μ‚¬"**μ™€ κ°™μ€ κ°λ…

```mermaid
graph TB
    subgraph "μ „μ²΄ λ¨λΈ (6GB)"
        A[λ μ΄μ–΄ 1-33<br/>2GB]
        B[λ μ΄μ–΄ 34-66<br/>2GB]
        C[λ μ΄μ–΄ 67-100<br/>2GB]
    end

    A --> D[GPU 0<br/>2GB λ©”λ¨λ¦¬]
    B --> E[GPU 1<br/>2GB λ©”λ¨λ¦¬]
    C --> F[CPU<br/>μ‹μ¤ν… λ©”λ¨λ¦¬]

    G[λ°μ΄ν„°] --> D
    D --> H[μ¤‘κ°„ κ²°κ³Ό]
    H --> E
    E --> I[μ¤‘κ°„ κ²°κ³Ό]
    I --> F
    F --> J[μµμΆ… μ¶λ ¥]
```

### π”§ ν•µμ‹¬ μ½”λ“λ§

```python
# λ¨λΈμ„ λ‹¤λ¥Έ μ¥μΉμ— λ¶„μ‚°
class ShardedModel(nn.Module):
    def __init__(self):
        self.layers_gpu0 = LayerGroup(layers=33).to('cuda:0')
        self.layers_gpu1 = LayerGroup(layers=33).to('cuda:1')
        self.layers_cpu = LayerGroup(layers=34).to('cpu')

    def forward(self, x):
        x = self.layers_gpu0(x.to('cuda:0'))
        x = self.layers_gpu1(x.to('cuda:1'))
        x = self.layers_cpu(x.to('cpu'))
        return x
```

---

## 5. λ”¥λ¬λ‹ ν›λ ¨ μ‹ λ©”λ¨λ¦¬ μ‚¬μ© κµ¬μ΅°

### π“ λ©”λ¨λ¦¬ μ‚¬μ© κµ¬μ„±μ”μ†

```mermaid
pie title λ”¥λ¬λ‹ ν›λ ¨ λ©”λ¨λ¦¬ κµ¬μ„±
    "λ¨λΈ νλΌλ―Έν„°" : 25
    "κ·Έλλ””μ–ΈνΈ" : 25
    "Optimizer μƒνƒ" : 35
    "Activation" : 15
```

κ° κµ¬μ„±μ”μ†λ³„ μµμ ν™” λ°©λ²•

- λ¨λΈ νλΌλ―Έν„°: Model Sharding, Mixed Precision
- κ·Έλλ””μ–ΈνΈ: Mixed Precision, Gradient Accumulation
- Optimizer μƒνƒ: Mixed Precision, ZeRO Optimizer
- Activation: Activation Checkpointing, Gradient Accumulation

---

## 6. μµμ ν™” κΈ°λ²• μ μ© μμ„

### π― λ‹¨κ³„λ³„ μ μ© κ°€μ΄λ“

```mermaid
graph TD
    A[λ©”λ¨λ¦¬ λ¶€μ΅± λ¬Έμ ] --> B{1λ‹¨κ³„: Mixed Precision}
    B -->|50% μ μ•½| C{μ¶©λ¶„ν•κ°€?}
    C -->|No| D{2λ‹¨κ³„: Gradient Accumulation}
    D -->|λ°°μΉ ν¬κΈ° ν•΄κ²°| E{μ¶©λ¶„ν•κ°€?}
    E -->|No| F{3λ‹¨κ³„: Activation Checkpointing}
    F -->|μ¶”κ°€ 30% μ μ•½| G{μ¶©λ¶„ν•κ°€?}
    G -->|No| H[4λ‹¨κ³„: Model Sharding]

    C -->|Yes| I[μ™„λ£]
    E -->|Yes| I
    G -->|Yes| I
    H --> I

    style B fill:#4caf50
    style D fill:#2196f3
    style F fill:#ff9800
    style H fill:#f44336
```

---

## 7. μ‹¤μ  μ„±λ¥ λΉ„κµ (GPT-2 Large κΈ°μ¤€)

### π“ κ° κΈ°λ²•λ³„ ν¨κ³Ό

|μµμ ν™” κΈ°λ²•|λ©”λ¨λ¦¬ μ‚¬μ©λ‰|ν›λ ¨ μ†λ„|κµ¬ν„ λ‚μ΄λ„|
|---|---|---|---|
|κΈ°λ³Έ μ„¤μ •|12GB|100%|β­|
|Mixed Precision|6GB (-50%)|115% (+15%)|β­β­|
|+ Gradient Accumulation|6GB|110%|β­β­|
|+ Activation Checkpointing|4GB (-67%)|90% (-10%)|β­β­β­|
|+ Model Sharding|2GB/μ¥μΉ|80% (-20%)|β­β­β­β­β­|

---

## 8. μ‹¤μƒν™ λΉ„μ λ΅ μ™„μ „ μ΄ν•΄ν•κΈ°

### π  μ΄μ‚¬ λΉ„μ 

- Gradient Accumulation: μ‘μ€ μ°¨λ΅ μ—¬λ¬ λ² λ‚λ¥΄κΈ° (λ€μ‹  μ‹κ°„ λ” κ±Έλ¦Ό)
- Mixed Precision: μ§μ„ μ••μ¶•ν•΄μ„ λ¶€ν”Ό μ¤„μ΄κΈ° (μ•½κ°„μ ν’μ§ μ†μ‹¤)
- Activation Checkpointing: μΌλ¶€ μ§λ§ μ„μ‹ λ³΄κ΄€, λ‚λ¨Έμ§€λ” λ‚μ¤‘μ— λ‹¤μ‹ κ°€μ Έμ¤κΈ°
- Model Sharding: ν° κ°€κµ¬λ¥Ό λ¶„ν•΄ν•΄μ„ μ—¬λ¬ μ¥μ†μ— λ‚λ„μ–΄ λ³΄κ΄€

### π’° λ κ΄€λ¦¬ λΉ„μ 

- Gradient Accumulation: λ¶„ν•  κ²°μ  (λ©ν‘ κΈμ•΅μ€ λ™μΌ, λ‹Ήμ¥ ν•„μ”ν• λ μ¤„μ„)
- Mixed Precision: λ™μ „ λ€μ‹  μ§€ν μ‚¬μ© (κ³µκ°„ μ μ•½, κ°€μΉ λ™μΌ)
- Activation Checkpointing: μμμ¦ μΌλ¶€λ§ λ³΄κ΄€, ν•„μ”μ‹ μ¬λ°κΈ‰
- Model Sharding: μμ‚°μ„ μ—¬λ¬ μ€ν–‰μ— λ¶„μ‚° ν¬μ

---

# DeepSpeed vs Mac MPS μµμ ν™” κµ¬ν„ λΉ„κµ

## 1. Gradient Accumulation κµ¬ν„ λ°©μ‹

### π§ **DeepSpeed (Linux + CUDA)**

**μλ™ν™”λ κµ¬ν„**: μ„¤μ • νμΌμ—μ„ μλ™μΌλ΅ μ²λ¦¬

```json
// deepspeed_config.json
{
  "train_batch_size": 64,
  "train_micro_batch_size_per_gpu": 16,
  "gradient_accumulation_steps": 4  // μλ™ κ³„μ‚°: 64/16 = 4
}
```

```python
# DeepSpeedκ°€ μλ™μΌλ΅ μ²λ¦¬
model_engine, optimizer, _, _ = deepspeed.initialize(
    model=model,
    config="deepspeed_config.json"
)

# κ°„λ‹¨ν• ν›λ ¨ λ£¨ν”„
for batch in dataloader:
    loss = model_engine(batch)
    model_engine.backward(loss)  # μλ™μΌλ΅ accumulation μ²λ¦¬
    model_engine.step()          # μλ™μΌλ΅ step μ¤μΌ€μ¤„λ§
```

### π **Mac MPS**

**μλ™ κµ¬ν„**: μ§μ ‘ accumulation λ΅μ§ μ‘μ„±

```python
# μλ™μΌλ΅ accumulation κµ¬ν„
accumulation_steps = 4
effective_batch_size = 16 * 4  # 64

optimizer.zero_grad()
for i, batch in enumerate(dataloader):
    with torch.autocast(device_type='mps', dtype=torch.float16):
        outputs = model(batch)
        loss = criterion(outputs, batch['labels'])
        loss = loss / accumulation_steps  # μλ™μΌλ΅ μ¤μΌ€μΌλ§

    loss.backward()  # accumulate

    if (i + 1) % accumulation_steps == 0:
        optimizer.step()
        optimizer.zero_grad()
```

---

## 2. Mixed Precision κµ¬ν„ λ°©μ‹

### π§ **DeepSpeed**

**μ™„μ „ μλ™ν™”**: ZeROμ™€ ν†µν•©λμ–΄ μλ™ μµμ ν™”

```json
// deepspeed_config.json
{
  "fp16": {
    "enabled": true,
    "auto_cast": true,           // μλ™ νƒ€μ… λ³€ν™
    "loss_scale": 0,             // λ™μ  μ¤μΌ€μΌλ§
    "initial_scale_power": 16
  }
}
```

```python
# DeepSpeedκ°€ λ¨λ“  κ²ƒμ„ μλ™ μ²λ¦¬
model_engine, _, _, _ = deepspeed.initialize(
    model=model,
    config="deepspeed_config.json"
)

# μ¶”κ°€ μ½”λ“ μ—†μ΄ μλ™ mixed precision
loss = model_engine(batch)
model_engine.backward(loss)  # μλ™ gradient scaling
model_engine.step()
```

### π **Mac MPS**

**μλ™ κ΄€λ¦¬**: PyTorch AMP μ§μ ‘ μ‚¬μ©

```python
# μλ™μΌλ΅ scalerμ™€ autocast κ΄€λ¦¬
scaler = torch.cuda.amp.GradScaler()  # MPSμ©μ€ λ³„λ„ μ¤μΌ€μΌλ¬

for batch in dataloader:
    optimizer.zero_grad()

    # μλ™μΌλ΅ autocast λ²”μ„ μ§€μ •
    with torch.autocast(device_type='mps', dtype=torch.float16):
        outputs = model(batch)
        loss = criterion(outputs, batch['labels'])

    # μλ™μΌλ΅ scaled backward
    scaler.scale(loss).backward()
    scaler.step(optimizer)
    scaler.update()
```

---

## 3. Activation Checkpointing κµ¬ν„ λ°©μ‹

### π§ **DeepSpeed**

**μλ™ μµμ ν™”**: λ©”λ¨λ¦¬ μ‚¬μ©λ‰μ— λ”°λΌ μλ™ μ μ©

```json
// deepspeed_config.json
{
  "activation_checkpointing": {
    "partition_activations": true,
    "cpu_checkpointing": false,      // GPU λ©”λ¨λ¦¬ μ¶©λ¶„ν•λ©΄ GPUμ—μ„
    "contiguous_memory_optimization": true,
    "number_checkpoints": 4,         // μλ™μΌλ΅ μµμ  μ§€μ  μ„ νƒ
    "synchronize_checkpoint_boundary": false
  }
}
```

```python
# DeepSpeedκ°€ μλ™μΌλ΅ λ¨λΈμ— checkpointing μ μ©
model_engine, _, _, _ = deepspeed.initialize(
    model=model,
    config="deepspeed_config.json"
)
# λ³„λ„ μ½”λ“ μ—†μ΄ μλ™ μ μ©λ¨
```

### π **Mac MPS**

**μλ™ μ§€μ •**: μ²΄ν¬ν¬μΈνΈ μ„μΉλ¥Ό μ§μ ‘ μ„ νƒ

```python
from torch.utils.checkpoint import checkpoint

class ManualCheckpointModel(nn.Module):
    def __init__(self, model):
        super().__init__()
        self.embeddings = model.embeddings
        self.encoder_layers = model.encoder.layers
        self.classifier = model.classifier

    def forward(self, x):
        x = self.embeddings(x)

        # μλ™μΌλ΅ μ²΄ν¬ν¬μΈνΈ μ μ©ν•  λ μ΄μ–΄ κ·Έλ£Ή μ§€μ •
        for i in range(0, len(self.encoder_layers), 4):  # 4κ° λ μ΄μ–΄λ§λ‹¤
            layer_group = nn.Sequential(*self.encoder_layers[i:i+4])
            x = checkpoint(layer_group, x)  # μλ™ μ²΄ν¬ν¬μΈνΈ

        return self.classifier(x)

# μ‚¬μ©
checkpointed_model = ManualCheckpointModel(original_model)
```

---

## 4. Model Sharding κµ¬ν„ λ°©μ‹

### π§ **DeepSpeed**

**ZeROλ¥Ό ν†µν• μλ™ λ¶„μ‚°**: λ©”λ¨λ¦¬μ— λ”°λΌ μλ™μΌλ΅ μµμ  λ¶„μ‚°

```json
// deepspeed_config.json
{
  "zero_optimization": {
    "stage": 3,                    // ZeRO-3: νλΌλ―Έν„°κΉμ§€ λ¶„μ‚°
    "offload_optimizer": {
      "device": "cpu",             // Optimizerλ¥Ό CPUλ΅
      "pin_memory": true
    },
    "offload_param": {
      "device": "cpu",             // νλΌλ―Έν„°λ¥Ό CPUλ΅
      "pin_memory": true
    },
    "overlap_comm": true,          // ν†µμ‹ κ³Ό μ—°μ‚° μ¤λ²„λ©
    "contiguous_gradients": true,
    "reduce_bucket_size": 2e8,
    "stage3_prefetch_bucket_size": 2e8,
    "stage3_param_persistence_threshold": 2e8
  }
}
```

```mermaid
graph TD
    A[λ¨λΈ νλΌλ―Έν„°] --> B[ZeRO-3 μλ™ λ¶„μ‚°]
    B --> C[GPU 0: μΌλ¶€ νλΌλ―Έν„°]
    B --> D[GPU 1: μΌλ¶€ νλΌλ―Έν„°]
    B --> E[CPU: Optimizer States]
    B --> F[CPU: μ—¬μ  νλΌλ―Έν„°]

    G[Forward Pass] --> H[ν•„μ”ν• νλΌλ―Έν„°λ§ GPUλ΅ μλ™ λ΅λ“]
    H --> I[μ—°μ‚° μ™„λ£ ν›„ CPUλ΅ μλ™ μ¤ν”„λ΅λ“]
```

### π **Mac MPS**

**μλ™ λ¶„μ‚°**: ν†µν•© λ©”λ¨λ¦¬ μ•„ν‚¤ν…μ² ν™μ©ν• μλ™ κ΄€λ¦¬

```python
class ManualShardedModel(nn.Module):
    def __init__(self, model_config):
        super().__init__()

        # μλ™μΌλ΅ λ μ΄μ–΄λ¥Ό λ‹¤λ¥Έ μ„μΉμ— λ°°μΉ
        self.gpu_layers = nn.ModuleList([
            TransformerLayer(config) for _ in range(12)  # GPUμ— 12κ°
        ]).to('mps')

        self.cpu_layers = nn.ModuleList([
            TransformerLayer(config) for _ in range(12)  # CPUμ— 12κ°
        ]).to('cpu')

        # μ¤‘μ”ν• λ μ΄μ–΄λ” GPUμ— μ μ§€
        self.embeddings = nn.Embedding(config.vocab_size, config.hidden_size).to('mps')
        self.classifier = nn.Linear(config.hidden_size, config.vocab_size).to('mps')

    def forward(self, x):
        # GPUμ—μ„ μ‹μ‘
        x = self.embeddings(x.to('mps'))

        # GPU λ μ΄μ–΄λ“¤ μ²λ¦¬
        for layer in self.gpu_layers:
            x = layer(x)

        # CPUλ΅ μ΄λ™ν•μ—¬ μ²λ¦¬ (Mac ν†µν•© λ©”λ¨λ¦¬λ΅ λΉ λ¥Έ μ΄λ™)
        x = x.to('cpu')
        for layer in self.cpu_layers:
            x = layer(x)

        # μµμΆ… λ¶„λ¥λ¥Ό μ„ν•΄ λ‹¤μ‹ GPUλ΅
        x = self.classifier(x.to('mps'))
        return x
```

```mermaid
graph TD
    A[ν†µν•© λ©”λ¨λ¦¬ μ•„ν‚¤ν…μ²] --> B[MPS GPU]
    A --> C[CPU]

    D[λ¨λΈ λ μ΄μ–΄] --> E[μ¤‘μ” λ μ΄μ–΄: MPS GPU]
    D --> F[μΌλ° λ μ΄μ–΄: CPU]

    G[λ°μ΄ν„° νλ¦„] --> H[MPSμ—μ„ μ„λ² λ”©]
    H --> I[MPS λ μ΄μ–΄ μ²λ¦¬]
    I --> J[CPUλ΅ μ΄λ™]
    J --> K[CPU λ μ΄μ–΄ μ²λ¦¬]
    K --> L[MPSλ΅ λ³µκ·€]
    L --> M[λ¶„λ¥ λ μ΄μ–΄]
```

---

## 5. μΆ…ν•© λΉ„κµν‘

|μµμ ν™” κΈ°λ²•|DeepSpeed (Linux+CUDA)|Mac MPS|μ¥λ‹¨μ  λΉ„κµ|
|---|---|---|---|
|**Gradient Accumulation**|β… μ™„μ „ μλ™ν™”<br/>μ„¤μ •λ§μΌλ΅ λ™μ‘|β οΈ μλ™ κµ¬ν„<br/>μ§μ ‘ λ£¨ν”„ μ‘μ„±|DS: νΈλ¦¬ν•¨<br/>MPS: μ„Έλ°€ν• μ μ–΄|
|**Mixed Precision**|β… ZeRO ν†µν•©<br/>λ™μ  μµμ ν™”|β οΈ PyTorch AMP<br/>μλ™ μ¤μΌ€μΌλ§|DS: μ•μ •μ„±<br/>MPS: ν¬λ…μ„±|
|**Activation Checkpointing**|β… μλ™ μµμ ν™”<br/>λ©”λ¨λ¦¬ κΈ°λ° κ²°μ •|β οΈ μλ™ μ§€μ •<br/>κ°λ°μ νλ‹¨|DS: μλ™ν™”<br/>MPS: μ»¤μ¤ν„°λ§μ΄μ§•|
|**Model Sharding**|β… ZeRO-3<br/>μ™„μ „ ν¬λ…|β οΈ ν†µν•©λ©”λ¨λ¦¬ ν™μ©<br/>μλ™ κ΄€λ¦¬|DS: ν™•μ¥μ„±<br/>MPS: λ‹¨μμ„±|

---

## 6. μ‹¤μ  λ©”λ¨λ¦¬ κ΄€λ¦¬ λ°©μ‹

### π§ **DeepSpeed λ©”λ¨λ¦¬ κ΄€λ¦¬**

```mermaid
graph TD
    A[ZeRO Stage 1] --> B[Optimizer States λ¶„μ‚°]
    A --> C[ZeRO Stage 2] --> D[+ Gradients λ¶„μ‚°]
    C --> E[ZeRO Stage 3] --> F[+ Parameters λ¶„μ‚°]

    G[μλ™ CPU Offloading] --> H[GPU λ©”λ¨λ¦¬ λ¶€μ΅±μ‹]
    H --> I[μλ™μΌλ΅ CPU μ΄λ™]
    I --> J[ν•„μ”μ‹ GPUλ΅ λ³µκ·€]

    style A fill:#e3f2fd
    style C fill:#bbdefb
    style E fill:#2196f3
```

### π **Mac MPS λ©”λ¨λ¦¬ κ΄€λ¦¬**

```mermaid
graph TD
    A[ν†µν•© λ©”λ¨λ¦¬ ν’€] --> B[MPS GPU μμ—­]
    A --> C[CPU μμ—­]
    A --> D[Neural Engine μμ—­]

    B --> E[μ„λ² λ”© + μ¤‘μ” λ μ΄μ–΄]
    C --> F[λ€λ¶€λ¶„μ νΈλμ¤ν¬λ¨Έ λ μ΄μ–΄]
    D --> G[μ¶”λ΅  μµμ ν™”]

    H[λ©”λ¨λ¦¬ μ΄λ™] --> I[Zero-copy λλ” λΉ λ¥Έ λ³µμ‚¬]

    style A fill:#4caf50
    style B fill:#81c784
    style C fill:#a5d6a7
    style D fill:#c8e6c9
```

---

## 7. μ„±λ¥ λ° μ‚¬μ©μ„± λΉ„κµ

### π“ **κ°λ° νΈμμ„±**

|μΈ΅λ©΄|DeepSpeed|Mac MPS|μΉμ|
|---|---|---|---|
|**μ„¤μ • λ³µμ΅λ„**|JSON μ„¤μ • νμΌ|μλ™ μ½”λ“ μ‘μ„±|π† DeepSpeed|
|**λ””λ²„κΉ… μ©μ΄μ„±**|λΈ”λ™λ°•μ¤|ν¬λ…ν• μ μ–΄|π† Mac MPS|
|**μ»¤μ¤ν„°λ§μ΄μ§•**|μ ν•μ |μ™„μ „ν• μμ |π† Mac MPS|
|**ν•™μµ κ³΅μ„ **|λ‚®μ|λ†’μ|π† DeepSpeed|
|**μ—λ¬ μ²λ¦¬**|μλ™ λ³µκµ¬|μλ™ μ²λ¦¬|π† DeepSpeed|

### π“ **μ„±λ¥**

|μΈ΅λ©΄|DeepSpeed|Mac MPS|μΉμ|
|---|---|---|---|
|**Multi-GPU ν™•μ¥μ„±**|λ›°μ–΄λ‚¨|λ¶κ°€λ¥|π† DeepSpeed|
|**λ‹¨μΌ GPU ν¨μ¨μ„±**|λ§¤μ° λ†’μ|λ†’μ|π† DeepSpeed|
|**λ©”λ¨λ¦¬ μµμ ν™”**|κ·Ήλ„λ΅ ν¨μ¨μ |ν¨μ¨μ |π† DeepSpeed|
|**κ°λ°/ν”„λ΅ν† νƒ€μ΄ν•‘**|λΉ λ¥Έ μ„¤μ •|μ μ—°ν• μ‹¤ν—|π† Mac MPS|
|**λ€ν• λ¨λΈ (50B+)**|νΉν™”λ¨|μ ν•μ |π† DeepSpeed|

---

## 8. μ–Έμ  λ¬΄μ—‡μ„ μ„ νƒν• κΉ?

### π― **DeepSpeedλ¥Ό μ„ νƒν•΄μ•Ό ν•λ” κ²½μ°**

- **ν”„λ΅λ•μ… ν™κ²½**μ—μ„ λ€κ·λ¨ λ¨λΈ ν›λ ¨
- **Multi-GPU** ν™κ²½ ν™μ© κ°€λ¥
- **λΉ λ¥Έ κ°λ°**μ΄ μ°μ„ μμ„
- **13B+ νλΌλ―Έν„°** λ¨λΈ ν›λ ¨
- **μ•μ •μ„±**μ΄ μ¤‘μ”ν• ν”„λ΅μ νΈ

### π― **Mac MPSλ¥Ό μ„ νƒν•΄μ•Ό ν•λ” κ²½μ°**

- **λ΅μ»¬ κ°λ°/ν”„λ΅ν† νƒ€μ΄ν•‘** ν™κ²½
- **μ‹¤ν—μ  μµμ ν™”** κΈ°λ²• ν…μ¤νΈ
- **μ„Έλ°€ν• μ μ–΄**κ°€ ν•„μ”ν• μ—°κµ¬
- **7B μ΄ν•** λ¨λΈλ΅ μ¶©λ¶„
- **κµμ΅/ν•™μµ** λ©μ 

λ‘ λ°©μ‹ λ¨λ‘ κ°™μ€ μµμ ν™” μ•„μ΄λ””μ–΄λ¥Ό κµ¬ν„ν•μ§€λ§, **μλ™ν™” vs μλ™μ μ–΄**μ μ² ν•™ μ°¨μ΄κ°€ κ°€μ¥ ν½λ‹λ‹¤!
