
# âœ… 1. ê¸°ë³¸ ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¶ˆëŸ¬ì˜¤ê¸°
import os
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import matplotlib.font_manager as fm
from transformers import AutoTokenizer

# âœ… 2. í•œê¸€ í°íŠ¸ ì„¤ì • (NanumBarunGothic.ttf ì‚¬ìš©)

font_path = r"C:\Users\ì¬í˜•ë \Desktop\ì½”ë”©ì¹œêµ¬ë“¤\upstageailab-nlp-summarization-nlp_3\data\fonts\NanumBarunGothic.ttf"
fm.fontManager.addfont(font_path)
plt.rcParams['font.family'] = 'NanumBarunGothic'
plt.rcParams['axes.unicode_minus'] = False

# í°íŠ¸ ê²½ë¡œ: data/fonts/NanumBarunGothic.ttf
fontprop = fm.FontProperties(fname=font_path)
plt.rcParams['font.family'] = fontprop.get_name()
plt.rcParams['axes.unicode_minus'] = False  # ë§ˆì´ë„ˆìŠ¤(-) ê¹¨ì§ ë°©ì§€

# âœ… 3. ë°ì´í„° ë¡œë“œ (src/data)
train_path = r"C:\Users\ì¬í˜•ë \Desktop\ì½”ë”©ì¹œêµ¬ë“¤\upstageailab-nlp-summarization-nlp_3\src\data\train.csv"
test_path = r"C:\Users\ì¬í˜•ë \Desktop\ì½”ë”©ì¹œêµ¬ë“¤\upstageailab-nlp-summarization-nlp_3\src\data\test.csv"

train = pd.read_csv(train_path, encoding="utf-8")  # or cp949 if needed
test = pd.read_csv(test_path, encoding="utf-8")

# âœ… 4. ê¸°ë³¸ ì •ë³´ í™•ì¸
print("âœ… train.csv shape:", train.shape)
print("âœ… test.csv shape:", test.shape)
print("\nğŸ“Œ train ì»¬ëŸ¼ ëª©ë¡:", train.columns.tolist())
print("\nğŸ“Œ train ìƒ˜í”Œ:\n", train.head())

# âœ… 5. ê²°ì¸¡ì¹˜ & ì¤‘ë³µ í™•ì¸
print("\nğŸ“Œ ê²°ì¸¡ì¹˜:\n", train.isnull().sum())
print("\nğŸ“Œ ì¤‘ë³µ í–‰ ê°œìˆ˜:", train.duplicated().sum())

# âœ… 6. í…ìŠ¤íŠ¸ ê¸¸ì´ í†µê³„ ë¶„ì„
train['text_length'] = train['dialogue'].apply(len)
print("\nğŸ“Œ text ê¸¸ì´ í†µê³„:\n", train['text_length'].describe())

print("ğŸ“Œ í† í”½ë³„ ìƒ˜í”Œ í…ìŠ¤íŠ¸ (ìµœëŒ€ 5ê°œê¹Œì§€ ì¶œë ¥):")
unique_topics = sorted(train["topic"].unique())[:5]  # ë„ˆë¬´ ë§ìœ¼ë©´ ì œí•œ

for topic in unique_topics:
    sample_text = train[train["topic"] == topic]["dialogue"].values[0][:300]
    print(f"\nğŸŸ¡ [Topic: {topic}] ìƒ˜í”Œ í…ìŠ¤íŠ¸ (ì• 300ì):\n{sample_text}")

# âœ… 1. í…ìŠ¤íŠ¸ ê¸¸ì´ ë¶„í¬ ì‹œê°í™”
plt.figure(figsize=(10, 6))
sns.histplot(train['text_length'], bins=50, kde=True, color='royalblue')
plt.title("í…ìŠ¤íŠ¸ ê¸¸ì´ ë¶„í¬", fontsize=16)
plt.xlabel("ë¬¸ì ìˆ˜", fontsize=13)
plt.ylabel("ìƒ˜í”Œ ìˆ˜", fontsize=13)
plt.tight_layout()
plt.grid(True)
plt.show()

# âœ… 2. ìš”ì•½ í…ìŠ¤íŠ¸ ê¸¸ì´ ë¶„í¬ (summary)
train['summary_len'] = train['summary'].apply(len)
plt.figure(figsize=(10, 6))
sns.histplot(train['summary_len'], bins=50, kde=True, color='salmon')
plt.title("ìš”ì•½ í…ìŠ¤íŠ¸ ê¸¸ì´ ë¶„í¬")
plt.xlabel("ë¬¸ì ìˆ˜")
plt.ylabel("ìƒ˜í”Œ ìˆ˜")
plt.tight_layout()
plt.savefig("summary_length_distribution.png", dpi=200)
plt.close()

# âœ… 3. ë ˆì´ë¸” ë¶„í¬ (ìƒìœ„ 100ê°œ)
top_topics = train['topic'].value_counts().nlargest(100)
plt.figure(figsize=(12, 6))
sns.barplot(x=top_topics.index, y=top_topics.values, palette="Set3")
plt.title("Topic ìƒìœ„ 100ê°œ ë¶„í¬")
plt.xticks(rotation=75)
plt.tight_layout()
plt.savefig("top100_topic_distribution.png", dpi=200)
plt.close()

# âœ… 4. ê³ ìœ  í† í”½ ê°œìˆ˜ ì¶œë ¥
print("ê³ ìœ  Topic ê°œìˆ˜:", train['topic'].nunique())

# âœ… 5. íŠ¹ìˆ˜ë¬¸ì/ìˆ«ì í¬í•¨ ë¹„ìœ¨
import re

def count_special_ratio(text):
    return len(re.findall(r"[^\w\s]", text)) / len(text)

train['special_ratio'] = train['dialogue'].apply(count_special_ratio)
print("í‰ê·  íŠ¹ìˆ˜ë¬¸ì ë¹„ìœ¨:", train['special_ratio'].mean())

# âœ… 6. tokenizer ê¸°ì¤€ í† í° ê¸¸ì´
tokenizer = AutoTokenizer.from_pretrained("hyunwoongko/kobart") # ìš”ì•½ taskì— ì í•©
train['dialogue_token_len'] = train['dialogue'].apply(lambda x: len(tokenizer.encode(x, truncation=False)))
train['summary_token_len'] = train['summary'].apply(lambda x: len(tokenizer.encode(x, truncation=False)))

plt.figure(figsize=(10, 5))
sns.histplot(train['dialogue_token_len'], color="dodgerblue", label="ì…ë ¥")
sns.histplot(train['summary_token_len'], color="tomato", label="ìš”ì•½")
plt.title("Token ê¸¸ì´ ë¶„í¬")
plt.xlabel("Token ìˆ˜")
plt.legend()
plt.tight_layout()
plt.savefig("token_length_comparison.png", dpi=200)
plt.close()

# ìš”ì•½ vs ì…ë ¥ ê¸¸ì´ ë¹„ìœ¨ ë¶„í¬
# â†’ í‰ê· ê°’ì´ ë„ˆë¬´ ë†’ê±°ë‚˜ ë‚®ìœ¼ë©´ ëª¨ë¸ì´ ì˜ ëª» ë°°ì›€
#â†’ ë³´í†µ 3~8 ì‚¬ì´ ë¶„í¬ê°€ ì•ˆì •ì 
train['dialogue_len'] = train['dialogue'].apply(len)
train['summary_len'] = train['summary'].apply(len)
train['length_ratio'] = train['dialogue_len'] / train['summary_len']

plt.figure(figsize=(10, 6))
sns.histplot(train['length_ratio'], bins=50, kde=True, color="purple")
plt.title("ì…ë ¥ ê¸¸ì´ / ìš”ì•½ ê¸¸ì´ ë¹„ìœ¨ ë¶„í¬", fontsize=16)
plt.xlabel("ë¹„ìœ¨ (dialogue / summary)")
plt.ylabel("ìƒ˜í”Œ ìˆ˜")
plt.tight_layout()
plt.savefig("dialogue_summary_ratio.png", dpi=200)
plt.close()

# tokenizer ì ìš© í›„ ì…ë ¥/ìš”ì•½ í† í° ìˆ˜
# â†’ max_seq_len, max_target_len ì„¤ì • ê·¼ê±°ê°€ ë¨
# â†’ ex) 95% quantile ê¸°ì¤€ìœ¼ë¡œ ìë¥´ëŠ” ê²Œ ì¢‹ìŒ
tokenizer = AutoTokenizer.from_pretrained("hyunwoongko/kobart")

train['input_token_len'] = train['dialogue'].apply(lambda x: len(tokenizer.encode(x, truncation=False)))
train['output_token_len'] = train['summary'].apply(lambda x: len(tokenizer.encode(x, truncation=False)))

plt.figure(figsize=(10, 6))
sns.histplot(train['input_token_len'], color="dodgerblue", label="ì…ë ¥", kde=True)
sns.histplot(train['output_token_len'], color="tomato", label="ìš”ì•½", kde=True)
plt.title("Token ê¸¸ì´ ë¶„í¬ (tokenizer ì ìš© í›„)", fontsize=16)
plt.xlabel("í† í° ìˆ˜")
plt.legend()
plt.tight_layout()
plt.savefig("token_length_distribution.png", dpi=200)
plt.close()

# topicë³„ ìš”ì•½ ê¸¸ì´ ì°¨ì´ í™•ì¸
plt.figure(figsize=(14, 6))
sns.boxplot(x='topic', y='summary_len', data=train[train['topic'].isin(train['topic'].value_counts().nlargest(10).index)])
plt.title("ìƒìœ„ 10ê°œ Topicë³„ ìš”ì•½ ê¸¸ì´ ë¶„í¬", fontsize=16)
plt.xticks(rotation=60)
plt.tight_layout()
plt.savefig("summary_length_per_topic.png", dpi=200)
plt.close()

# ì¤‘ë³µ ëŒ€í™” íƒì§€
duplicate_dialogues = train['dialogue'].duplicated().sum()
print(f"ğŸ“Œ ì¤‘ë³µëœ dialogue ìˆ˜: {duplicate_dialogues}ê°œ ({duplicate_dialogues/train.shape[0]*100:.2f}%)")